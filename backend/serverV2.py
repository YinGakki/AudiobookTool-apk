import os
import sys
import ctypes
# ==========================================
# 1. ç¯å¢ƒé…ç½® (æ›¿ä»£ BAT æ–‡ä»¶)
# ==========================================
# è·å–å½“å‰è„šæœ¬æ‰€åœ¨çš„ç»å¯¹è·¯å¾„ (ç›¸å½“äº %cd%)
base_dir = os.path.dirname(os.path.abspath(__file__))

# è®¾ç½®ç¯å¢ƒå˜é‡ CL (å¯¹åº” set CL=/utf-8)
os.environ['CL'] = '/utf-8'

# æ„å»º ffmpeg çš„è·¯å¾„
ffmpeg_path = os.path.join(base_dir, "ffmpeg-8.0-full_build", "bin")

# å°†å½“å‰ç›®å½•å’Œ ffmpeg è·¯å¾„åŠ å…¥ç³»ç»Ÿ PATH
# æ’å…¥åˆ°æœ€å‰é¢ï¼Œç¡®ä¿ä¼˜å…ˆçº§
os.environ['PATH'] = base_dir + os.pathsep + ffmpeg_path + os.pathsep + os.environ['PATH']

import shutil
import argparse
import base64
import logging
import random
import io
import json
import re
import time
import requests
from typing import Optional, List, Dict, Any
from pydantic import BaseModel
from fastapi import FastAPI, HTTPException, Request, UploadFile, File, Form, Response
from fastapi.responses import JSONResponse, FileResponse
from fastapi.staticfiles import StaticFiles
from fastapi.middleware.cors import CORSMiddleware
from pydub import AudioSegment
import soundfile as sf
from pydub.effects import normalize, low_pass_filter
from pydub.scipy_effects import high_pass_filter
import zipfile 
from fastapi.responses import StreamingResponse
import urllib.parse
import string
import asyncio
import math
from pydub.silence import detect_leading_silence
import uuid
from pydub.effects import normalize as pydub_normalize
import uvicorn
from urllib.parse import urlparse, urlunparse, unquote
from ollama import Client  # å¯¼å…¥Ollamaå®¢æˆ·ç«¯

os.system('cls' if os.name == 'nt' else 'clear')

# --- åŸºæœ¬é…ç½®å’Œç›®å½•å®šä¹‰ ---
ROOT_DIR = os.path.dirname(os.path.abspath(__file__))

from collections import deque
# ... å…¶ä»– imports ä¿æŒä¸å˜

# =============================================================
# ã€æ–°å¢ã€‘æ—¥å¿—æ•è·å­˜å‚¨æœºåˆ¶
# =============================================================
# å…¨å±€å­˜å‚¨ï¼Œç”¨äºä¿å­˜æœ€æ–°çš„ N æ¡æ—¥å¿— (è®¾ç½®ä¸º 10 æ¡ï¼Œæ–¹ä¾¿å‰ç«¯æ˜¾ç¤º)
LOG_HISTORY = deque(maxlen=25) 

# è‡ªå®šä¹‰æ—¥å¿—å¤„ç†å™¨ï¼Œå°†æ—¥å¿—å†™å…¥ LOG_HISTORY
class FrontendLogHandler(logging.Handler):
    def emit(self, record):
        # ä½¿ç”¨å¤„ç†å™¨ä¸Šçš„ formatter æ¥æ ¼å¼åŒ–æ—¥å¿—
        log_message = self.format(record)
        LOG_HISTORY.append(log_message)

class Logger:
    def __init__(self, filename, mode="a", encoding="utf-8"):
        self.filename = filename
        self.mode = mode
        self.encoding = encoding
        # æ‰“å¼€æ–‡ä»¶å¥æŸ„
        self.file = open(filename, mode, encoding=encoding, buffering=1)

    def write(self, message):
        # åªå†™å…¥æ–‡ä»¶ï¼Œä¸å†™å…¥ original_stdout
        self.file.write(message)
        self.file.flush()

    def flush(self):
        self.file.flush()

    def isatty(self):
        # è¿”å› Falseï¼Œè¿™æ · print() å°±ä¸ä¼šå°è¯•æ·»åŠ é¢œè‰²ç­‰æ§åˆ¶ç 
        return False

    def close(self):
        self.file.close()
import logging

# é…ç½®æ ‡å‡† logging æ¨¡å—
log_file_path = os.path.join(ROOT_DIR, "app.log")  # logger ä¸“ç”¨çš„æ—¥å¿—æ–‡ä»¶
print_capture_file_path = os.path.join(ROOT_DIR, "print.log") # print ä¸“ç”¨çš„æ—¥å¿—æ–‡ä»¶

# 1. å®šä¹‰ Formatter (ä¸æ‚¨çš„æ§åˆ¶å°å’Œæ–‡ä»¶æ—¥å¿—æ ¼å¼ä¿æŒä¸€è‡´)
formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')

# 2. è·å– Root Logger
# æ³¨æ„ï¼šè¿™å°†æ›¿æ¢æ‰€æœ‰å·²å­˜åœ¨çš„é…ç½®
root_logger = logging.getLogger()
root_logger.setLevel(logging.INFO)

# --- é‡æ–°æ·»åŠ åŸæœ‰çš„ Handlers ---

# 3. Handler 1: logger çš„æ—¥å¿—å†™å…¥ app.log
file_handler = logging.FileHandler(log_file_path, mode='w', encoding='utf-8')
file_handler.setFormatter(formatter)
root_logger.addHandler(file_handler)

# 4. Handler 2: logger çš„æ—¥å¿—åŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°
stream_handler = logging.StreamHandler(sys.stdout)
stream_handler.setFormatter(formatter)
root_logger.addHandler(stream_handler)

# --- ã€æ–°å¢ã€‘å‰ç«¯æ—¥å¿—æ•è· Handler ---

# 5. Handler 3: æ•è·æ—¥å¿—åˆ°å‰ç«¯å­˜å‚¨ (LOG_HISTORY)
frontend_handler = FrontendLogHandler()
frontend_handler.setLevel(logging.INFO) # åªæ•è· INFO åŠä»¥ä¸Šæ—¥å¿—
frontend_handler.setFormatter(formatter) # ä½¿ç”¨ç›¸åŒçš„æ ¼å¼
root_logger.addHandler(frontend_handler)

logger = logging.getLogger(__name__)
# ================== æ ¸å¿ƒï¼šå¯ç”¨ print() é‡å®šå‘ ==================
# åˆ›å»ºä¸€ä¸ª Logger å®ä¾‹æ¥æ•è·æ‰€æœ‰ print() è¾“å‡º
print_logger = Logger(filename=print_capture_file_path, mode="a", encoding="utf-8")

# å°† stdout å’Œ stderr é‡å®šå‘åˆ°è¿™ä¸ªå®ä¾‹
sys.stdout = print_logger
sys.stderr = print_logger

PROJECTS_DIR = "projects"
WAV_DIR = "wav"
OUTPUT_DIR = "output"
TEMP_DIR = "temp_prompts"
EMO_PROMPTS_DIR = "emo_prompts"
for dir_path in [PROJECTS_DIR, WAV_DIR, OUTPUT_DIR, TEMP_DIR, EMO_PROMPTS_DIR]:
    os.makedirs(dir_path, exist_ok=True)

app = FastAPI(title="AI Voice Studio Pro - Backend Service")
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"])

app.mount("/emo_prompts", StaticFiles(directory=EMO_PROMPTS_DIR), name="emo_prompts")

CATEGORIES_FILE = os.path.join(WAV_DIR, 'timbre_categories.json')

TTS_TAIL_ANALYSIS_DURATION_MS = 20  # åˆ†æç»“å°¾å¤šå°‘æ¯«ç§’çš„éŸ³é¢‘
TTS_TAIL_ENERGY_THRESHOLD_DBFS = -30  # èƒ½é‡é˜ˆå€¼ï¼Œé«˜äºæ­¤å€¼åˆ™åˆ¤å®šä¸ºå¯èƒ½æˆªæ–­
TTS_GENERATION_MAX_RETRIES = 5  # åç«¯ TTS ç”Ÿæˆçš„å†…éƒ¨æœ€å¤§é‡è¯•æ¬¡æ•°

# =================================================================
#               API MODELS (Pydantic)
# =================================================================
class MergeCharactersRequest(BaseModel):
    novel_name: str
    target_name: str
    source_names: List[str]
    chapter_files: List[str] 
    
class ChapterTxt(BaseModel):
    id: int
    title: str
    content: str

class ProcessTxtRequest(BaseModel):
    novel_name: str
    chapter_titles: List[str]

class TTSRequestV2(BaseModel):
    novel_name: str; chapter_name: str; line_identifier: str; speaker: str; timbre: str;
    tts_text: str
    inference_mode: str; instruct_text: Optional[str] = None
    tts_model: Optional[str] = None
    tone: Optional[str] = None
    intensity: Optional[int] = None
    emo_audio_prompt: Optional[str] = None  
    emo_weight: Optional[int] = None

class SpliceRequest(BaseModel):
    novel_name: str; chapter_name: str; wav_files: List[str]

class CharactersInChaptersRequest(BaseModel):
    novel_name: str; chapter_files: List[str]

class UpdateConfigRequest(BaseModel):
    novel_name: str; config_data: dict

class UpdateChapterRequest(BaseModel):
    filepath: str
    content: List[Dict]
    
class SearchSentencesRequest(BaseModel):
    novel_name: str
    character_name: str
    chapter_titles: List[str]
    
class EffectRequest(BaseModel):
    novel_name: str
    chapter_name: str
    file_name: str
    effect_type: str
    
class DownloadRequest(BaseModel):
    file_paths: List[str]    
    
class ProcessSingleChapterRequest(BaseModel):
    novel_name: str
    chapter_title: str    
    model_name: Optional[str] = None # model_name åœ¨é¢„è§ˆæ—¶ä¸æ˜¯å¿…éœ€çš„
    force_regenerate: Optional[bool] = False
    preview_only: Optional[bool] = False # æ–°å¢å­—æ®µ

class ChoralRequest(BaseModel):
    novel_name: str
    chapter_name: str
    line_identifier: str
    tts_text: str
    selected_timbres: List[str]  
    original_speaker: str  # åŸå§‹éŸ³é¢‘çš„è§’è‰²å
    original_timbre: str   # åŸå§‹éŸ³é¢‘çš„éŸ³è‰²å  
    tts_model: Optional[str] = None # <-- æ–°å¢
    
class STTResponse(BaseModel):
    status: str
    text: Optional[str] = None
    message: Optional[str] = None
    
class CreateCategoryRequest(BaseModel):
    category_name: str

class SetTimbreCategoryRequest(BaseModel):
    timbre_name: str
    category_name: str

class ReplaceRule(BaseModel):
    original_word: str
    replacement_word: str
    description: Optional[str] = None

class UpdateReplaceDictRequest(BaseModel):
    rules: List[ReplaceRule]
    
class RenameCharacterRequest(BaseModel):
    novel_name: str
    old_name: str
    new_name: str
    chapter_files: List[str]
    
class CheckFilesRequest(BaseModel):
    novel_name: str
    chapter_name: str
    filenames: List[str]
    
class AddCharacterRequest(BaseModel):
    novel_name: str
    character_name: str
    
# =================================================================
#               HELPER FUNCTIONS
# =================================================================
PROMPT_TEMPLATE = """
# Role
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šæœ‰å£°ä¹¦è„šæœ¬ç¼–è¾‘ï¼Œä¸ä»…æ“…é•¿æƒ…æ„Ÿæ ‡æ³¨ï¼Œæ›´å…·å¤‡**é€å­—æ ¡å¯¹**èƒ½åŠ›ã€‚

# Goals
å°†å°è¯´æ–‡æœ¬è½¬æ¢ä¸ºæœ‰å£°ä¹¦JSONæ•°ç»„ï¼Œ**ç»å¯¹ä¸é—æ¼ä»»ä½•æ±‰å­—**ã€‚

# Rules
1. **é›¶é—æ¼åŸåˆ™ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰**ï¼š
   - åŸæ–‡ä¸­çš„æ¯ä¸€ä¸ªæ±‰å­—ï¼ˆåŒ…æ‹¬â€œä»–è¯´é“â€ã€â€œç¬‘ç€è¯´â€ç­‰æç¤ºè¯­ï¼‰éƒ½å¿…é¡»å‡ºç°åœ¨ content å­—æ®µä¸­ã€‚å¦‚æœå¼€å¤´ä¸€è¡Œæ˜¯ç±»ä¼¼ç¬¬*ç«  *****ä¹‹ç±»çš„ä¸€å®šæ˜¯æ—ç™½è¯´çš„ã€‚
   - ä¸¥ç¦å°†åŸæ–‡å½“ä½œå…ƒæ•°æ®å¤„ç†ï¼Œ**ä¸¥ç¦çœç•¥**ã€‚

2. **ç»“æ„æ‹†åˆ†ï¼ˆé˜²åå­—å…³é”®ï¼‰**ï¼š
   - é‡åˆ°ã€æ—ç™½æè¿°+äººç‰©å¯¹è¯ã€‘çš„æ··åˆæ®µè½ï¼ˆä¾‹å¦‚ï¼š`å°¼è«è¯´é“ï¼šâ€œ...â€`ï¼‰ï¼Œå¿…é¡»**æ‹†åˆ†**ä¸ºä¸¤ä¸ªå¯¹è±¡ï¼š
     1. æ—ç™½å¯¹è±¡ -> content: "å°¼è«è¯´é“ï¼š"
     2. è§’è‰²å¯¹è±¡ -> content: "..."
   - æ ‡ç‚¹ç¬¦å·ï¼ˆå¦‚å†’å·ã€é€—å·ï¼‰è·Ÿéšå‰åŠéƒ¨åˆ†çš„æ—ç™½ã€‚

3. **è§’è‰²ä¸æƒ…æ„Ÿ**ï¼š
   - å¼•å·`""`å†… -> å¯¹åº”è§’è‰²ï¼Œæ ¹æ®è¯­å¢ƒé€‰ Toneã€‚
   - å¼•å·å¤– -> å¯¹åº”"æ—ç™½"ï¼ŒTone ç»Ÿä¸€å®šä¸º "å¹³é™"ï¼Œintensity è®¾ä¸º 5ã€‚
   - Tone ä»…é™ï¼š[å–œ, æ€’, å“€, æƒ§, åŒæ¶, ä½è½, æƒŠå–œ, å¹³é™]ã€‚

4. **æ ¼å¼è§„èŒƒ**ï¼š
   - content å»é™¤ä¸æˆå¯¹çš„å­¤ç«‹å¼•å·ã€‚
   - ä»…è¾“å‡ºçº¯ JSON æ•°ç»„ï¼Œæ—  Markdown æ ‡è®°ã€‚

# Output Example
åŸæ–‡ï¼šå°¼è«è‡ªè¨€è‡ªè¯­é“ï¼šâ€œå› ä¸ºæˆ‘ç­”åº”äº†ã€‚â€
JSONï¼š
[
  {"speaker": "æ—ç™½", "content": "å°¼è«è‡ªè¨€è‡ªè¯­é“ï¼š", "tone": "å¹³é™", "intensity": 5},
  {"speaker": "å°¼è«", "content": "å› ä¸ºæˆ‘ç­”åº”äº†ã€‚", "tone": "ä½è½", "intensity": 3}
]
(ä¸¥ç¦è¾“å‡º Markdown æ ‡è®°æˆ–ä»»ä½•è§£é‡Šæ€§æ–‡å­—ï¼Œä»…è¾“å‡ºçº¯ JSON)
è¯·ä¸¥æ ¼éµå¾ªä»¥ä¸Šæ‰€æœ‰è§„åˆ™è¿›è¡Œå›å¤ã€‚
ä»¥ä¸‹æ˜¯éœ€è¦è½¬æ¢çš„å°è¯´ç« èŠ‚å†…å®¹ï¼š"""
MAX_RETRIES = 3
MODEL_NAME = "gemini-2.5-flash"
SAFETY_SETTINGS = [
    {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
]
CONFIG_FILE = "config.json"
def initialize_llm_config():
    default_config = {
        "general": {
            "default_model": "gemini",
            "proxy": {
                "enabled": False,
                "protocol": "socks5h",
                "address": "127.0.0.1",
                "port": "1080"
            },
            "default_tts_model": "cosyvoice_v2"
        },
        "audio_export": {
            "format": "mp3",
            "quality": "256k"
        },
        "tts_models": { # æ–°å¢
            "cosyvoice_v2": {
                "display_name": "CosyVoice2",
                "endpoint": "http://127.0.0.1:5010/api/tts",
                "default_mode": "zero_shot"
            },
            "indextts_v2": {
                "display_name": "IndexTTS2",
                "endpoint": "http://127.0.0.1:5020/api/tts",
                "default_mode": "cross_lingual"
            }
        },
        "models": {
            "gemini": {
                "display_name": "Gemini",
                "model_name": "gemini-2.5-flash",
                "api_key": "",
                "max_chars": 8000,
                "use_proxy": True
            },
            "aliyun": {
                "display_name": "é˜¿é‡Œäº‘å¹³å°",
                "model_name": "deepseek-r1",
                "api_key": "",
                "max_chars": 6000,
                "use_proxy": False
            },
            "volcengine": {
                "display_name": "ç«å±±å¼•æ“",
                "model_name": "deepseek-v3-1-terminus",
                "api_key": "",
                "max_chars": 8000,
                "use_proxy": False
            },
            "chatanywhere": {
                "display_name": "ChatAnywhere",
                "model_name": "gemini-2.5-flash",
                "api_key": "",
                "max_chars": 8000,
                "use_proxy": False
            },
            "ollama": {  # æ³¨æ„ï¼šè¿™é‡Œå»ºè®®å°†keyä»chatanywhereæ”¹ä¸ºollamaï¼Œé¿å…æ··æ·†
                "display_name": "Ollama",  # æ˜¾ç¤ºåç§°
                "model_name": "qwen3-coder:480b-cloud",  # å®˜æ–¹ç¤ºä¾‹ä¸­çš„æ¨¡å‹
                "api_key": "",  # ç”¨äºå¡«å†™OLLAMA_API_KEY
                "max_chars": 8000,  # æœ€å¤§å¤„ç†å­—ç¬¦æ•°
                "use_proxy": False,  # æ˜¯å¦ä½¿ç”¨ä»£ç†
                "stream": True  # æ˜¯å¦å¯ç”¨æµå¼å“åº”ï¼ˆå®˜æ–¹ç¤ºä¾‹ä½¿ç”¨stream=Trueï¼‰
            }
        },
        "elevenlabs": {
            "api_key": ""
        }
    }
    if not os.path.exists(CONFIG_FILE):
        with open(CONFIG_FILE, 'w', encoding='utf-8') as f:
            json.dump(default_config, f, ensure_ascii=False, indent=4)
        logger.info(f"'{CONFIG_FILE}' æœªæ‰¾åˆ°ï¼Œå·²åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶ã€‚")

@app.get("/api/get_llm_config")
async def get_llm_config():
    if not os.path.exists(CONFIG_FILE):
        raise HTTPException(status_code=404, detail="LLMé…ç½®æ–‡ä»¶æœªæ‰¾åˆ°ã€‚")
    return FileResponse(CONFIG_FILE)

class LLMConfigRequest(BaseModel):
    config: Dict

@app.get("/api/latest_logs")
async def get_latest_logs():
    """è¿”å›æœ€è¿‘ N æ¡æ—¥å¿—è®°å½•ï¼Œæœ€æ–°çš„æ—¥å¿—æ’åœ¨æœ€å‰é¢ã€‚"""
    # å°† deque è½¬æ¢ä¸ºåˆ—è¡¨å¹¶åè½¬ï¼Œç¡®ä¿æœ€æ–°çš„æ—¥å¿—åœ¨åˆ—è¡¨çš„ç¬¬ä¸€ä¸ªä½ç½®
    return {"logs": list(LOG_HISTORY)[::-1]}

@app.post("/api/update_llm_config")
async def update_llm_config(req: LLMConfigRequest):
    try:
        with open(CONFIG_FILE, 'w', encoding='utf-8') as f:
            json.dump(req.config, f, ensure_ascii=False, indent=4)
        return {"status": "success", "message": "æ¨¡å‹é…ç½®å·²æˆåŠŸä¿å­˜ã€‚"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"å†™å…¥é…ç½®æ–‡ä»¶å¤±è´¥: {e}")

async def generate_with_volcengine(chapter_content: str, model_id: str, api_key: str, max_chars: int, proxies: Optional[Dict]) -> List[Dict]:
    text_chunks = smart_chunk_text(chapter_content, max_length=max_chars)
    all_json_parts = []
    
    async def process_chunk(chunk_text, index):
        """Inner function to process a single chunk for Volcengine."""
        chunk_prompt = PROMPT_TEMPLATE + '\n\n' + chunk_text
        api_url = "https://ark.cn-beijing.volces.com/api/v3/chat/completions" # Volcengine Ark API endpoint
        headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}
        payload = {
            "model": model_id,
            "messages": [
                {"role": "system", "content": "You are a helpful assistant that strictly follows user instructions and outputs JSON arrays."},
                {"role": "user", "content": chunk_prompt}
            ]
        }
        
        for attempt in range(MAX_RETRIES):
            logger.info(f"    - (Volcengine API Call) Processing chunk {index + 1}/{len(text_chunks)}, Attempt {attempt + 1}...")
            try:
                response = requests.post(api_url, headers=headers, json=payload, proxies=proxies, timeout=300)
                response.raise_for_status()
                response_data = response.json()
                
                if not response_data.get('choices') or not response_data['choices']: # ç¡®ä¿ choices å­˜åœ¨ä¸”ä¸ä¸ºç©º
                    logger.warning(f"    - (Volcengine API Call) Chunk {index + 1}, Attempt {attempt + 1}: Volcengine API response has no choices or empty choices: {json.dumps(response_data)}")
                    raise ValueError(f"Volcengine APIå“åº”æ— æ•ˆ: {json.dumps(response_data)}")

                choice_obj = response_data['choices'][0] # è·å–åˆ—è¡¨çš„ç¬¬ä¸€ä¸ªå­—å…¸å…ƒç´  (è¿™æ˜¯ä¸€ä¸ªå­—å…¸)

                finish_reason = choice_obj.get('finish_reason') # ä» choice_obj (å­—å…¸) ä¸­è·å– finish_reason
                if finish_reason != "stop":
                    raise ValueError(f"Volcengine æ¨¡å‹ç”Ÿæˆå¼‚å¸¸ï¼ŒåŸå› : {finish_reason}ã€‚")

                message_content = choice_obj.get("message", {}).get("content") # ä» choice_obj (å­—å…¸) ä¸­è·å– message.content
                if message_content is None:
                    raise ValueError("Volcengine APIå“åº”ä¸­ 'choices[0].message.content' å­—æ®µç¼ºå¤±ã€‚")

                # Volcengine Ark å¹³å°çš„æ¨¡å‹é€šå¸¸ç›´æ¥è¿”å› JSON å­—ç¬¦ä¸²ï¼Œè€Œä¸æ˜¯åŒ…è£¹åœ¨ ```json``` é‡Œ
                parsed_json = validate_and_parse_json_array(message_content)
                if parsed_json is not None:
                    logger.info(f"    - (Volcengine API Call) Chunk {index + 1} Succeeded.")
                    return parsed_json
                else:
                    raise ValueError(f"Volcengine è¿”å›å†…å®¹ä¸æ˜¯ä¸€ä¸ªå®Œæ•´çš„JSONæ•°ç»„ã€‚åŸå§‹å†…å®¹: {message_content[:200]}...") # å¢åŠ åŸå§‹å†…å®¹æç¤º
            except Exception as e:
                logger.warning(f"    - (Volcengine API Call) Chunk {index + 1} Attempt {attempt + 1} Failed: {e}")
                if attempt < MAX_RETRIES - 1:
                    await asyncio.sleep(5)
                else:
                    return None
    
    for i, chunk in enumerate(text_chunks):
        json_part = await process_chunk(chunk, i)
        if json_part:
            all_json_parts.extend(json_part)
        else:
            logger.error(f"Volcengine Chunk {i+1} failed to process after all retries. Skipping this chunk.")
            
    if not all_json_parts and text_chunks:
        raise Exception("æ‰€æœ‰æ–‡æœ¬å—éƒ½æœªèƒ½æˆåŠŸå¤„ç†ã€‚")
        
    logger.info(f"Volcengine æ‰€æœ‰å—å¤„ç†å®Œæ¯•ï¼Œåˆå¹¶äº† {len(all_json_parts)} æ¡JSONè®°å½•ã€‚")
    return all_json_parts
    
async def generate_with_qwen(chapter_content: str, model_id: str, api_key: str, max_chars: int, proxies: Optional[Dict]) -> List[Dict]:
    text_chunks = smart_chunk_text(chapter_content, max_length=max_chars)
    all_json_parts = []
    
    async def process_chunk(chunk_text, index):
        """Inner function to process a single chunk."""
        chunk_prompt = PROMPT_TEMPLATE + '\n\n' + chunk_text
        api_url = "https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation"
        headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}
        payload = {
            "model": model_id,
            "input": {"messages": [{"role": "system", "content": "You are a helpful assistant that strictly follows user instructions."}, {"role": "user", "content": chunk_prompt}]},
            "parameters": {"result_format": "message"}
        }
        
        for attempt in range(MAX_RETRIES):
            logger.info(f"    - (Qwen API Call) Processing chunk {index + 1}/{len(text_chunks)}, Attempt {attempt + 1}...")
            try:
                response = requests.post(api_url, headers=headers, json=payload, proxies=proxies, timeout=300)
                response.raise_for_status()
                response_data = response.json()
                
                if response_data.get("output", {}).get("choices"):
                    message = response_data["output"]["choices"][0].get("message", {})
                    api_text = message.get("content")
                    if api_text:
                        if api_text.strip().startswith("```json"):
                            api_text = api_text.strip()[7:-3].strip()
                        parsed_json = validate_and_parse_json_array(api_text)
                        if parsed_json is not None:
                            return parsed_json
                raise ValueError(f"APIå“åº”æ— æ•ˆæˆ–å†…å®¹æ ¼å¼ä¸æ­£ç¡®: {json.dumps(response_data)}")
            except Exception as e:
                logger.warning(f"    - (Qwen API Call) Chunk {index + 1} Attempt {attempt + 1} Failed: {e}")
                if attempt < MAX_RETRIES - 1:
                    await asyncio.sleep(5)
                else:
                    return None
    
    for i, chunk in enumerate(text_chunks):
        json_part = await process_chunk(chunk, i)
        if json_part:
            all_json_parts.extend(json_part)
        else:
            logger.error(f"Chunk {i+1} failed to process after all retries. Skipping this chunk.")
            
    if not all_json_parts and text_chunks:
        raise Exception("æ‰€æœ‰æ–‡æœ¬å—éƒ½æœªèƒ½æˆåŠŸå¤„ç†ã€‚")
        
    logger.info(f"æ‰€æœ‰å—å¤„ç†å®Œæ¯•ï¼Œåˆå¹¶äº† {len(all_json_parts)} æ¡JSONè®°å½•ã€‚")
    return all_json_parts

async def generate_with_gemini(chapter_content: str, model_id: str, api_key: str, max_chars: int, proxies: Optional[Dict]) -> List[Dict]:
    text_chunks = smart_chunk_text(chapter_content, max_length=max_chars)
    all_json_parts = []

    async def process_chunk(chunk_text, index):
        chunk_prompt = PROMPT_TEMPLATE + '\n\n' + chunk_text
        api_url = f"https://gemini.yingakki.dpdns.org/v1beta/models/{model_id}:generateContent"
        headers = {'Content-Type': 'application/json', 'x-goog-api-key': api_key}
        payload = {"contents": [{"parts": [{"text": chunk_prompt}]}], "safetySettings": SAFETY_SETTINGS, "generationConfig": {"response_mime_type": "application/json"}}

        for attempt in range(MAX_RETRIES):
            logger.info(f"    - (Gemini API Call) Processing chunk {index + 1}/{len(text_chunks)}, Attempt {attempt + 1}...")
            try:
                response = requests.post(api_url, headers=headers, json=payload, proxies=proxies, timeout=300)
                response.raise_for_status()
                response_data = response.json()
                prompt_feedback = response_data.get("promptFeedback")
                if prompt_feedback and prompt_feedback.get("blockReason") == "PROHIBITED_CONTENT":
                    logger.warning(f"    - (Gemini API Call) Chunk {index + 1}, Attempt {attempt + 1}: Content blocked by Gemini safety policy: PROHIBITED_CONTENT. Returning placeholder.")
                    # è¿”å›ä¸€ä¸ªåŒ…å«ç‰¹æ®Šå ä½ç¬¦çš„ JSON æ•°ç»„ï¼Œè·³è¿‡é‡è¯•
                    return [{"speaker": "æ—ç™½", "content": "ï¼ˆç”±äºå†…å®¹å®‰å…¨å®¡æŸ¥ï¼Œæ­¤å¤„æ–‡æœ¬æœªèƒ½ç”Ÿæˆï¼‰", "tone": "æ­£å¸¸", "intensity": 5, "delay": 500}]
                if not response_data.get('candidates'):
                    logger.warning(f"    - (Gemini API Call) Chunk {index + 1}, Attempt {attempt + 1}: API response has no candidates: {json.dumps(response_data)}")
                    raise ValueError(f"APIå“åº”æ— æ•ˆ: {json.dumps(response_data)}")
                candidate = response_data['candidates'][0]
                finish_reason = candidate.get('finishReason')
                if finish_reason != "STOP": raise ValueError(f"æ¨¡å‹ç”Ÿæˆå¼‚å¸¸ï¼ŒåŸå› : {finish_reason}ã€‚")
                api_text = candidate.get('content', {}).get('parts', [{}])[0].get('text')
                if api_text is None: raise ValueError("APIå“åº”ä¸­ 'content.parts.text' å­—æ®µç¼ºå¤±ã€‚")
                parsed_json = validate_and_parse_json_array(api_text)
                if parsed_json is not None:
                    logger.info(f"    - (Gemini API Call) Chunk {index + 1} Succeeded.")
                    return parsed_json
                else:
                    raise ValueError(f"è¿”å›å†…å®¹ä¸æ˜¯ä¸€ä¸ªå®Œæ•´çš„JSONæ•°ç»„ã€‚")
            except Exception as e:
                logger.warning(f"    - (Gemini API Call) Chunk {index + 1} Attempt {attempt + 1} Failed: {e}")
                if attempt < MAX_RETRIES - 1: await asyncio.sleep(5)
        return None

    tasks = [process_chunk(chunk, i) for i, chunk in enumerate(text_chunks)]
    results = await asyncio.gather(*tasks)
    for result in results:
        if result: all_json_parts.extend(result)

    if not all_json_parts and text_chunks: raise Exception("æ‰€æœ‰æ–‡æœ¬å—éƒ½æœªèƒ½æˆåŠŸå¤„ç†ã€‚")
    return all_json_parts    

async def generate_with_chatanywhere(chapter_content: str, model_id: str, api_key: str, max_chars: int, proxies: Optional[Dict]) -> List[Dict]:
    text_chunks = smart_chunk_text(chapter_content, max_length=max_chars)
    all_json_parts = []
    
    async def process_chunk(chunk_text, index):
        """å¤„ç†å•ä¸ªæ–‡æœ¬å—çš„ChatAnywhere APIè°ƒç”¨"""
        chunk_prompt = PROMPT_TEMPLATE + '\n\n' + chunk_text
        api_url = "https://api.chatanywhere.tech/v1/chat/completions"
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        payload = {
            "model": model_id,
            "messages": [
                {"role": "system", "content": "You are a helpful assistant that strictly follows user instructions and outputs JSON arrays."},
                {"role": "user", "content": chunk_prompt}
            ],
            "temperature": 0.7
        }
        
        for attempt in range(MAX_RETRIES):
            logger.info(f"    - (ChatAnywhere API Call) Processing chunk {index + 1}/{len(text_chunks)}, Attempt {attempt + 1}...")
            try:
                response = requests.post(api_url, headers=headers, json=payload, proxies=proxies, timeout=300)
                response.raise_for_status()
                response_data = response.json()
                
                if not response_data.get('choices'):
                    logger.warning(f"    - (ChatAnywhere API Call) No choices in response: {json.dumps(response_data)}")
                    raise ValueError("APIå“åº”ä¸åŒ…å«ç”Ÿæˆç»“æœ")

                choice = response_data['choices'][0]
                finish_reason = choice.get('finish_reason')
                if finish_reason != "stop":
                    raise ValueError(f"ç”Ÿæˆæœªæ­£å¸¸ç»“æŸï¼ŒåŸå› : {finish_reason}")

                message_content = choice.get('message', {}).get('content')
                if not message_content:
                    raise ValueError("å“åº”å†…å®¹ä¸ºç©º")

                # å¤„ç†å¯èƒ½åŒ…å«çš„ä»£ç å—æ ‡è®°
                if message_content.strip().startswith("```json"):
                    message_content = message_content.strip()[7:-3].strip()

                parsed_json = validate_and_parse_json_array(message_content)
                if parsed_json is not None:
                    logger.info(f"    - (ChatAnywhere API Call) Chunk {index + 1} Succeeded.")
                    return parsed_json
                else:
                    raise ValueError(f"è¿”å›å†…å®¹ä¸æ˜¯æœ‰æ•ˆçš„JSONæ•°ç»„: {message_content[:200]}...")

            except Exception as e:
                logger.warning(f"    - (ChatAnywhere API Call) Chunk {index + 1} Attempt {attempt + 1} Failed: {e}")
                if attempt < MAX_RETRIES - 1:
                    await asyncio.sleep(5)
        return None
    
    # å¤„ç†æ‰€æœ‰æ–‡æœ¬å—
    for i, chunk in enumerate(text_chunks):
        json_part = await process_chunk(chunk, i)
        if json_part:
            all_json_parts.extend(json_part)
        else:
            logger.error(f"ChatAnywhere Chunk {i+1} å¤„ç†å¤±è´¥ï¼Œå·²è·³è¿‡")
            
    if not all_json_parts and text_chunks:
        raise Exception("æ‰€æœ‰æ–‡æœ¬å—å¤„ç†å¤±è´¥")
        
    logger.info(f"ChatAnywhere å¤„ç†å®Œæˆï¼Œå…±åˆå¹¶ {len(all_json_parts)} æ¡è®°å½•")
    return all_json_parts

async def generate_with_ollama(chapter_content: str, model_id: str, api_key: str, max_chars: int, proxies: Optional[Dict]) -> List[Dict]:
    text_chunks = smart_chunk_text(chapter_content, max_length=max_chars)
    all_json_parts = []
    
    # åˆå§‹åŒ–Ollamaå®¢æˆ·ç«¯ï¼ˆå¯¹åº”å®˜æ–¹çš„Clienté…ç½®ï¼‰
    client = Client(
        host="https://ollama.com",  # å®˜æ–¹APIåœ°å€
        headers={'Authorization': f'Bearer {api_key}'}  # ä»å‚æ•°è·å–APIå¯†é’¥ï¼ˆæ›¿ä»£ç¯å¢ƒå˜é‡ï¼‰
    )
    
    async def process_chunk(chunk_text, index):
        """å¤„ç†å•ä¸ªæ–‡æœ¬å—çš„Ollama APIè°ƒç”¨"""
        chunk_prompt = PROMPT_TEMPLATE + '\n\n' + chunk_text
        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨ï¼ˆä¸åŸä»£ç çš„messagesç»“æ„ä¸€è‡´ï¼‰
        messages = [
            {"role": "system", "content": "You are a helpful assistant that strictly follows user instructions and outputs JSON arrays."},
            {"role": "user", "content": chunk_prompt}
        ]
        
        for attempt in range(MAX_RETRIES):
            logger.info(f"    - (Ollama API Call) Processing chunk {index + 1}/{len(text_chunks)}, Attempt {attempt + 1}...")
            try:
                # è°ƒç”¨Ollamaçš„chatæ¥å£ï¼ˆå¯ç”¨æµå¼å“åº”ï¼Œä¸å®˜æ–¹ç¤ºä¾‹ä¸€è‡´ï¼‰
                response_content = ""
                for part in client.chat(model_id, messages=messages, stream=True):
                    # æ‹¼æ¥æµå¼è¿”å›çš„å†…å®¹
                    response_content += part['message'].get('content', '')
                
                if not response_content:
                    raise ValueError("å“åº”å†…å®¹ä¸ºç©º")
                
                # å¤„ç†å¯èƒ½åŒ…å«çš„ä»£ç å—æ ‡è®°ï¼ˆä¸åŸä»£ç é€»è¾‘ä¸€è‡´ï¼‰
                if response_content.strip().startswith("```json"):
                    response_content = response_content.strip()[7:-3].strip()
                
                # éªŒè¯å¹¶è§£æJSONï¼ˆå¤ç”¨åŸä»£ç çš„å·¥å…·å‡½æ•°ï¼‰
                parsed_json = validate_and_parse_json_array(response_content)
                if parsed_json is not None:
                    logger.info(f"    - (Ollama API Call) Chunk {index + 1} Succeeded.")
                    return parsed_json
                else:
                    raise ValueError(f"è¿”å›å†…å®¹ä¸æ˜¯æœ‰æ•ˆçš„JSONæ•°ç»„: {response_content[:200]}...")

            except Exception as e:
                logger.warning(f"    - (Ollama API Call) Chunk {index + 1} Attempt {attempt + 1} Failed: {e}")
                if attempt < MAX_RETRIES - 1:
                    await asyncio.sleep(5)  # é‡è¯•é—´éš”
        return None
    
    # å¤„ç†æ‰€æœ‰æ–‡æœ¬å—ï¼ˆä¸åŸä»£ç é€»è¾‘ä¸€è‡´ï¼‰
    for i, chunk in enumerate(text_chunks):
        json_part = await process_chunk(chunk, i)
        if json_part:
            all_json_parts.extend(json_part)
        else:
            logger.error(f"Ollama Chunk {i+1} å¤„ç†å¤±è´¥ï¼Œå·²è·³è¿‡")
            
    if not all_json_parts and text_chunks:
        raise Exception("æ‰€æœ‰æ–‡æœ¬å—å¤„ç†å¤±è´¥")
        
    logger.info(f"Ollama å¤„ç†å®Œæˆï¼Œå…±åˆå¹¶ {len(all_json_parts)} æ¡è®°å½•")
    return all_json_parts

@app.get("/api/emo_prompts/list")
async def list_emo_prompts():
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æƒ…ç»ªå‚è€ƒéŸ³é¢‘æ–‡ä»¶ã€‚"""
    if not os.path.isdir(EMO_PROMPTS_DIR):
        return {"emo_prompts": []}
    try:
        files = [f for f in os.listdir(EMO_PROMPTS_DIR) if f.lower().endswith(('.wav', '.mp3', '.m4a', '.ogg'))]
        return {"emo_prompts": sorted(files)}
    except Exception as e:
        logger.error(f"æ— æ³•åˆ—å‡ºæƒ…ç»ªå‚è€ƒéŸ³é¢‘: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="æœåŠ¡å™¨æ— æ³•è¯»å–æƒ…ç»ªå‚è€ƒéŸ³é¢‘ç›®å½•ã€‚")

@app.post("/api/emo_prompts/upload")
async def upload_emo_prompt(file: UploadFile = File(...)):
    """ä¸Šä¼ ä¸€ä¸ªæ–°çš„æƒ…ç»ªå‚è€ƒéŸ³é¢‘ã€‚"""
    # ä½¿ç”¨å®‰å…¨çš„æ–‡ä»¶å
    safe_filename = "".join(c for c in file.filename if c.isalnum() or c in "._-").rstrip()
    if not safe_filename:
        raise HTTPException(status_code=400, detail="æ— æ•ˆçš„æ–‡ä»¶åã€‚")
        
    file_path = os.path.join(EMO_PROMPTS_DIR, safe_filename)
    if os.path.exists(file_path):
        raise HTTPException(status_code=409, detail=f"æ–‡ä»¶ '{safe_filename}' å·²å­˜åœ¨ã€‚")
    
    try:
        with open(file_path, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
        # å¯é€‰ï¼šåœ¨è¿™é‡Œæ·»åŠ éŸ³é‡æ ‡å‡†åŒ–ç­‰å¤„ç†
        return {"status": "success", "message": f"æƒ…ç»ªå‚è€ƒéŸ³é¢‘ '{safe_filename}' å·²æˆåŠŸä¸Šä¼ ã€‚"}
    except Exception as e:
        logger.error(f"ä¸Šä¼ æƒ…ç»ªå‚è€ƒéŸ³é¢‘å¤±è´¥: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"ä¸Šä¼ æ–‡ä»¶æ—¶æœåŠ¡å™¨å‘ç”Ÿé”™è¯¯: {e}")

@app.delete("/api/emo_prompts/delete")
async def delete_emo_prompt(filename: str):
    """åˆ é™¤ä¸€ä¸ªæƒ…ç»ªå‚è€ƒéŸ³é¢‘ã€‚"""
    # å®‰å…¨æ€§æ£€æŸ¥
    if ".." in filename or filename.startswith('/'):
        raise HTTPException(status_code=400, detail="æ— æ•ˆçš„æ–‡ä»¶åã€‚")
        
    file_path = os.path.join(EMO_PROMPTS_DIR, filename)
    if not os.path.exists(file_path):
        raise HTTPException(status_code=404, detail=f"æ–‡ä»¶ '{filename}' æœªæ‰¾åˆ°ã€‚")
    
    try:
        os.remove(file_path)
        return {"status": "success", "message": f"æƒ…ç»ªå‚è€ƒéŸ³é¢‘ '{filename}' å·²è¢«åˆ é™¤ã€‚"}
    except Exception as e:
        logger.error(f"åˆ é™¤æƒ…ç»ªå‚è€ƒéŸ³é¢‘å¤±è´¥: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"åˆ é™¤æ–‡ä»¶æ—¶æœåŠ¡å™¨å‘ç”Ÿé”™è¯¯: {e}")
        
# +++ æ–°å¢/æ›¿æ¢ï¼šä¸€ä¸ªä¸å‰ç«¯JSä¸¥æ ¼åŒæ­¥çš„å®‰å…¨åŒ–å‡½æ•° +++
def sanitize_for_filename(text: str) -> str:
    """
    A sanitizer that matches the JS frontend logic for WAV filenames.
    Python: "".join(c for c in text if c.isalnum() or c in " _-").rstrip()
    This keeps letters, numbers, underscore, hyphen, and space.
    """
    if not text:
        return ""
    # ç§»é™¤éå­—æ¯ã€éæ•°å­—ã€éç©ºæ ¼ã€éä¸‹åˆ’çº¿ã€éè¿å­—ç¬¦çš„æ‰€æœ‰å­—ç¬¦
    return "".join(c for c in text if c.isalnum() or c in " _-").rstrip()
    
# +++ æ–°å¢ï¼šé‡å‘½åè§’è‰²APIæ¥å£ +++
@app.post("/api/rename_character")
async def rename_character(req: RenameCharacterRequest):
    """
    Renames a character within the specified chapter JSON files.
    """
    novel_name = req.novel_name
    old_name = req.old_name
    new_name = req.new_name
    chapter_files = req.chapter_files

    if not all([novel_name, old_name, new_name, chapter_files]):
        raise HTTPException(status_code=400, detail="è¯·æ±‚å‚æ•°ä¸å®Œæ•´ã€‚")
    
    if old_name == new_name:
        return {"status": "info", "message": "æ–°æ—§åç§°ç›¸åŒï¼Œæœªè¿›è¡Œä»»ä½•æ“ä½œã€‚"}

    json_dir = os.path.join(PROJECTS_DIR, novel_name, 'chapters_json')
    if not os.path.isdir(json_dir):
        raise HTTPException(status_code=404, detail="å°è¯´ç« èŠ‚ç›®å½•æœªæ‰¾åˆ°ã€‚")
    
    # ğŸ”§ æ·»åŠ ç¼ºå¤±çš„è·¯å¾„å®šä¹‰
    project_dir = os.path.join(PROJECTS_DIR, novel_name)
    timbres_path = os.path.join(project_dir, 'character_timbres.json')
    profiles_path = os.path.join(project_dir, 'character_profiles.json')
    
    modified_count = 0
    try:
        for chapter_filename in chapter_files:
            if not chapter_filename.endswith('.json'):
                continue
            
            chapter_path = os.path.join(json_dir, chapter_filename)
            if not os.path.exists(chapter_path):
                logger.warning(f"é‡å‘½åæ“ä½œè·³è¿‡ä¸å­˜åœ¨çš„æ–‡ä»¶: {chapter_path}")
                continue

            file_was_modified = False
            with open(chapter_path, 'r', encoding='utf-8') as f:
                content = json.load(f)
            
            for item in content:
                if item.get('speaker') == old_name:
                    item['speaker'] = new_name
                    file_was_modified = True
            
            if file_was_modified:
                with open(chapter_path, 'w', encoding='utf-8') as f:
                    json.dump(content, f, ensure_ascii=False, indent=2)
                modified_count += 1
        
        # --- 2. ä¿®æ”¹ character_timbres.json æ–‡ä»¶ ---
        if os.path.exists(timbres_path):  # âœ… ç°åœ¨å˜é‡å·²å®šä¹‰
            with open(timbres_path, 'r', encoding='utf-8') as f: 
                timbres_config = json.load(f)
            if old_name in timbres_config:
                timbres_config[new_name] = timbres_config.pop(old_name)
                with open(timbres_path, 'w', encoding='utf-8') as f:
                    json.dump(timbres_config, f, ensure_ascii=False, indent=2)
                logger.info(f"å·²æ›´æ–°éŸ³è‰²é…ç½®æ–‡ä»¶: '{old_name}' æ›´åä¸º '{new_name}'")
        
        # --- 3. ä¿®æ”¹ character_profiles.json æ–‡ä»¶ ---
        if os.path.exists(profiles_path):  # âœ… ç°åœ¨å˜é‡å·²å®šä¹‰
            with open(profiles_path, 'r', encoding='utf-8') as f: 
                profiles_config = json.load(f)
            if old_name in profiles_config:
                profiles_config[new_name] = profiles_config.pop(old_name)
                with open(profiles_path, 'w', encoding='utf-8') as f:
                    json.dump(profiles_config, f, ensure_ascii=False, indent=4)
                logger.info(f"å·²æ›´æ–°è§’è‰²ç®€ä»‹æ–‡ä»¶: '{old_name}' æ›´åä¸º '{new_name}'")

        
        return {
            "status": "success", 
            "message": f"è§’è‰²ã€Œ{old_name}ã€å·²é‡å‘½åä¸ºã€Œ{new_name}ã€ï¼Œå…±ä¿®æ”¹äº† {modified_count} ä¸ªç« èŠ‚æ–‡ä»¶ã€‚"
        }

    except Exception as e:
        logger.error(f"é‡å‘½åè§’è‰²æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"æœåŠ¡å™¨å¤„ç†é‡å‘½åæ—¶å‘ç”Ÿé”™è¯¯: {e}")


@app.post("/api/merge_characters")
async def merge_characters(req: MergeCharactersRequest):
    """
    Merges source character names into a target name.
    If the timbre/voice for the source and target characters are the same,
    it will intelligently rename the corresponding WAV files instead of requiring regeneration.
    """
    novel_name = req.novel_name
    target_name = req.target_name
    source_names = req.source_names
    chapter_files = req.chapter_files

    if not all([novel_name, target_name, source_names]):
        raise HTTPException(status_code=400, detail="è¯·æ±‚å‚æ•°ä¸å®Œæ•´ã€‚")
    
    if target_name in source_names:
        raise HTTPException(status_code=400, detail="ç›®æ ‡åç§°ä¸èƒ½åŒ…å«åœ¨æºåç§°åˆ—è¡¨ä¸­ã€‚")

    project_dir = os.path.join(PROJECTS_DIR, novel_name)
    json_dir = os.path.join(project_dir, 'chapters_json')
    
    # ğŸ”§ åœ¨è¿™é‡Œæ·»åŠ ç¼ºå¤±çš„è·¯å¾„å®šä¹‰
    timbres_path = os.path.join(project_dir, 'character_timbres.json')
    profiles_path = os.path.join(project_dir, 'character_profiles.json')
    
    output_wav_base_dir = os.path.join(OUTPUT_DIR, novel_name, 'wavs')

    if not os.path.isdir(json_dir):
        return {"status": "success", "message": "æ²¡æœ‰æ‰¾åˆ°å¯å¤„ç†çš„ç« èŠ‚æ–‡ä»¶ã€‚"}

    # --- 1. åŠ è½½éŸ³è‰²é…ç½® ---
    character_timbres = {}
    if os.path.exists(timbres_path):  # ç°åœ¨è¿™ä¸ªå˜é‡å·²å®šä¹‰
        with open(timbres_path, 'r', encoding='utf-8') as f:
            character_timbres = json.load(f)
    
    target_timbre = character_timbres.get(target_name)
    source_names_set = set(source_names)
    
    modified_files_count = 0
    renamed_wav_count = 0

    try:
        # --- 2. éå†å¹¶ä¿®æ”¹æ‰€æœ‰ç« èŠ‚JSONæ–‡ä»¶ ---
        for chapter_filename in chapter_files:
            if not chapter_filename.endswith('.json'):
                continue

            chapter_json_path = os.path.join(json_dir, chapter_filename)
            file_modified = False
            
            with open(chapter_json_path, 'r', encoding='utf-8') as f:
                content = json.load(f)
            
            safe_chapter_name = os.path.splitext(chapter_filename)[0]
            chapter_wav_dir = os.path.join(output_wav_base_dir, safe_chapter_name)

            for index, item in enumerate(content):
                current_speaker = item.get('speaker')
                if current_speaker in source_names_set:
                    # --- 3. æ™ºèƒ½åˆ¤æ–­ä¸é‡å‘½åWAVæ–‡ä»¶ ---
                    source_timbre = character_timbres.get(current_speaker)
                    
                    # æ ¸å¿ƒæ¡ä»¶: ä»…å½“æºéŸ³è‰²å­˜åœ¨ï¼Œä¸”ä¸ç›®æ ‡éŸ³è‰²ç›¸åŒæ—¶ï¼Œæ‰è¿›è¡Œé‡å‘½å
                    if source_timbre and source_timbre == target_timbre:
                        # æ„å»ºæ—§æ–‡ä»¶åå’Œæ–°æ–‡ä»¶å
                        safe_speaker_old = "".join(c for c in current_speaker if c.isalnum() or c in " _-").rstrip()
                        safe_speaker_new = "".join(c for c in target_name if c.isalnum() or c in " _-").rstrip()
                        safe_timbre_name = "".join(c for c in source_timbre if c.isalnum() or c in " _-").rstrip()

                        old_wav_name = f"{index:04d}-{safe_speaker_old}-{safe_timbre_name}.wav"
                        new_wav_name = f"{index:04d}-{safe_speaker_new}-{safe_timbre_name}.wav"
                        
                        old_wav_path = os.path.join(chapter_wav_dir, old_wav_name)
                        new_wav_path = os.path.join(chapter_wav_dir, new_wav_name)

                        # æ‰§è¡Œé‡å‘½å
                        if os.path.exists(old_wav_path):
                            try:
                                os.rename(old_wav_path, new_wav_path)
                                renamed_wav_count += 1
                                logger.info(f"WAVé‡å‘½å: {old_wav_path} -> {new_wav_path}")
                            except OSError as e:
                                logger.error(f"é‡å‘½åWAVæ–‡ä»¶å¤±è´¥: {e}")

                    # --- 4. ä¿®æ”¹JSONå†…å®¹ ---
                    item['speaker'] = target_name
                    file_modified = True
            
            if file_modified:
                with open(chapter_json_path, 'w', encoding='utf-8') as f:
                    json.dump(content, f, ensure_ascii=False, indent=2)
                modified_files_count += 1
        
        logger.info(f"åœ¨ {modified_files_count} ä¸ªç« èŠ‚æ–‡ä»¶ä¸­å®Œæˆäº†è§’è‰²ååˆå¹¶ã€‚")
        logger.info(f"æˆåŠŸè‡ªåŠ¨é‡å‘½åäº† {renamed_wav_count} ä¸ªWAVéŸ³é¢‘æ–‡ä»¶ã€‚")

        # --- 5. æ¸…ç†é…ç½®æ–‡ä»¶ ---
        if os.path.exists(profiles_path):  # ç°åœ¨è¿™ä¸ªå˜é‡å·²å®šä¹‰
            with open(profiles_path, 'r', encoding='utf-8') as f:
                profiles = json.load(f)
            for name in source_names:
                if name in profiles:
                    del profiles[name]
            with open(profiles_path, 'w', encoding='utf-8') as f:
                json.dump(profiles, f, ensure_ascii=False, indent=4)
            logger.info("å·²ä»è§’è‰²ç®€ä»‹ä¸­ç§»é™¤è¢«åˆå¹¶çš„è§’è‰²ã€‚")

        if character_timbres: # ä½¿ç”¨å·²ç»åŠ è½½çš„éŸ³è‰²é…ç½®
            if target_name not in character_timbres:
                for name in source_names:
                    if name in character_timbres:
                        character_timbres[target_name] = character_timbres[name]
                        logger.info(f"ç›®æ ‡è§’è‰² '{target_name}' ç»§æ‰¿äº†æºè§’è‰² '{name}' çš„éŸ³è‰²ã€‚")
                        break
            for name in source_names:
                if name in character_timbres:
                    del character_timbres[name]
            with open(timbres_path, 'w', encoding='utf-8') as f:
                json.dump(character_timbres, f, ensure_ascii=False, indent=2)
            logger.info("å·²ä»éŸ³è‰²é…ç½®ä¸­ç§»é™¤è¢«åˆå¹¶çš„è§’è‰²ã€‚")

        return {
            "status": "success", 
            "message": f"æˆåŠŸåˆå¹¶è§’è‰²ã€‚{renamed_wav_count}ä¸ªéŸ³é¢‘æ–‡ä»¶è¢«è‡ªåŠ¨é‡å‘½åï¼Œæ— éœ€é‡æ–°ç”Ÿæˆã€‚"
        }

    except Exception as e:
        logger.error(f"åˆå¹¶è§’è‰²æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"æœåŠ¡å™¨å¤„ç†åˆå¹¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        
def clean_json_content(json_content: List[Dict]) -> List[Dict]:
    """
    Removes entries from the chapter JSON content if their 'content' field
    consists only of punctuation and whitespace.
    """
    # å®šä¹‰ä¸€ä¸ªåŒ…å«ä¸­è‹±æ–‡å¸¸ç”¨æ ‡ç‚¹ç¬¦å·çš„é›†åˆ
    # string.punctuation åŒ…å« '!"#$%&\'()*+,-./:;<=>?@[\\]^_`{|}~'
    # æˆ‘ä»¬å†é¢å¤–è¡¥å……ä¸­æ–‡æ ‡ç‚¹å’Œä¸€äº›ç‰¹æ®Šç¬¦å·
    punctuation_to_remove = set(string.punctuation) | set("ã€‚ï¼Œã€ï¼›ï¼šï¼Ÿï¼â€”â€¦â€œâ€ã€Šã€‹â€˜â€™ï¼ˆï¼‰Â·")

    cleaned_list = []
    for item in json_content:
        content = item.get('content', '').strip()
        
        if not content:
            # å¦‚æœå†…å®¹ä¸ºç©ºæˆ–åªæœ‰ç©ºæ ¼ï¼Œç›´æ¥è·³è¿‡
            continue

        # ç§»é™¤æ‰€æœ‰å®šä¹‰çš„æ ‡ç‚¹ç¬¦å·
        content_without_punctuation = ''.join(char for char in content if char not in punctuation_to_remove)
        
        # å†æ¬¡å»é™¤å¯èƒ½å‰©ä½™çš„ç©ºç™½ï¼ˆä¾‹å¦‚ï¼Œå¦‚æœåŸæ–‡æ˜¯ "ã€‚ ã€‚"ï¼‰
        if content_without_punctuation.strip():
            # å¦‚æœç§»é™¤äº†æ ‡ç‚¹å’Œç©ºæ ¼åï¼Œè¿˜æœ‰å‰©ä½™å†…å®¹ï¼Œè¯´æ˜æ˜¯æœ‰æ•ˆè¯­å¥ï¼Œä¿ç•™
            cleaned_list.append(item)
        else:
            # å¦åˆ™ï¼Œè¯´æ˜æ­¤è¡ŒåªåŒ…å«æ ‡ç‚¹ï¼Œå°†å…¶ä¸¢å¼ƒ
            logger.info(f"æ­£åœ¨æ¸…ç†æ— æ•ˆè¯­å¥: {item}")

    return cleaned_list
    
def _get_chapter_character_presence_map(novel_name: str) -> Dict[str, Any]:
    """
    éå†æŒ‡å®šå°è¯´çš„æ‰€æœ‰å·²å¤„ç†ç« èŠ‚ï¼Œæ„å»ºç« èŠ‚-è§’è‰²æ˜ å°„å’Œç« èŠ‚é¡ºåºåˆ—è¡¨ã€‚
    è¿”å›:
        {
            'chapter_character_map': {'ç« èŠ‚æ ‡é¢˜A': ['è§’è‰²1', 'è§’è‰²2'], ...},
            'chapter_order_list': ['ç« èŠ‚æ ‡é¢˜1', 'ç« èŠ‚æ ‡é¢˜2', ...]
        }
    """
    json_dir = os.path.join(PROJECTS_DIR, novel_name, 'chapters_json')
    if not os.path.isdir(json_dir):
        logger.warning(f"å°è¯´ '{novel_name}' çš„ç« èŠ‚JSONç›®å½•æœªæ‰¾åˆ°ã€‚")
        return {'chapter_character_map': {}, 'chapter_order_list': []}

    chapter_character_map = {}
    chapter_order_list = [] # ä¿æŒç« èŠ‚çš„åŸå§‹é¡ºåº

    # 1. è·å–æ‰€æœ‰ç« èŠ‚æ–‡ä»¶ï¼Œå¹¶æ ¹æ®æ–‡ä»¶åï¼ˆå³ç« èŠ‚æ ‡é¢˜ï¼‰è¿›è¡Œè‡ªç„¶æ’åº
    #    è¿™é‡Œå‡è®¾ç« èŠ‚æ–‡ä»¶åæ˜¯ "ç« èŠ‚æ ‡é¢˜.json"
    chapter_files = sorted([f for f in os.listdir(json_dir) if f.endswith('.json')])

    for filename in chapter_files:
        chapter_filepath = os.path.join(json_dir, filename)
        chapter_title = os.path.splitext(filename)[0] # ä»æ–‡ä»¶åä¸­æå–ç« èŠ‚æ ‡é¢˜

        try:
            with open(chapter_filepath, 'r', encoding='utf-8') as f:
                chapter_data = json.load(f)
            
            speakers_in_chapter = sorted(list(set(item['speaker'] for item in chapter_data if 'speaker' in item))) 
            
            chapter_character_map[chapter_title] = speakers_in_chapter
            chapter_order_list.append(chapter_title)

        except Exception as e:
            logger.error(f"è¯»å–æˆ–è§£æç« èŠ‚æ–‡ä»¶ '{chapter_filepath}' å¤±è´¥ï¼Œè·³è¿‡: {e}", exc_info=True)
            continue
    
    logger.info(f"ä¸ºå°è¯´ '{novel_name}' æˆåŠŸæ„å»ºç« èŠ‚è§’è‰²åˆ†å¸ƒå›¾ã€‚")
    return {
        'chapter_character_map': chapter_character_map,
        'chapter_order_list': chapter_order_list
    }
    
def validate_and_parse_json_array(text: str) -> Optional[list]:
    stripped_text = text.strip()
    if not (stripped_text.startswith('[') and stripped_text.endswith(']')):
        return None
    try:
        return json.loads(stripped_text)
    except json.JSONDecodeError:
        return None

def apply_replacement_rules(text: str, novel_name: str) -> str:
    """
    åŠ è½½å¹¶åº”ç”¨å°è¯´ä¸“å±æ›¿æ¢è¯å…¸ä¸­çš„è§„åˆ™ã€‚
    """
    replaced_text = text
    replace_dict_path = os.path.join(PROJECTS_DIR, novel_name, 'replace_dict.json')
    
    if os.path.exists(replace_dict_path):
        try:
            with open(replace_dict_path, 'r', encoding='utf-8') as f:
                replace_rules_data = json.load(f)
            rules = replace_rules_data.get("rules", [])
            
            # å¯¹è§„åˆ™è¿›è¡Œæ’åºï¼šä»æœ€é•¿çš„ original_word å¼€å§‹æ›¿æ¢ï¼Œé¿å…çŸ­è¯å½±å“é•¿è¯
            rules.sort(key=lambda x: len(x.get("original_word", "")), reverse=True)
            
            applied_replacements = 0
            for rule in rules:
                original = rule.get("original_word")
                replacement = rule.get("replacement_word")
                if original and replacement:
                    new_text, num_replacements = re.subn(re.escape(original), replacement, replaced_text)
                    if num_replacements > 0:
                        replaced_text = new_text
                        applied_replacements += num_replacements
            
            if applied_replacements > 0:
                logger.debug(f"æˆåŠŸä¸ºå°è¯´ã€Œ{novel_name}ã€åº”ç”¨äº† {applied_replacements} æ¬¡æ›¿æ¢è§„åˆ™ (TTSé˜¶æ®µ)ã€‚")
            else:
                logger.debug(f"å°è¯´ã€Œ{novel_name}ã€çš„æ›¿æ¢è¯å…¸ä¸­æ²¡æœ‰åŒ¹é…åˆ°ä»»ä½•æ›¿æ¢é¡¹ (TTSé˜¶æ®µ)ã€‚")

        except (json.JSONDecodeError, FileNotFoundError) as e:
            logger.warning(f"åŠ è½½æˆ–è§£æå°è¯´ã€Œ{novel_name}ã€æ›¿æ¢è¯å…¸å¤±è´¥: {e}ã€‚å°†ä½¿ç”¨åŸå§‹æ–‡æœ¬ (TTSé˜¶æ®µ)ã€‚")
        except Exception as e:
            logger.error(f"åº”ç”¨æ›¿æ¢è§„åˆ™æ—¶å‘ç”Ÿé”™è¯¯: {e}ã€‚å°†ä½¿ç”¨åŸå§‹æ–‡æœ¬ (TTSé˜¶æ®µ)ã€‚")
    else:
        logger.debug(f"replace_dict.json not found for novel '{novel_name}'. No replacements applied (TTSé˜¶æ®µ).")

    return replaced_text
    
async def generate_chapter_json(chapter_content: str, model_name: str) -> List[Dict]:
    try:
        with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
            llm_config = json.load(f)
    except FileNotFoundError:
        raise Exception("LLMé…ç½®æ–‡ä»¶ä¸¢å¤±ï¼Œæ— æ³•æ‰§è¡Œæ“ä½œã€‚")

    model_settings = llm_config.get("models", {}).get(model_name)
    if not model_settings: raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹: {model_name}")

    api_key = model_settings.get("api_key")
    if not api_key: raise Exception(f"æ¨¡å‹ '{model_name}' çš„ API Key æœªåœ¨é…ç½®æ–‡ä»¶ä¸­è®¾ç½®ã€‚")

    proxies = None
    proxy_config = llm_config.get("general", {}).get("proxy", {})
    if model_settings.get("use_proxy") and proxy_config.get("enabled"):
        p_addr = f"{proxy_config.get('protocol', 'socks5h')}://{proxy_config.get('address')}:{proxy_config.get('port')}"
        proxies = {"http": p_addr, "https": p_addr}
        logger.info(f"ä¸ºæ¨¡å‹ {model_name} å¯ç”¨ä»£ç†: {p_addr}")

    max_chars = model_settings.get("max_chars", 5000)
    actual_model_name = model_settings.get("model_name") 
    if not actual_model_name:
        raise Exception(f"æ¨¡å‹ '{model_name}' çš„ model_name æœªåœ¨é…ç½®æ–‡ä»¶ä¸­è®¾ç½®ã€‚")
    
    if "aliyun" in model_name.lower(): # ç”¨æ–°çš„é”®å 'aliyun' åˆ¤æ–­
        return await generate_with_qwen(chapter_content, actual_model_name, api_key, max_chars, proxies)
        
    elif "gemini" in model_name.lower():
        return await generate_with_gemini(chapter_content, actual_model_name, api_key, max_chars, proxies)
        
    elif "volcengine" in model_name.lower():
        return await generate_with_volcengine(chapter_content, actual_model_name, api_key, max_chars, proxies)
    
    elif "chatanywhere" in model_name.lower():
        return await generate_with_chatanywhere(chapter_content, actual_model_name, api_key, max_chars, proxies)
        
    elif "ollama" in model_name.lower():
        return await generate_with_ollama(chapter_content, actual_model_name, api_key, max_chars, proxies)

    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹: {model_name}")
        
        
async def analyze_character(character_name: str, context_text: str, model_name_to_use: str) -> Optional[Dict]:
    """
    Analyzes a character based on text context using the specified model.
    """
    analysis_prompt = f"""è¯·æ ¹æ®ä»¥ä¸‹å°è¯´æ–‡æœ¬ç‰‡æ®µï¼Œæ·±å…¥åˆ†æè§’è‰² â€œ{character_name}â€ çš„äººç‰©ç‰¹å¾ã€‚
ä½ çš„ä»»åŠ¡æ˜¯æå–å…³é”®ä¿¡æ¯ï¼Œå¹¶åªè¿”å›ä¸€ä¸ªä¸¥æ ¼ç¬¦åˆä»¥ä¸‹æ ¼å¼çš„JSONå¯¹è±¡ï¼Œä¸è¦åŒ…å«ä»»ä½•é¢å¤–çš„è§£é‡Šæˆ–Markdownæ ‡è®°ã€‚
JSONå¯¹è±¡å¿…é¡»åŒ…å«ä»¥ä¸‹ä¸‰ä¸ªé”®ï¼š
- "gender": (string) è§’è‰²çš„æ€§åˆ«ï¼Œæ¨æµ‹ä¸º "ç”·", "å¥³", æˆ– "æœªçŸ¥"ã€‚
- "ageGroup": (string) è§’è‰²çš„å¹´é¾„æ®µï¼Œä» "å­©ç«¥", "å°‘å¹´", "é’å¹´", "ä¸­å¹´", "è€å¹´" ä¸­é€‰æ‹©ä¸€ä¸ªæœ€è´´åˆ‡çš„ã€‚
- "identity": (string) è§’è‰²çš„èº«ä»½èƒŒæ™¯ã€èŒä¸šã€æ€§æ ¼ã€å‡ºç°åœºæ™¯ã€ä¸å…¶ä»–è§’è‰²çš„å…³ç³»ç­‰å’Œæè¿°ï¼Œä»¥ä¾¿åç»­å¸®åŠ©åˆ¤æ–­è§’è‰²å¹´é¾„ï¼Œå’Œå…¶ä»–è§’è‰²æ˜¯ä¸æ˜¯åŒä¸€ä¸ªäººï¼Œ200ä¸ªå­—ä»¥å†…ã€‚
è¯·ç¡®ä¿ä½ çš„å›å¤ä¸­åªåŒ…å«è¿™ä¸€ä¸ªJSONå¯¹è±¡ã€‚

æ–‡æœ¬ç‰‡æ®µ:
---
{context_text[:2000]}
---
"""
    try:
        with open(CONFIG_FILE, 'r', encoding='utf-8') as f: config = json.load(f)
    except FileNotFoundError: return None

    # --- æ ¸å¿ƒä¿®æ”¹ï¼šä½¿ç”¨ä¼ å…¥çš„ model_name_to_use ---
    model_settings = config.get("models", {}).get(model_name_to_use)
    if not model_settings:
        logger.error(f"åˆ†æè§’è‰²æ—¶æœªæ‰¾åˆ°æ¨¡å‹ '{model_name_to_use}' çš„é…ç½®ã€‚")
        return None

    api_key = model_settings.get("api_key")
    actual_model_name = model_settings.get("model_name")
    if not api_key or not actual_model_name:
        logger.error(f"æ¨¡å‹ '{model_name_to_use}' çš„ API Key æˆ– model_name æœªé…ç½®ã€‚")
        return None

    proxies = None
    if model_settings.get("use_proxy") and config.get("general", {}).get("proxy", {}).get("enabled"):
        p_cfg = config["general"]["proxy"]
        p_addr = f"{p_cfg.get('protocol', 'socks5h')}://{p_cfg.get('address')}:{p_cfg.get('port')}"
        proxies = {"http": p_addr, "https": p_addr}

    # æ ¹æ®æ¨¡å‹å¹³å°é€‰æ‹©ä¸åŒçš„API URLå’ŒPayload
    api_url = ""
    headers = {}
    payload = {}

    if "gemini" in model_name_to_use.lower():
        api_url = f"https://gemini.yingakki.dpdns.org/v1beta/models/{actual_model_name}:generateContent"
        headers = {'Content-Type': 'application/json', 'x-goog-api-key': api_key}
        payload = {"contents": [{"parts": [{"text": analysis_prompt}]}], "safetySettings": SAFETY_SETTINGS, "generationConfig": {"response_mime_type": "application/json"}}
    elif "aliyun" in model_name_to_use.lower():
        api_url = "https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation"
        headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}
        payload = {
            "model": actual_model_name,
            "input": {"messages": [{"role": "system", "content": "You are a helpful assistant that strictly follows user instructions to return JSON objects."}, {"role": "user", "content": analysis_prompt}]},
            "parameters": {"result_format": "message"}
        }
    elif "volcengine" in model_name_to_use.lower():
        api_url = "https://ark.cn-beijing.volces.com/api/v3/chat/completions"
        headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}
        payload = {
            "model": actual_model_name,
            "messages": [
                {"role": "system", "content": "You are a helpful assistant that strictly follows user instructions and outputs JSON objects."},
                {"role": "user", "content": analysis_prompt}
            ]
        }
    elif "chatanywhere" in model_name_to_use.lower():
        api_url = "https://api.chatanywhere.tech/v1/chat/completions"
        headers = {
        "Authorization": f"Bearer {api_key}", 
        "Content-Type": "application/json"
        }
        payload = {
            "model": actual_model_name,
            "messages": [
                {"role": "system", "content": "You are a helpful assistant that strictly follows user instructions to return JSON objects."},
                {"role": "user", "content": analysis_prompt}
            ],
            "temperature": 0.7  # ä» curl ç¤ºä¾‹ä¸­æ·»åŠ äº† temperature å‚æ•°
        }
    
    elif "ollama" in model_name_to_use.lower():
        # åˆå§‹åŒ–Ollamaå®¢æˆ·ç«¯ï¼ˆå¯¹åº”å®˜æ–¹SDKç”¨æ³•ï¼‰
        client = Client(
            host="https://ollama.com",  # Ollamaå®˜æ–¹APIåœ°å€
            headers={"Authorization": f"Bearer {api_key}"}
        )
        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨ï¼ˆä¸ChatAnywhereçš„payload.messagesç»“æ„å¯¹é½ï¼‰
        messages = [
            {"role": "system", "content": "You are a helpful assistant that strictly follows user instructions to return JSON objects."},
            {"role": "user", "content": analysis_prompt}
        ]
        # è°ƒç”¨Ollamaçš„chatæ¥å£ï¼ˆå¯ç”¨æµå¼å“åº”æ—¶éœ€è¦å¾ªç¯å¤„ç†ï¼‰
        response = client.chat(
            model=actual_model_name,
            messages=messages,
            stream=False  # æ ¹æ®éœ€æ±‚é€‰æ‹©æ˜¯å¦å¯ç”¨æµå¼ï¼Œè¿™é‡Œä¿æŒä¸ChatAnywhereçš„åŒæ­¥å“åº”ä¸€è‡´
        )
        # æå–å“åº”å†…å®¹ï¼ˆOllamaçš„å“åº”ç»“æ„ä¸OpenAIç±»ä¼¼ï¼‰
        message_content = response["message"]["content"]

    else:
        logger.error(f"ä¸æ”¯æŒçš„æ¨¡å‹å¹³å°ç”¨äºè§’è‰²åˆ†æ: {model_name_to_use}")
        return None

    try:
        response = requests.post(api_url, headers=headers, json=payload, proxies=proxies, timeout=120)
        response.raise_for_status()
        response_data = response.json()

        api_text = ""
        if "gemini" in model_name_to_use.lower():
            api_text = response_data['candidates'][0]['content']['parts'][0]['text']
        elif "aliyun" in model_name_to_use.lower():
            api_text = response_data["output"]["choices"][0]["message"]["content"]
        elif "volcengine" in model_name_to_use.lower():
            if response_data.get("choices"):
                api_text = response_data["choices"][0].get("message", {}).get("content")
        elif "chatanywhere" in model_name_to_use.lower():
            api_text = response_data["choices"][0]["message"]["content"]
        elif "ollama" in model_name_to_use.lower():
            # ï¼ˆæ¥å‰é¢çš„å®¢æˆ·ç«¯åˆå§‹åŒ–å’Œè¯·æ±‚è°ƒç”¨ï¼‰
            # Ollamaçš„å“åº”ç»“æ„ä¸­ï¼Œå†…å®¹ç›´æ¥åœ¨messageçš„contentå­—æ®µ
            api_text = response["message"]["content"]

        if not api_text:
             logger.warning(f"æ¨¡å‹ '{model_name_to_use}' è¿”å›çš„å“åº”ä¸­ç¼ºå°‘å†…å®¹å­—æ®µã€‚å®Œæ•´å“åº”: {json.dumps(response_data)}")
             return None
             
        for match in re.finditer(r'\{.*?\}', api_text, re.DOTALL):
            try:
                profile = json.loads(match.group(0))
                if "gender" in profile and "identity" in profile:
                    logger.info(f"æˆåŠŸä½¿ç”¨æ¨¡å‹ '{actual_model_name}' ä¸º '{character_name}' è§£æå‡ºç®€ä»‹ã€‚")
                    return profile
            except json.JSONDecodeError:
                continue

        logger.warning(f"æœªèƒ½ä»AIä¸º '{character_name}' è¿”å›çš„æ–‡æœ¬ä¸­è§£æå‡ºæœ‰æ•ˆçš„ç®€ä»‹ã€‚åŸå§‹å†…å®¹: {api_text[:100]}...")
        return None
    except Exception as e:
        logger.error(f"Request for character '{character_name}' profile failed: {e}")
        return None
        
        
@app.get("/api/get_chapter_source_text")
async def get_chapter_source_text(novel_name: str, chapter_title: str):
    """
    Retrieves the raw text content of a specific chapter from the source.txt file.
    Includes cache-control headers to prevent stale data.
    """
    project_dir = os.path.join(PROJECTS_DIR, novel_name)
    source_path = os.path.join(project_dir, 'source.txt')

    if not os.path.exists(source_path):
        raise HTTPException(status_code=404, detail=f"å°è¯´æºæ–‡ä»¶ 'source.txt' æœªæ‰¾åˆ°ã€‚")

    try:
        # 1. è¯»å–æ•´ä¸ªæºæ–‡ä»¶å†…å®¹
        with open(source_path, 'r', encoding='utf-8-sig', errors='ignore') as f:
            content = f.read()
        
        # 2. ä½¿ç”¨å·²æœ‰çš„ get_chapters_from_txt å‡½æ•°æ¥åˆ‡åˆ†ç« èŠ‚
        all_chapters_map = {c['title']: c['content'] for c in get_chapters_from_txt(content)}
        
        # 3. ä»åˆ‡åˆ†å¥½çš„ç« èŠ‚ä¸­æŸ¥æ‰¾ç›®æ ‡ç« èŠ‚çš„å†…å®¹
        chapter_content = all_chapters_map.get(chapter_title)
        
        if chapter_content is None:
            raise HTTPException(status_code=404, detail=f"åœ¨æºæ–‡ä»¶ä¸­æœªæ‰¾åˆ°ç« èŠ‚æ ‡é¢˜: '{chapter_title}'")
            
        # Define headers that tell the browser not to cache this response. <--- æ–°å¢
        headers = {
            "Cache-Control": "no-cache, no-store, must-revalidate",
            "Pragma": "no-cache",
            "Expires": "0",
        }
        return JSONResponse(content={"status": "success", "content": chapter_content}, headers=headers) # <--- ä¿®æ”¹ï¼šä½¿ç”¨ JSONResponse å¹¶æ·»åŠ  headers

    except Exception as e:
        logger.error(f"è·å–ç« èŠ‚ '{chapter_title}' åŸæ–‡å¤±è´¥: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"æœåŠ¡å™¨è¯»å–æˆ–å¤„ç†æºæ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        
# +++ è·å–ç« èŠ‚åŸæ–‡APIæ¥å£ +++
@app.get("/api/get_chapter_source_text")
async def get_chapter_source_text(novel_name: str, chapter_title: str):
    """
    Retrieves the raw text content of a specific chapter from the source.txt file.
    """
    project_dir = os.path.join(PROJECTS_DIR, novel_name)
    source_path = os.path.join(project_dir, 'source.txt')

    if not os.path.exists(source_path):
        raise HTTPException(status_code=404, detail=f"å°è¯´æºæ–‡ä»¶ 'source.txt' æœªæ‰¾åˆ°ã€‚")

    try:
        # 1. è¯»å–æ•´ä¸ªæºæ–‡ä»¶å†…å®¹
        with open(source_path, 'r', encoding='utf-8-sig', errors='ignore') as f:
            content = f.read()
        
        # 2. ä½¿ç”¨å·²æœ‰çš„ get_chapters_from_txt å‡½æ•°æ¥åˆ‡åˆ†ç« èŠ‚
        #    è¿™å°†è¿”å›ä¸€ä¸ªå­—å…¸åˆ—è¡¨ï¼Œå¦‚ [{'title': 'ç¬¬ä¸€ç« ', 'content': '...'}, ...]
        all_chapters_map = {c['title']: c['content'] for c in get_chapters_from_txt(content)}
        
        # 3. ä»åˆ‡åˆ†å¥½çš„ç« èŠ‚ä¸­æŸ¥æ‰¾ç›®æ ‡ç« èŠ‚çš„å†…å®¹
        chapter_content = all_chapters_map.get(chapter_title)
        
        if chapter_content is None:
            raise HTTPException(status_code=404, detail=f"åœ¨æºæ–‡ä»¶ä¸­æœªæ‰¾åˆ°ç« èŠ‚æ ‡é¢˜: '{chapter_title}'")
            
        return {"status": "success", "content": chapter_content}

    except Exception as e:
        logger.error(f"è·å–ç« èŠ‚ '{chapter_title}' åŸæ–‡å¤±è´¥: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"æœåŠ¡å™¨è¯»å–æˆ–å¤„ç†æºæ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")


# +++ æ–°å¢ï¼šåªè¿”å›ç« èŠ‚åˆ—è¡¨çš„è½»é‡çº§æ¥å£ +++
@app.get("/api/list_chapters")
async def list_chapters(novel_name: str):
    """
    Gets a detailed list of all chapters for a novel, including their
    processed and spliced status.
    """
    project_dir = os.path.join(PROJECTS_DIR, novel_name)
    source_path = os.path.join(project_dir, 'source.txt')
    json_dir = os.path.join(project_dir, 'chapters_json')
    output_novel_dir = os.path.join(OUTPUT_DIR, novel_name)

    if not os.path.exists(source_path):
        raise HTTPException(status_code=404, detail=f"Novel source file not found for '{novel_name}'")

    try:
        with open(source_path, 'r', encoding='utf-8-sig', errors='ignore') as f:
            content = f.read()
        
        all_chapters_from_txt = get_chapters_from_txt(content)
        processed_jsons = {f for f in os.listdir(json_dir) if f.endswith('.json')} if os.path.isdir(json_dir) else set()
        spliced_audios = {f for f in os.listdir(output_novel_dir) if f.endswith(('.mp3', '.wav', '.m4a', '.ogg'))} if os.path.isdir(output_novel_dir) else set()

        chapter_details = []
        for i, chap in enumerate(all_chapters_from_txt):
            safe_title = sanitize_for_filename(chap['title']) # ä½¿ç”¨ä¸æ‹¼æ¥æ—¶ä¸€è‡´çš„ sanitize
            is_processed = f"{safe_title}.json" in processed_jsons
            is_spliced = any(f.startswith(safe_title) for f in spliced_audios)
            
            chapter_details.append({
                "id": i,
                "title": chap['title'], 
                "processed": is_processed, 
                "spliced": is_spliced
            })
        
        return {"chapters": chapter_details}
    except Exception as e:
        logger.error(f"Error listing chapters for novel '{novel_name}': {e}")
        raise HTTPException(status_code=500, detail=f"Error reading novel details: {e}")
        
# å·®å¼‚ç‚¹ï¼šä¸º /api/get_novel_content æ¥å£æ·»åŠ ç¦æ­¢ç¼“å­˜çš„å“åº”å¤´
@app.get("/api/get_novel_content")
async def get_novel_content(filepath: str):
    """
    Serves the content of a specific processed chapter JSON file,
    with cache-control headers to prevent stale data.
    """
    try:
        path_parts = filepath.split('/', 1)
        if len(path_parts) != 2:
            raise HTTPException(status_code=400, detail="Invalid filepath format.")
        
        novel_name, chapter_filename = path_parts
        full_path = os.path.join(PROJECTS_DIR, novel_name, 'chapters_json', chapter_filename)
    except Exception:
        raise HTTPException(status_code=400, detail="Could not parse filepath.")

    project_root = os.path.abspath(PROJECTS_DIR)
    if not os.path.abspath(full_path).startswith(project_root):
        raise HTTPException(status_code=403, detail="ç¦æ­¢è®¿é—®ã€‚")

    if not os.path.exists(full_path):
        raise HTTPException(status_code=404, detail=f"å°è¯´ç« èŠ‚æ–‡ä»¶æœªæ‰¾åˆ°: {filepath}")
    
    # Define headers that tell the browser not to cache this response.
    headers = {
        "Cache-Control": "no-cache, no-store, must-revalidate",
        "Pragma": "no-cache",
        "Expires": "0",
    }
    # Return the FileResponse with the custom headers.
    return FileResponse(full_path, headers=headers)
    
# main.py -> HELPER FUNCTIONS
@app.delete("/api/delete_novel")
async def delete_novel(novel_name: str):
    """
    Deletes a novel project, including its project files and output audio.
    Includes comprehensive security checks.
    """
    if not novel_name:
        raise HTTPException(status_code=400, detail="å°è¯´åç§°ä¸èƒ½ä¸ºç©ºã€‚")

    # --- æ ¸å¿ƒå®‰å…¨æ£€æŸ¥ï¼šé˜²æ­¢ç›®å½•éå†æ”»å‡» ---
    # ç¡®ä¿ novel_name æ˜¯ä¸€ä¸ªçº¯ç²¹çš„ç›®å½•åï¼Œä¸åŒ…å«ä»»ä½•è·¯å¾„åˆ†éš”ç¬¦
    if novel_name != os.path.basename(novel_name) or ".." in novel_name:
        logger.warning(f"æ½œåœ¨çš„ç›®å½•éå†æ”»å‡»è¢«é˜»æ­¢: {novel_name}")
        raise HTTPException(status_code=403, detail="éæ³•çš„å°è¯´åç§°ã€‚")

    # --- å®šä½è¦åˆ é™¤çš„ç›®å½• ---
    project_dir = os.path.join(PROJECTS_DIR, novel_name)
    output_dir = os.path.join(OUTPUT_DIR, novel_name)

    # æ£€æŸ¥é¡¹ç›®ç›®å½•æ˜¯å¦å­˜åœ¨ï¼Œè¿™æ˜¯åˆ é™¤çš„å¿…è¦æ¡ä»¶
    if not os.path.isdir(project_dir):
        raise HTTPException(status_code=404, detail=f"å°è¯´é¡¹ç›® '{novel_name}' æœªæ‰¾åˆ°ã€‚")

    # --- æ‰§è¡Œåˆ é™¤æ“ä½œ ---
    errors = []
    # 1. åˆ é™¤é¡¹ç›®æ–‡ä»¶ç›®å½•
    try:
        shutil.rmtree(project_dir)
        logger.info(f"æˆåŠŸåˆ é™¤é¡¹ç›®ç›®å½•: {project_dir}")
    except Exception as e:
        error_msg = f"åˆ é™¤é¡¹ç›®ç›®å½• '{project_dir}' å¤±è´¥: {e}"
        logger.error(error_msg)
        errors.append(error_msg)

    # 2. åˆ é™¤è¾“å‡ºæ–‡ä»¶ç›®å½• (å¦‚æœå­˜åœ¨)
    if os.path.isdir(output_dir):
        try:
            shutil.rmtree(output_dir)
            logger.info(f"æˆåŠŸåˆ é™¤è¾“å‡ºç›®å½•: {output_dir}")
        except Exception as e:
            error_msg = f"åˆ é™¤è¾“å‡ºç›®å½• '{output_dir}' å¤±è´¥: {e}"
            logger.error(error_msg)
            errors.append(error_msg)
    
    if errors:
        # å¦‚æœæœ‰ä»»ä½•ä¸€ä¸ªåˆ é™¤æ“ä½œå¤±è´¥ï¼Œéƒ½è¿”å›ä¸€ä¸ªé”™è¯¯
        raise HTTPException(status_code=500, detail="åˆ é™¤è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: " + "; ".join(errors))

    return {"status": "success", "message": f"å°è¯´é¡¹ç›® '{novel_name}' å·²è¢«æ°¸ä¹…åˆ é™¤ã€‚"}
    
def smart_chunk_text(text: str, max_length: int) -> List[str]:
    """
    Splits a long text into smaller chunks without breaking sentences.
    """
    chunks = []
    current_pos = 0
    
    while current_pos < len(text):
        # ç¡®å®šå½“å‰å—çš„æœ€å¤§ç»“æŸä½ç½®
        end_pos = min(current_pos + max_length, len(text))
        
        # å¦‚æœå·²ç»åˆ°è¾¾æ–‡æœ¬æœ«å°¾ï¼Œç›´æ¥æ·»åŠ å‰©ä½™éƒ¨åˆ†
        if end_pos == len(text):
            chunks.append(text[current_pos:])
            break
            
        # ä»æœ€å¤§ç»“æŸä½ç½®å‘å‰æŸ¥æ‰¾æœ€ä½³åˆ†å‰²ç‚¹
        # åˆ†å‰²ç‚¹çš„ä¼˜å…ˆçº§ï¼šæ¢è¡Œç¬¦ > å¥å· > æ„Ÿå¹å· > é—®å·
        split_delimiters = ['\n', 'ã€‚', 'ï¼', 'ï¼Ÿ', 'ï¼›', 'ï¼Œ']
        best_split_pos = -1

        for delimiter in split_delimiters:
            # rfind åœ¨æŒ‡å®šèŒƒå›´å†…ä»å³å‘å·¦æŸ¥æ‰¾
            found_pos = text.rfind(delimiter, current_pos, end_pos)
            if found_pos != -1:
                best_split_pos = found_pos + 1 # åˆ†å‰²ç‚¹åœ¨æ ‡ç‚¹ä¹‹å
                break
        
        # å¦‚æœæ‰¾ä¸åˆ°ä»»ä½•åˆé€‚çš„æ ‡ç‚¹ï¼Œåˆ™å¼ºåˆ¶åœ¨ max_length å¤„åˆ†å‰²
        if best_split_pos == -1:
            best_split_pos = end_pos
            
        # æ·»åŠ å—åˆ°åˆ—è¡¨ï¼Œå¹¶æ›´æ–°å½“å‰ä½ç½®
        chunks.append(text[current_pos:best_split_pos])
        current_pos = best_split_pos
        
    logger.info(f"æ–‡æœ¬è¢«æ™ºèƒ½åˆ†å‰²ä¸º {len(chunks)} ä¸ªå—ã€‚")
    return chunks
    
async def normalize_character_names(new_names: List[str], existing_characters_with_profiles: Dict, context_text: str, model_name_to_use: str) -> Dict[str, str]:
    if not new_names or not existing_characters_with_profiles:
        return {}
    
    existing_names_formatted = json.dumps(existing_characters_with_profiles, ensure_ascii=False, indent=2)

    normalization_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å°è¯´ç¼–è¾‘ï¼Œæ“…é•¿æ ¹æ®äººç‰©ç®€ä»‹å’Œä¸Šä¸‹æ–‡ï¼Œè¯†åˆ«è§’è‰²çš„ä¸åŒç§°è°“ã€‚
ä½ çš„ä»»åŠ¡æ˜¯åˆ¤æ–­â€œæ–°å‡ºç°çš„åå­—â€æ˜¯å¦æ˜¯â€œå·²å­˜åœ¨è§’è‰²â€çš„åˆ«åã€‚

---
ã€å·²çŸ¥ä¿¡æ¯ã€‘

1. å·²å­˜åœ¨è§’è‰²çš„ç®€ä»‹:
{existing_names_formatted}

2. æ–°åå­—å‡ºç°çš„ç« èŠ‚ä¸Šä¸‹æ–‡:
{context_text[:2500]}
---

ã€å¾…åˆ¤æ–­ã€‘

æ–°å‡ºç°çš„åå­—åˆ—è¡¨:
{json.dumps(new_names, ensure_ascii=False)}

---
ã€ä»»åŠ¡è¦æ±‚ã€‘

1. **ç»¼åˆåˆ†æ**: ä»”ç»†é˜…è¯»ã€å·²çŸ¥ä¿¡æ¯ã€‘ï¼Œåˆ¤æ–­â€œæ–°å‡ºç°çš„åå­—â€æ˜¯å¦æŒ‡ä»£â€œå·²å­˜åœ¨è§’è‰²â€ã€‚
   - ä¾æ®åŒ…æ‹¬ä½†ä¸é™äºï¼šå§“åå…³è”æ€§ï¼ˆå¦‚â€œå¼ çœŸäººâ€ -> â€œå¼ ä¸‰ä¸°â€ï¼‰ã€ä¸Šä¸‹æ–‡ä¸­çš„è¡Œä¸ºã€å¯¹è¯ã€ä»–äººå¯¹ä»–ä»¬çš„ç§°å‘¼ç­‰ã€‚
   - ç‰¹åˆ«æ³¨æ„äººç‰©ç®€ä»‹ä¸­çš„â€œæ€§åˆ«â€ã€â€œå¹´é¾„æ®µâ€ã€â€œèº«ä»½â€ç­‰å…³é”®ç‰¹å¾æ˜¯å¦åŒ¹é…ã€‚
2. **ä¸¥æ ¼çš„JSONè¾“å‡º**: ä½ çš„å›ç­”å¿…é¡»æ˜¯ä¸€ä¸ªä¸¥æ ¼çš„JSONå¯¹è±¡ï¼Œä»£è¡¨ä¸€ä¸ªä»â€œæ–°åå­—â€åˆ°â€œå·²å­˜åœ¨è§’è‰²åâ€çš„æ˜ å°„ã€‚
3. **åªæ˜ å°„ç¡®å®šçš„åˆ«å**: å¦‚æœä¸€ä¸ªæ–°åå­—æ— æ³•ã€éå¸¸æœ‰ä¿¡å¿ƒåœ°ã€‘ç¡®å®šæ˜¯ä»»ä½•å·²å­˜åœ¨è§’è‰²çš„åˆ«åï¼Œåˆ™ã€ç»å¯¹ä¸è¦ã€‘åœ¨JSONä¸­åŒ…å«å®ƒã€‚å®å¯æ¼æ‰ï¼Œä¸å¯é”™åˆ¤ã€‚
4. **ç©ºç»“æœ**: å¦‚æœæ²¡æœ‰ä»»ä½•æ–°åå­—æ˜¯åˆ«åï¼Œåˆ™å¿…é¡»è¿”å›ä¸€ä¸ªç©ºçš„JSONå¯¹è±¡ `{{}}`ã€‚
5. **æ— é¢å¤–æ–‡æœ¬**: é™¤äº†JSONå¯¹è±¡ï¼Œä¸è¦è¾“å‡ºä»»ä½•å…¶ä»–è§£é‡Šã€è¯´æ˜æˆ–Markdownæ ‡è®°ã€‚

ä¾‹å¦‚ï¼Œå¦‚æœåˆ¤æ–­â€œç‹å§‘å¨˜â€çš„è¨€è¡Œå’Œç®€ä»‹éƒ½ç¬¦åˆâ€œç‹è¯­å«£â€ï¼Œåˆ™åº”è¿”å›ï¼š
`{{"ç‹å§‘å¨˜": "ç‹è¯­å«£"}}`
"""
    try:
        with open(CONFIG_FILE, 'r', encoding='utf-8') as f: config = json.load(f)
    except FileNotFoundError: return {}
    
    model_settings = config.get("models", {}).get(model_name_to_use)
    if not model_settings: return {}
    
    api_key = model_settings.get("api_key")
    actual_model_name = model_settings.get("model_name")
    if not api_key or not actual_model_name: return {}
    
    proxies = None
    if model_settings.get("use_proxy") and config.get("general", {}).get("proxy", {}).get("enabled"):
        p_cfg = config["general"]["proxy"]
        p_addr = f"{p_cfg.get('protocol', 'socks5h')}://{p_cfg.get('address')}:{p_cfg.get('port')}"
        proxies = {"http": p_addr, "https": p_addr}

    api_url, headers, payload = "", {}, {}
    if "gemini" in model_name_to_use.lower():
        api_url = f"https://gemini.yingakki.dpdns.org/v1beta/models/{actual_model_name}:generateContent"
        headers = {'Content-Type': 'application/json', 'x-goog-api-key': api_key}
        payload = {"contents": [{"parts": [{"text": normalization_prompt}]}], "safetySettings": SAFETY_SETTINGS, "generationConfig": {"response_mime_type": "application/json"}}
    elif "aliyun" in model_name_to_use.lower():
        api_url = "https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation"
        headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}
        payload = {
            "model": actual_model_name,
            "input": {"messages": [{"role": "system", "content": "You are a helpful assistant that strictly follows user instructions to return JSON objects."}, {"role": "user", "content": normalization_prompt}]},
            "parameters": {"result_format": "message"}
        }
    elif "volcengine" in model_name_to_use.lower():
        api_url = "https://ark.cn-beijing.volces.com/api/v3/chat/completions"
        headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}
        payload = {
            "model": actual_model_name,
            "messages": [
                {"role": "system", "content": "You are a helpful assistant that strictly follows user instructions and outputs JSON objects."},
                {"role": "user", "content": normalization_prompt}
            ]
        }
    elif "chatanywhere" in model_name_to_use.lower():
        api_url = "https://api.chatanywhere.tech/v1/chat/completions"
        headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}
        payload = {
            "model": actual_model_name,
            "messages": [
                {"role": "system", "content": "You are a helpful assistant that strictly follows user instructions to return JSON objects."},
                {"role": "user", "content": normalization_prompt}
            ],
        }
    elif "ollama" in model_name_to_use.lower():
        # Ollamaå®¢æˆ·ç«¯åˆå§‹åŒ–ï¼ˆæ›¿ä»£api_urlå’Œheadersé…ç½®ï¼‰
        client = Client(
            host="https://ollama.com",  # OllamaæœåŠ¡åœ°å€
            headers={"Authorization": f"Bearer {api_key}"}  # è®¤è¯ä¿¡æ¯
        )
        # æ„å»ºæ¶ˆæ¯ä½“ï¼ˆä¸ChatAnywhereçš„payload.messagesç»“æ„å®Œå…¨å¯¹é½ï¼‰
        messages = [
            {"role": "system", "content": "You are a helpful assistant that strictly follows user instructions to return JSON objects."},
            {"role": "user", "content": normalization_prompt}
        ]
        # è°ƒç”¨Ollamaçš„chatæ¥å£ï¼ˆå¯¹åº”ChatAnywhereçš„APIè¯·æ±‚ï¼‰
        response = client.chat(
            model=actual_model_name,  # å¯¹åº”payloadä¸­çš„"model"å­—æ®µ
            messages=messages         # å¯¹åº”payloadä¸­çš„"messages"å­—æ®µ
        )

    else:
        return {}
    
    try:
        response = requests.post(api_url, headers=headers, json=payload, proxies=proxies, timeout=180)
        response.raise_for_status()
        response_data = response.json()

        api_text = ""
        if "gemini" in model_name_to_use.lower():
            api_text = response_data['candidates'][0]['content']['parts'][0]['text']
        elif "aliyun" in model_name_to_use.lower():
            api_text = response_data["output"]["choices"][0]["message"]["content"]
        elif "volcengine" in model_name_to_use.lower():
            api_text = response_data["choices"]["message"]["content"]
        elif "chatanywhere" in model_name_to_use.lower():
            api_text = response_data["choices"][0]["message"]["content"]
        elif "ollama" in model_name_to_use.lower():
            # Ollamaçš„å“åº”å†…å®¹ç›´æ¥åœ¨messageçš„contentå­—æ®µï¼ˆæ— choicesæ•°ç»„ï¼‰
            api_text = response["message"]["content"]
            
        match = re.search(r'\{.*\}', api_text, re.DOTALL)
        if match:
            mapping = json.loads(match.group(0))
            existing_names = list(existing_characters_with_profiles.keys())
            cleaned_mapping = {k: v for k, v in mapping.items() if k in new_names and v in existing_names}
            logger.info(f"è§’è‰²åå½’ä¸€åŒ–æ˜ å°„ç»“æœ: {cleaned_mapping}")
            return cleaned_mapping
        return {}
    except Exception as e:
        logger.error(f"è§’è‰²åå½’ä¸€åŒ–å¤±è´¥: {e}")
        return {}
        
@app.post("/api/process_single_chapter")
async def process_single_chapter(req: ProcessSingleChapterRequest):
    """
    Processes a single chapter.
    If preview_only is true, returns the raw text content without processing.
    If force_regenerate is true, deletes existing audio files first.
    """
    project_dir = os.path.join(PROJECTS_DIR, req.novel_name)
    source_path = os.path.join(project_dir, 'source.txt')
    json_dir = os.path.join(project_dir, 'chapters_json') # <-- æ·»åŠ è¿™ä¸€è¡Œ
    profiles_path = os.path.join(project_dir, 'character_profiles.json') # <-- æ·»åŠ è¿™ä¸€è¡Œ
    
    # --- 1. åŠ è½½èµ„æº (æå‰) ---
    try:
        with open(source_path, 'r', encoding='utf-8-sig', errors='ignore') as f:
            content = f.read()
        all_chapters_map = {c['title']: c['content'] for c in get_chapters_from_txt(content)}
        chapter_content = all_chapters_map.get(req.chapter_title)

        if not chapter_content:
            raise HTTPException(status_code=404, detail=f"Chapter '{req.chapter_title}' not found.")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è¯»å–é¡¹ç›®æ–‡ä»¶å¤±è´¥: {e}")

    # --- 2. é¢„è§ˆæ¨¡å¼é€»è¾‘ ---
    if req.preview_only:
        logger.info(f"ä¸ºç« èŠ‚ '{req.chapter_title}' æä¾›åŸæ–‡é¢„è§ˆã€‚")
        return {"status": "preview", "content": chapter_content}
    # ---

    # 3. æ£€æŸ¥å¤„ç†æ¨¡å¼æ‰€å¿…éœ€çš„ model_name
    if not req.model_name:
        raise HTTPException(status_code=400, detail="å¤„ç†ç« èŠ‚éœ€è¦æä¾› model_nameã€‚")

    # --- æ–‡ä»¶æ¸…ç†é€»è¾‘ ---
    if req.force_regenerate:
        logger.warning(f"å¼ºåˆ¶é‡æ–°ç”Ÿæˆæ¨¡å¼å·²æ¿€æ´»ï¼Œå°†ä¸ºç« èŠ‚ '{req.chapter_title}' æ¸…ç†æ—§æ–‡ä»¶ã€‚")
        safe_title = "".join(c for c in req.chapter_title if c.isalnum() or c in " _-").rstrip()
        
        # 1. åˆ é™¤å•å¥ WAV æ–‡ä»¶ç›®å½•
        chapter_wav_dir = os.path.join(OUTPUT_DIR, req.novel_name, 'wavs', safe_title)
        if os.path.isdir(chapter_wav_dir):
            try:
                shutil.rmtree(chapter_wav_dir)
                logger.info(f"  - å·²åˆ é™¤ç›®å½•: {chapter_wav_dir}")
            except Exception as e:
                logger.error(f"  - åˆ é™¤ç›®å½• {chapter_wav_dir} å¤±è´¥: {e}")

        # 2. åˆ é™¤æœ€ç»ˆæ‹¼æ¥çš„éŸ³é¢‘æ–‡ä»¶
        final_audio_dir = os.path.join(OUTPUT_DIR, req.novel_name)
        if os.path.isdir(final_audio_dir):
            for f in os.listdir(final_audio_dir):
                if f.startswith(safe_title):
                    file_to_delete = os.path.join(final_audio_dir, f)
                    try:
                        os.remove(file_to_delete)
                        logger.info(f"  - å·²åˆ é™¤æ–‡ä»¶: {file_to_delete}")
                    except Exception as e:
                        logger.error(f"  - åˆ é™¤æ–‡ä»¶ {file_to_delete} å¤±è´¥: {e}")

    try:
        # Load existing JSON content for the chapter, if any, to preserve line_id
        current_chapter_json_path = os.path.join(json_dir, f"{sanitize_for_filename(req.chapter_title)}.json")
        existing_chapter_data_map = {} 
        if os.path.exists(current_chapter_json_path):
            with open(current_chapter_json_path, 'r', encoding='utf-8') as f:
                existing_data = json.load(f)
            for item in existing_data:
                if 'line_id' in item:
                    existing_chapter_data_map[item['line_id']] = item

        with open(source_path, 'r', encoding='utf-8-sig', errors='ignore') as f:
            content = f.read()
        all_chapters_map = {c['title']: c['content'] for c in get_chapters_from_txt(content)}
        
        try:
            with open(profiles_path, 'r', encoding='utf-8') as f:
                character_profiles = json.load(f)
        except (FileNotFoundError, json.JSONDecodeError):
            character_profiles = {}

    except Exception as e:
         raise HTTPException(status_code=500, detail=f"è¯»å–é¡¹ç›®æ–‡ä»¶å¤±è´¥: {e}")

    existing_character_names = list(character_profiles.keys())
    title = req.chapter_title
    chapter_content = all_chapters_map.get(title)

    if not chapter_content:
        raise HTTPException(status_code=404, detail=f"Chapter '{title}' not found in source text.")

    logger.info(f"Processing single chapter: {title}")
    
    try:
        raw_json_content = await generate_chapter_json(chapter_content, req.model_name)
        full_json_content = clean_json_content(raw_json_content)
        
        # --- ç”Ÿæˆæˆ–ä¿ç•™ line_id ---
        chapter_was_modified_for_line_id = False
        for item in full_json_content:
            if 'line_id' not in item:
                item['line_id'] = str(uuid.uuid4()) # ç”Ÿæˆæ–°çš„ UUID
                chapter_was_modified_for_line_id = True
        
        # --- å¦‚æœæœ‰ line_id è¢«ç”Ÿæˆï¼Œé™é»˜ä¿å­˜ç« èŠ‚ JSON ---
        if chapter_was_modified_for_line_id:
            logger.info(f"ç« èŠ‚ '{title}' å› ç¼ºå°‘ line_id è€Œè¢«æ›´æ–°ï¼Œæ­£åœ¨é™é»˜ä¿å­˜ã€‚")
            os.makedirs(json_dir, exist_ok=True) 
            safe_title = sanitize_for_filename(title)
            output_path = os.path.join(json_dir, f"{safe_title}.json")
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(full_json_content, f, ensure_ascii=False, indent=2)
            logger.info(f"ç« èŠ‚ '{title}' å”¯ä¸€IDå·²é™é»˜ä¿å­˜åˆ°: {output_path}")

        speakers_in_chapter = sorted(list({item['speaker'] for item in full_json_content if 'speaker' in item and item['speaker'] != "æ—ç™½"}))
        new_potential_names = [name for name in speakers_in_chapter if name not in existing_character_names]

        name_mapping = {}
        if new_potential_names and character_profiles:
            name_mapping = await normalize_character_names(new_potential_names, character_profiles, chapter_content, req.model_name)

        if name_mapping:
            logger.info(f"åº”ç”¨åç§°æ˜ å°„: {name_mapping}")
            for item in full_json_content:
                if item.get('speaker') in name_mapping:
                    original_name = item['speaker']
                    new_name = name_mapping[original_name]
                    item['speaker'] = new_name
                    logger.info(f"  - å°† '{original_name}' æ›¿æ¢ä¸º '{new_name}'")

        os.makedirs(json_dir, exist_ok=True) 
        safe_title = sanitize_for_filename(title)
        output_path = os.path.join(json_dir, f"{safe_title}.json")
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(full_json_content, f, ensure_ascii=False, indent=2)
        logger.info(f"Chapter '{title}' successfully saved to: {output_path}")

        final_speakers_in_chapter = sorted(list({item['speaker'] for item in full_json_content if 'speaker' in item and item['speaker'] != "æ—ç™½"}))
        truly_new_characters = [name for name in final_speakers_in_chapter if name not in existing_character_names]
        
        newly_analyzed_count = 0
        if truly_new_characters:
            logger.info(f"å‘ç°çœŸæ­£çš„æ–°è§’è‰²: {', '.join(truly_new_characters)}")
            
            profiles_were_updated = False
            for char_name in truly_new_characters:
                context_for_analysis = chapter_content
                first_occurrence = chapter_content.find(char_name)
                
                if first_occurrence != -1:
                    context_window = 1200
                    start = max(0, first_occurrence - context_window)
                    end = min(len(chapter_content), first_occurrence + len(char_name) + context_window)
                    context_for_analysis = chapter_content[start:end]
                
                logger.info(f"ä¸ºæ–°è§’è‰² '{char_name}' è¯·æ±‚ç®€ä»‹åˆ†æ (ä½¿ç”¨æ¨¡å‹: {req.model_name})...")
                profile = await analyze_character(char_name, context_for_analysis, req.model_name)
                
                if profile:
                    character_profiles[char_name] = profile
                    profiles_were_updated = True

            if profiles_were_updated:
                with open(profiles_path, 'w', encoding='utf-8') as f:
                    json.dump(character_profiles, f, ensure_ascii=False, indent=4)
                
        return {"status": "success", "message": f"ç« èŠ‚ '{title}' å¤„ç†æˆåŠŸã€‚"}

    except Exception as e:
        logger.error(f"Failed to process single chapter '{title}': {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Failed to process chapter '{title}': {e}")
        
# å·®å¼‚ç‚¹ï¼šæ›¿æ¢ get_chapters_from_txt å‡½æ•°çš„å®ç°
# å·®å¼‚ç‚¹ï¼šä½¿ç”¨å…¨æ–°çš„ã€åŸºäºå¯å‘å¼è§„åˆ™çš„ get_chapters_from_txt å‡½æ•°

def get_chapters_from_txt(text_content: str) -> List[Dict]:
    """
    Intelligently splits a text into chapters using a weighted, heuristic-based
    engine to identify the dominant chapter pattern and filter out noise.
    """
    
    # === Stage 1: Candidate Generation & Feature Engineering ===

    # A comprehensive but tolerant regex to find ALL potential chapter-like lines.
    # It captures the entire line for analysis.
    candidate_regex = re.compile(
        r"^\s*("
        r"(?:ç¬¬\s*[é›¶ã€‡ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡\d]+\s*[ç« å›å·èŠ‚])"  # e.g., ç¬¬ä¸€ç« , ç¬¬100å›
        r"|"
        r"(?:[æ­£å¤–]ç¯‡|[ä¸Šä¸‹]éƒ¨)" # e.g., æ­£ç¯‡, ä¸Šéƒ¨
        r"|"
        r"(?:\d{1,5})" # e.g., 101
        r"|"
        r"(?:[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡é›¶ã€‡]+)" # e.g., ä¸€ç™¾é›¶ä¸€
        r"|"
        r"(?:[\(ï¼ˆ\[ã€]\s*\d+\s*[\)ï¼‰\]ã€‘])" # e.g., (101)
        r").*?$", re.MULTILINE
    )
    
    candidates = []
    # Use splitlines() to accurately determine if a line is standalone
    lines = text_content.splitlines()
    line_map = {line.strip(): i for i, line in enumerate(lines)}
    
    last_numeric_val = 0
    for match in candidate_regex.finditer(text_content):
        line_content = match.group(0).strip()
        
        # Feature Extraction
        features = {
            'text': line_content,
            'pos': match.start(),
            'pattern_type': None,
            'is_standalone': False,
            'length': len(line_content),
            'contains_chapter_word': any(kw in line_content for kw in ['ç« ', 'å›', 'å·', 'èŠ‚', 'ç¯‡', 'éƒ¨']),
            'is_sequential': False,
            'numeric_val': 0
        }
        
        # Determine pattern type
        if re.search(r'^ç¬¬\s*[\dä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡é›¶ã€‡]+\s*[ç« å›å·èŠ‚]', line_content):
            features['pattern_type'] = 'formal_chapter' # Highest weight
        elif re.search(r'^\d+', line_content):
            features['pattern_type'] = 'numeric_list'
        else:
            features['pattern_type'] = 'other'

        # Check if it's a standalone line (heuristic: next line is empty or doesn't start immediately)
        line_index = line_map.get(line_content)
        if line_index is not None and (line_index + 1 >= len(lines) or not lines[line_index + 1].strip()):
            features['is_standalone'] = True

        # Check for sequential numbering (heuristic)
        numeric_part = re.search(r'\d+', line_content)
        if numeric_part:
            current_numeric_val = int(numeric_part.group(0))
            if current_numeric_val == last_numeric_val + 1:
                features['is_sequential'] = True
            last_numeric_val = current_numeric_val
            features['numeric_val'] = current_numeric_val

        candidates.append(features)

    if not candidates:
        if text_content.strip():
            return [] # ç›´æ¥è¿”å›ç©ºåˆ—è¡¨
        return []

    # === Stage 2: Weighted Scoring & Decision ===
    
    pattern_scores = {}
    for cand in candidates:
        score = 0
        # Weights - these can be tuned
        if cand['is_standalone']: score += 50
        if cand['length'] > 50: score -= 100 # Heavy penalty for long lines
        if cand['contains_chapter_word']: score += 20
        if cand['is_sequential']: score += 10
        
        # Base score for pattern type
        if cand['pattern_type'] == 'formal_chapter': score += 30
        
        pattern_scores.setdefault(cand['pattern_type'], []).append(score)

    # Calculate average score for each pattern type
    avg_scores = {
        pattern: sum(scores) / len(scores)
        for pattern, scores in pattern_scores.items()
        if scores
    }
    
    if not avg_scores: # If no patterns scored positively
        dominant_pattern = 'other' # Fallback
    else:
        # The pattern with the highest average score wins
        dominant_pattern = max(avg_scores, key=avg_scores.get)
    
    #logger.info(f"Dominant chapter pattern identified: {dominant_pattern} with scores: {avg_scores}")
    
    # === Stage 3: Precise Extraction ===
    
    # Filter candidates to only include those matching the dominant pattern
    final_titles = [cand for cand in candidates if cand['pattern_type'] == dominant_pattern and cand['length'] <= 50]

    if not final_titles:
        # Fallback if the dominant pattern was wrong or filtered out everything
        logger.warning("Dominant pattern resulted in no chapters. Falling back to simple extraction.")
        # As a simple fallback, let's use the original regex and split
        return [] # ç›´æ¥è¿”å›ç©ºåˆ—è¡¨


    chapters = []
    # Handle content before the first real chapter
    first_chapter_pos = final_titles[0]['pos']
    if first_chapter_pos > 0:
        intro_content = text_content[:first_chapter_pos].strip()
        if intro_content:
            chapters.append({"title": "å‰è¨€", "content": intro_content})

    for i, title_info in enumerate(final_titles):
        start_pos = title_info['pos']
        # End position is the start of the next chapter, or the end of the text
        end_pos = final_titles[i + 1]['pos'] if i + 1 < len(final_titles) else len(text_content)
        
        full_chapter_text = text_content[start_pos:end_pos].strip()
        
        # The full title is the first line of this chunk
        parts = full_chapter_text.split('\n', 1)
        full_title = parts[0].strip()
        content = full_chapter_text

        chapters.append({"title": full_title, "content": content})
        
    return chapters

@app.post("/api/apply_effect")
async def apply_audio_effect(req: EffectRequest):
    file_path = os.path.join(OUTPUT_DIR, req.novel_name, 'wavs', req.chapter_name, req.file_name)
    if not os.path.exists(file_path):
        raise HTTPException(status_code=404, detail="éŸ³é¢‘æ–‡ä»¶æœªæ‰¾åˆ°ã€‚")
    try:
        # å¼ºåˆ¶æŒ‡å®š frame_rateï¼Œé¿å… pydub ä»æ–‡ä»¶ä¸­è¯»å–é”™è¯¯çš„å€¼
        audio = AudioSegment.from_wav(file_path)
        processed_audio = None
        
        if req.effect_type == 'phone':
            # *** æ ¸å¿ƒä¿®æ”¹ï¼šå¢å¼ºæ‰‹æœºé€šè¯æ•ˆæœ ***
            
            # 1. æ»¤æ³¢ï¼Œé™åˆ¶é¢‘ç‡èŒƒå›´ (300Hz - 3400Hz)
            processed_audio = high_pass_filter(audio, 300)
            processed_audio = low_pass_filter(processed_audio, 3400)
            
            # 2. ï¼ˆæ–°å¢ï¼‰è½»å¾®å¢åŠ éŸ³é‡ï¼Œæ¨¡æ‹Ÿå‹ç¼©æ•ˆæœ
            processed_audio = processed_audio + 3 
            
            # 3. ï¼ˆæ–°å¢ï¼‰é™ä½é‡‡æ ·ç‡å’Œæ¯”ç‰¹æ·±åº¦ï¼Œäº§ç”Ÿâ€œlo-fiâ€æ„Ÿ
            #    å°†é‡‡æ ·ç‡é™è‡³ 8000 Hzï¼Œè¿™æ˜¯ç”µè¯è¯­éŸ³çš„æ ‡å‡†
            processed_audio = processed_audio.set_frame_rate(8000)
            #    å†å‡å›åŸå§‹é‡‡æ ·ç‡ï¼Œpydubä¼šè‡ªåŠ¨è¿›è¡Œé‡é‡‡æ ·ï¼Œè¿™ä¸ªè¿‡ç¨‹ä¼šå¸¦æ¥ç‹¬ç‰¹çš„æ•°ç æ„Ÿ
            processed_audio = processed_audio.set_frame_rate(audio.frame_rate)
            
        elif req.effect_type == 'megaphone':
            # å¢å¼ºå–‡å­æ•ˆæœ
            processed_audio = high_pass_filter(audio, 500)
            processed_audio = low_pass_filter(processed_audio, 5000)
            # å¢åŠ ä¸€ç‚¹å¤±çœŸæ„Ÿï¼Œé€šè¿‡è½»å¾®çš„è¿‡è½½
            processed_audio = processed_audio.apply_gain_stereo(+6).compress_dynamic_range(threshold=-10.0)
            
        elif req.effect_type == 'reverb':
            # å¢å¼ºæ··å“æ•ˆæœ
            # åˆ›å»ºä¸€ä¸ªæ›´å¼±ã€æ›´å»¶è¿Ÿçš„å›å£°
            reverb_audio_1 = audio - 18
            reverb_audio_2 = audio - 24
            # æ··åˆä¸»éŸ³è½¨å’Œä¸¤ä¸ªå»¶è¿Ÿçš„å›å£°
            processed_audio = audio.overlay(reverb_audio_1, position=150)
            processed_audio = processed_audio.overlay(reverb_audio_2, position=300)
            
        else:
            raise HTTPException(status_code=400, detail="æœªçŸ¥çš„ç‰¹æ•ˆç±»å‹ã€‚")

        if processed_audio:
            # å¯¼å‡ºå‰è¿›è¡Œæ ‡å‡†åŒ–ï¼Œé˜²æ­¢å‰Šæ³¢
            processed_audio = normalize(processed_audio)
            processed_audio.export(file_path, format="wav")
            # è¿”å›ä¸€ä¸ªæ›´å…·ä½“çš„æˆåŠŸæ¶ˆæ¯
            effect_name_map = {"phone": "æ‰‹æœºé€šè¯", "megaphone": "å–‡å­å–Šè¯", "reverb": "å®¤å†…å›å£°"}
            return {"status": "success", "message": f"'{effect_name_map.get(req.effect_type, req.effect_type)}' ç‰¹æ•ˆå·²åº”ç”¨ã€‚"}
        else:
            raise Exception("å¤„ç†éŸ³é¢‘å¤±è´¥ã€‚")
            
    except Exception as e:
        logger.error(f"å¤„ç†ç‰¹æ•ˆå¤±è´¥: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"æœåŠ¡å™¨å¤„ç†ç‰¹æ•ˆå¤±è´¥: {e}")

def trim_leading_silence(sound, silence_threshold=-50.0, chunk_size=10):
    """
    Trims silence from the beginning of a Pydub AudioSegment.
    """
    trim_ms = detect_leading_silence(
        sound,
        silence_threshold=silence_threshold,
        chunk_size=chunk_size
    )
    return sound[trim_ms:]

@app.post("/api/generate_choral_effect")
async def generate_choral_effect(req: ChoralRequest):
    if len(req.selected_timbres) < 2:
        raise HTTPException(status_code=400, detail="è¯·è‡³å°‘é€‰æ‹©ä¸¤ä¸ªéŸ³è‰²ã€‚")

    # 1. åŠ è½½é…ç½®å¹¶ç¡®å®š TTS æœåŠ¡ endpoint
    try:
        with open(CONFIG_FILE, 'r', encoding='utf-8') as f: config = json.load(f)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")

    tts_models_config = config.get("tts_models", {})
    default_tts_model = config.get("general", {}).get("default_tts_model")
    model_id_to_use = req.tts_model if req.tts_model in tts_models_config else default_tts_model
    
    if not model_id_to_use or model_id_to_use not in tts_models_config:
        raise HTTPException(status_code=400, detail="æœªæ‰¾åˆ°æœ‰æ•ˆçš„ç›®æ ‡ TTS æ¨¡å‹é…ç½®ã€‚")
    
    model_endpoint = tts_models_config[model_id_to_use].get("endpoint")
    if not model_endpoint:
        raise HTTPException(status_code=500, detail=f"æ¨¡å‹ '{model_id_to_use}' æœªé…ç½® endpointã€‚")

    # 2. ä¸ºæ¯ä¸ªéŸ³è‰²è°ƒç”¨ TTS å¾®æœåŠ¡ç”Ÿæˆå•äººè¯­éŸ³
    request_temp_dir = os.path.join(TEMP_DIR, f"choral_{uuid.uuid4()}")
    os.makedirs(request_temp_dir, exist_ok=True)
    generated_wav_paths = []
    
    try:
        for timbre_name in req.selected_timbres:
            logger.info(f"ä¸ºåˆå£°æ•ˆæœç”ŸæˆéŸ³è‰² '{timbre_name}' (ä½¿ç”¨æ¨¡å‹: {model_id_to_use})")
            
            # a. åŠ è½½å‚è€ƒéŸ³é¢‘å’Œæ–‡æœ¬
            timbre_dir = os.path.join(WAV_DIR, timbre_name)
            prompt_wav_path = os.path.join(timbre_dir, "1.wav")
            prompt_txt_path = os.path.join(timbre_dir, "1.txt")
            if not (os.path.exists(prompt_wav_path) and os.path.exists(prompt_txt_path)):
                logger.warning(f"éŸ³è‰² '{timbre_name}' æ–‡ä»¶ä¸å®Œæ•´ï¼Œå·²è·³è¿‡ã€‚")
                continue
            
            with open(prompt_wav_path, "rb") as f_wav:
                prompt_audio_b64 = base64.b64encode(f_wav.read()).decode('utf-8')
            with open(prompt_txt_path, 'r', encoding='utf-8') as f_txt:
                prompt_text = f_txt.read()

            # b. æ„å»º payload å¹¶è°ƒç”¨å¾®æœåŠ¡
            payload = {
                "tts_text": req.tts_text,
                "prompt_audio": prompt_audio_b64,
                "prompt_text": prompt_text
            }
            response = requests.post(model_endpoint, json=payload, timeout=300)
            response.raise_for_status()
            tts_response_data = response.json()

            if tts_response_data.get("status") == "success" and tts_response_data.get("audio"):
                audio_data = base64.b64decode(tts_response_data["audio"])
                temp_wav_path = os.path.join(request_temp_dir, f"{timbre_name}.wav")
                with open(temp_wav_path, "wb") as f_out:
                    f_out.write(audio_data)
                generated_wav_paths.append(temp_wav_path)
            else:
                logger.warning(f"éŸ³è‰² '{timbre_name}' ç”Ÿæˆå¤±è´¥: {tts_response_data.get('message')}")

        if not generated_wav_paths:
            raise Exception("æ‰€æœ‰é€‰å®šéŸ³è‰²çš„è¯­éŸ³å‡ç”Ÿæˆå¤±è´¥ã€‚")

        # 3. æ··åˆéŸ³é¢‘ (é€»è¾‘ä¸å˜)
        logger.info(f"æ­£åœ¨ä½¿ç”¨é«˜çº§æ··åˆæŠ€æœ¯å¤„ç† {len(generated_wav_paths)} ä¸ªéŸ³è½¨...")
        segments = [AudioSegment.from_wav(p) for p in generated_wav_paths]
        max_duration = max(s.duration_seconds for s in segments)
        canvas = AudioSegment.silent(duration=int(max_duration * 1000) + 100, frame_rate=segments[0].frame_rate)
        canvas = canvas.set_channels(2)
        for segment in segments:
            if segment.channels == 1: segment = segment.set_channels(2)
            random_gain = -6 - random.uniform(0, 4)
            processed_segment = segment.apply_gain(random_gain).pan(random.uniform(-0.8, 0.8))
            canvas = canvas.overlay(processed_segment, position=random.randint(5, 30))

        # 4. ä¿å­˜æœ€ç»ˆæ–‡ä»¶
        final_audio = normalize(canvas)
        safe_speaker = "".join(c for c in req.original_speaker if c.isalnum() or c in " _-").rstrip()
        safe_timbre = "".join(c for c in req.original_timbre if c.isalnum() or c in " _-").rstrip()
        output_wav_name = f"{req.row_index:04d}-{safe_speaker}-{safe_timbre}.wav"
        wav_output_dir = os.path.join(OUTPUT_DIR, req.novel_name, 'wavs', req.chapter_name)
        output_full_path = os.path.join(wav_output_dir, output_wav_name)
        final_audio.export(output_full_path, format="wav")
        
        return {"status": "success", "message": "å¤šäººåŒå£°æ•ˆæœç”ŸæˆæˆåŠŸï¼", "file_name": output_wav_name}

    finally:
        if os.path.exists(request_temp_dir):
            shutil.rmtree(request_temp_dir)

            
# =================================================================
#               CORE API ENDPOINTS
# =================================================================
@app.post("/api/upload_txt_novel")
async def upload_txt_novel(file: UploadFile = File(...)):
    novel_name = os.path.splitext(file.filename)[0]
    project_dir = os.path.join(PROJECTS_DIR, novel_name)
    os.makedirs(project_dir, exist_ok=True)
    source_path = os.path.join(project_dir, 'source.txt')
    marker_path = os.path.join(project_dir, '.is_txt_project')
    
    try:
        content_bytes = await file.read()
        
        # --- æ ¸å¿ƒä¿®å¤ï¼šä¼˜åŒ–è§£ç é€»è¾‘ï¼Œä¸ä¾èµ– chardet ---
        content_text = ""
        # å®šä¹‰ä¸€ä¸ªæŒ‰æˆåŠŸç‡æ’åºçš„ç¼–ç å°è¯•åˆ—è¡¨
        encodings_to_try = [
            'utf-8-sig',  # ä¼˜å…ˆå¤„ç†å¸¦ BOM çš„ UTF-8 (Windows è®°äº‹æœ¬å¸¸è§)
            'utf-8',      # æ ‡å‡† UTF-8
            'gb18030',    # æœ€å®½å®¹çš„ä¸­æ–‡ç¼–ç ï¼Œå®Œå…¨å…¼å®¹ GBK å’Œ GB2312
            'big5'        # å¤‡é€‰ï¼šç¹ä½“ä¸­æ–‡ç¼–ç 
        ]
        
        for encoding in encodings_to_try:
            try:
                content_text = content_bytes.decode(encoding)
                logger.info(f"æˆåŠŸä½¿ç”¨ç¼–ç  '{encoding}' è§£ç ä¸Šä¼ çš„æ–‡ä»¶ '{file.filename}'ã€‚")
                break  # è§£ç æˆåŠŸï¼Œç«‹å³è·³å‡ºå¾ªç¯
            except (UnicodeDecodeError, TypeError):
                logger.warning(f"å°è¯•ä½¿ç”¨ç¼–ç  '{encoding}' è§£ç å¤±è´¥ï¼Œç»§ç»­å°è¯•ä¸‹ä¸€ä¸ªã€‚")
                continue # ç»§ç»­å°è¯•åˆ—è¡¨ä¸­çš„ä¸‹ä¸€ä¸ªç¼–ç 
        
        # å¦‚æœæ‰€æœ‰å°è¯•éƒ½å¤±è´¥äº†ï¼Œæ‰§è¡Œæœ€ç»ˆçš„å›é€€æ–¹æ¡ˆ
        if not content_text:
            content_text = content_bytes.decode('utf-8', errors='replace')
            logger.error(f"æ‰€æœ‰ç¼–ç å°è¯•å‡å¤±è´¥ï¼Œå·²ä¸ºæ–‡ä»¶ '{file.filename}' å¼ºåˆ¶æ›¿æ¢æœªçŸ¥å­—ç¬¦ã€‚")
        # --- ä¿®å¤ç»“æŸ ---

        # å°†æ­£ç¡®è§£ç çš„ Unicode å­—ç¬¦ä¸²ï¼Œç»Ÿä¸€ä»¥æ ‡å‡†çš„ UTF-8 æ ¼å¼å†™å…¥æ–‡ä»¶
        with open(source_path, "w", encoding="utf-8") as buffer:
            buffer.write(content_text)
        
        with open(marker_path, 'w') as f:
            f.write('')
        
        chapters = get_chapters_from_txt(content_text)
        
        chapters_cache_path = os.path.join(project_dir, 'chapters_cache.json')
        chapters_to_cache = [{"title": chap["title"]} for chap in chapters]
        with open(chapters_cache_path, 'w', encoding='utf-8') as f:
            json.dump(chapters_to_cache, f, ensure_ascii=False, indent=2)
        logger.info(f"ç« èŠ‚åˆ—è¡¨å·²ç¼“å­˜åˆ°: {chapters_cache_path}")

        chapters_for_frontend = [{"id": i, "title": chap["title"]} for i, chap in enumerate(chapters)]

        return {
            "status": "success", 
            "message": f"å°è¯´ '{novel_name}' å·²æˆåŠŸä¸Šä¼ å¹¶ç»Ÿä¸€è½¬æ¢ä¸ºUTF-8ã€‚",
            "chapters": chapters_for_frontend
        }
    except Exception as e:
        # æ¸…ç†å¯èƒ½å·²åˆ›å»ºçš„æ–‡ä»¶
        if os.path.exists(project_dir):
            # ä¸ºäº†å®‰å…¨ï¼Œè¿™é‡Œå¯ä»¥é€‰æ‹©æ€§åˆ é™¤ï¼Œæˆ–è€…åœ¨å¼€å‘é˜¶æ®µä¿ç•™ä»¥ä¾¿è°ƒè¯•
            # shutil.rmtree(project_dir) 
            pass
        logger.error(f"å¤„ç†ä¸Šä¼ çš„TXTæ–‡ä»¶å¤±è´¥: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"ä¿å­˜æˆ–å¤„ç†æºæ–‡ä»¶å¤±è´¥: {e}")

@app.get("/api/list_novels")
async def list_novels():
    if not os.path.isdir(PROJECTS_DIR): return {"novels_details": {}}
    novels_details = {}
    novel_names = [d for d in os.listdir(PROJECTS_DIR) if os.path.isdir(os.path.join(PROJECTS_DIR, d))]
    
    for name in novel_names:
        project_dir = os.path.join(PROJECTS_DIR, name)
        source_path = os.path.join(project_dir, 'source.txt')
        json_dir = os.path.join(project_dir, 'chapters_json')
        output_novel_dir = os.path.join(OUTPUT_DIR, name)

        # --- æ ¸å¿ƒä¿®æ”¹ 3: æ£€æŸ¥æ ‡è®°æ–‡ä»¶ ---
        is_txt_project = os.path.exists(os.path.join(project_dir, '.is_txt_project'))

        if not os.path.exists(source_path): continue
        
        try:
            with open(source_path, 'r', encoding='utf-8-sig', errors='ignore') as f:
                content = f.read()
            
            all_chapters_from_txt = get_chapters_from_txt(content)
            processed_jsons = set()
            if os.path.isdir(json_dir):
                processed_jsons = {f for f in os.listdir(json_dir) if f.endswith('.json')}
            
            spliced_audios = set()
            if os.path.isdir(output_novel_dir):
                spliced_audios = {f for f in os.listdir(output_novel_dir) if f.endswith(('.mp3', '.wav', '.m4a', '.ogg'))}

            chapter_details = []
            for i, chap in enumerate(all_chapters_from_txt):
                safe_title = "".join(c for c in chap['title'] if c.isalnum() or c in " _-").rstrip()
                is_processed = f"{safe_title}.json" in processed_jsons
                is_spliced = any(f.startswith(safe_title) for f in spliced_audios)
                
                # --- æ ¸å¿ƒä¿®æ”¹ 4: ç»Ÿä¸€è¿”å›ç»“æ„ ---
                chapter_info = {
                    "id": i,  # ä¸ºTXTé¡¹ç›®æä¾›å”¯ä¸€ID
                    "title": chap['title'], 
                    "processed": is_processed, 
                    "spliced": is_spliced
                }
                chapter_details.append(chapter_info)
            
            # --- æ ¸å¿ƒä¿®æ”¹ 5: å°† isTxtProject æ ‡è®°æ·»åŠ åˆ°å“åº”ä¸­ ---
            novels_details[name] = {
                "chapters": chapter_details,
                "isTxtProject": is_txt_project
            }
        except Exception as e:
            logger.error(f"Error processing details for novel '{name}': {e}")
            continue
            
    return {"novels_details": novels_details}


@app.get("/api/get_character_profile")
async def get_character_profile(novel_name: str, character_name: str):
    decoded_char_name = urllib.parse.unquote(character_name)
    profiles_path = os.path.join(PROJECTS_DIR, novel_name, 'character_profiles.json')
    if not os.path.exists(profiles_path): raise HTTPException(status_code=404, detail="è¯¥å°è¯´çš„è§’è‰²ç®€ä»‹æ–‡ä»¶æœªæ‰¾åˆ°ã€‚")
    try:
        with open(profiles_path, 'r', encoding='utf-8') as f: profiles = json.load(f)
        profile = profiles.get(decoded_char_name)
        if not profile: raise HTTPException(status_code=404, detail=f"æœªæ‰¾åˆ°è§’è‰² '{decoded_char_name}' çš„ç®€ä»‹ã€‚")
        return profile
    except Exception as e: raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/get_novel_content")
async def get_novel_content(filepath: str):
    # filepath is expected to be novel_name/chapter_name.json
    file_path = os.path.join(PROJECTS_DIR, filepath)
    if not os.path.abspath(file_path).startswith(os.path.abspath(PROJECTS_DIR)):
        raise HTTPException(status_code=403, detail="ç¦æ­¢è®¿é—®ã€‚")
    if not os.path.exists(file_path):
        raise HTTPException(status_code=404, detail="å°è¯´æ–‡ä»¶æœªæ‰¾åˆ°ã€‚")
    return FileResponse(file_path)

@app.post("/api/get_characters_in_chapters")
async def get_characters_in_chapters(req: CharactersInChaptersRequest):
    if not req.chapter_files: return {"characters": []}
    all_speakers = set()
    json_dir = os.path.join(PROJECTS_DIR, req.novel_name, 'chapters_json')
    for chapter_file in req.chapter_files:
        file_path = os.path.join(json_dir, chapter_file)
        if not os.path.abspath(file_path).startswith(os.path.abspath(json_dir)): continue
        try:
            if os.path.exists(file_path):
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    for item in data:
                        if 'speaker' in item: all_speakers.add(item['speaker'])
        except Exception as e:
            logger.error(f"Error reading or parsing {file_path}: {e}")
            continue
    return {"characters": sorted(list(all_speakers))}

@app.get("/api/get_config")
async def get_config(novel_name: str):
    config_path = os.path.join(PROJECTS_DIR, novel_name, 'character_timbres.json')
    if not os.path.exists(config_path):
        return JSONResponse(content={})
    return FileResponse(config_path)

@app.post("/api/update_config")
async def update_config(req: UpdateConfigRequest):
    project_dir = os.path.join(PROJECTS_DIR, req.novel_name)
    if not os.path.isdir(project_dir):
        raise HTTPException(status_code=404, detail="å°è¯´é¡¹ç›®æœªæ‰¾åˆ°ã€‚")
    config_path = os.path.join(project_dir, 'character_timbres.json')
    try:
        with open(config_path, "w", encoding="utf-8") as f:
            json.dump(req.config_data, f, ensure_ascii=False, indent=2)
        return {"status": "success", "message": f"å°è¯´ '{req.novel_name}' çš„éŸ³è‰²é…ç½®å·²ä¿å­˜ã€‚"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æœåŠ¡å™¨ç«¯å†™å…¥æ–‡ä»¶å¤±è´¥: {e}")

@app.post("/api/tts_v2")
async def text_to_speech_v2(req: TTSRequestV2):
    try:
        # 1. åŠ è½½é…ç½® (é€»è¾‘ä¸å˜)
        with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
            config = json.load(f)

        # 2. ç¡®å®šè¦ä½¿ç”¨çš„ TTS æ¨¡å‹å’Œå…¶ endpoint (é€»è¾‘ä¸å˜)
        tts_models_config = config.get("tts_models", {})
        default_tts_model = config.get("general", {}).get("default_tts_model")
        
        model_id_to_use = req.tts_model if req.tts_model in tts_models_config else default_tts_model
        
        if not model_id_to_use or model_id_to_use not in tts_models_config:
            raise HTTPException(status_code=400, detail="æœªæ‰¾åˆ°æœ‰æ•ˆçš„ç›®æ ‡ TTS æ¨¡å‹é…ç½®ã€‚")
            
        model_endpoint = tts_models_config[model_id_to_use].get("endpoint")
        if not model_endpoint:
            raise HTTPException(status_code=500, detail=f"æ¨¡å‹ '{model_id_to_use}' æœªé…ç½® endpointã€‚")

        try:
            timbre_dir = os.path.join(WAV_DIR, req.timbre)
            prompt_wav_path = os.path.join(timbre_dir, "1.wav")
            prompt_txt_path = os.path.join(timbre_dir, "1.txt")
            
            if not os.path.exists(prompt_wav_path) or not os.path.exists(prompt_txt_path):
                raise FileNotFoundError(f"éŸ³è‰² '{req.timbre}' çš„å‚è€ƒæ–‡ä»¶ä¸å®Œæ•´ã€‚")

            with open(prompt_wav_path, "rb") as f_wav:
                prompt_audio_b64 = base64.b64encode(f_wav.read()).decode('utf-8')
            with open(prompt_txt_path, 'r', encoding='utf-8') as f_txt:
                prompt_text = f_txt.read()
        except FileNotFoundError as e:
            logger.error(f"åŠ è½½éŸ³è‰²æ–‡ä»¶å¤±è´¥: {e}")
            raise HTTPException(status_code=404, detail=str(e))
        except Exception as e:
            logger.error(f"å¤„ç†éŸ³è‰²æ–‡ä»¶æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", exc_info=True)
            raise HTTPException(status_code=500, detail=f"æœåŠ¡å™¨å¤„ç†éŸ³è‰²æ–‡ä»¶å¤±è´¥: {e}")
            
        processed_tts_text = apply_replacement_rules(req.tts_text, req.novel_name)
        
        final_tone = req.tone
        final_intensity = req.intensity

        emo_audio_prompt_b64 = None
        logger.info(f"æƒ…ç»ªå‚è€ƒéŸ³é¢‘: {req.emo_audio_prompt}")
        if req.inference_mode == 'emo_prompt' and req.emo_audio_prompt:
            try:
                # æ„å»ºæƒ…ç»ªå‚è€ƒéŸ³é¢‘åœ¨æœ¬æœåŠ¡å™¨ä¸Šçš„å®Œæ•´è·¯å¾„
                emo_prompt_path = os.path.join(EMO_PROMPTS_DIR, req.emo_audio_prompt)
                
                if os.path.exists(emo_prompt_path):
                    # è¯»å–æ–‡ä»¶å†…å®¹å¹¶è¿›è¡ŒBase64ç¼–ç 
                    with open(emo_prompt_path, "rb") as f_emo:
                        emo_audio_prompt_b64 = base64.b64encode(f_emo.read()).decode('utf-8')
                    logger.info(f"å·²æˆåŠŸåŠ è½½å¹¶ç¼–ç æƒ…ç»ªå‚è€ƒéŸ³é¢‘: {req.emo_audio_prompt}")
                else:
                    logger.warning(f"æƒ…ç»ªå‚è€ƒéŸ³é¢‘æ–‡ä»¶æœªæ‰¾åˆ°: {emo_prompt_path}ï¼Œå°†å¿½ç•¥æ­¤è®¾ç½®ã€‚")
            except Exception as e:
                logger.error(f"å¤„ç†æƒ…ç»ªå‚è€ƒéŸ³é¢‘æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
                # å³ä½¿å‡ºé”™ï¼Œä¹Ÿç»§ç»­æµç¨‹ï¼Œåªæ˜¯ä¸ä½¿ç”¨æƒ…ç»ªå‚è€ƒ
                pass
                
        # è§„åˆ™ 1: å¦‚æœ speaker æ˜¯â€œæ—ç™½â€
        if req.speaker == "æ—ç™½":
            final_tone = "å¹³é™"
            final_intensity = 5
        else:
            print("tone is ", final_tone)
        
        # --- 3. æ„å»ºåŒ…å«æ‰€æœ‰æ¨¡å¼ä¿¡æ¯çš„å®Œæ•´ payload ---
        payload = {
            "tts_text": processed_tts_text,
            "prompt_audio": prompt_audio_b64,
            "prompt_text": prompt_text,
            "inference_mode": req.inference_mode,
            "instruct_text": req.instruct_text,
            "tone": final_tone,          
            "intensity": final_intensity,
            "emo_audio_prompt": emo_audio_prompt_b64,
            "emo_weight": req.emo_weight
        }
                    
        best_audio_data_for_saving = None
        min_tail_dbfs_found_in_retries = float('inf') # åˆå§‹åŒ–ä¸ºæ­£æ— ç©·å¤§

        # åç«¯å†…éƒ¨çš„é‡è¯•å¾ªç¯
        for attempt in range(TTS_GENERATION_MAX_RETRIES):
            logger.info(f"æ­£åœ¨å‘ TTS æœåŠ¡ '{model_id_to_use}' ({model_endpoint}) å‘é€è¯·æ±‚ (Line ID: {req.line_identifier}, å°è¯•: {attempt + 1}/{TTS_GENERATION_MAX_RETRIES})ï¼Œæ¨¡å¼: '{req.inference_mode}'...")
            
            current_attempt_audio_data = None # å­˜å‚¨å½“å‰å°è¯•è·å–çš„éŸ³é¢‘æ•°æ®
            current_attempt_tail_dbfs = float('inf') # å­˜å‚¨å½“å‰å°è¯•çš„ç»“å°¾èƒ½é‡

            try:
                response = requests.post(model_endpoint, json=payload, timeout=300)
                response.raise_for_status() # å¦‚æœçŠ¶æ€ç ä¸æ˜¯ 2xxï¼Œåˆ™æŠ›å‡ºå¼‚å¸¸
                
                tts_response_data = response.json()
                if tts_response_data.get("status") != "success":
                    logger.warning(f"TTS æœåŠ¡è¿”å›é”™è¯¯ (å°è¯•: {attempt + 1}): {tts_response_data.get('message', 'æœªçŸ¥é”™è¯¯')}")
                    await asyncio.sleep(2) # çŸ­æš‚ç­‰å¾…åé‡è¯•
                    continue # ç»§ç»­ä¸‹ä¸€æ¬¡é‡è¯•

                audio_data_b64 = tts_response_data.get("audio")
                if not audio_data_b64:
                    logger.warning(f"TTS æœåŠ¡å“åº”ä¸­ç¼ºå°‘éŸ³é¢‘æ•°æ® (å°è¯•: {attempt + 1})ã€‚")
                    await asyncio.sleep(2) # çŸ­æš‚ç­‰å¾…åé‡è¯•
                    continue # ç»§ç»­ä¸‹ä¸€æ¬¡é‡è¯•
                    
                current_attempt_audio_data = base64.b64decode(audio_data_b64)

                # NEW: éŸ³é¢‘ç»“å°¾èƒ½é‡åˆ¤æ–­é€»è¾‘
                try:
                    audio_segment = AudioSegment.from_file(io.BytesIO(current_attempt_audio_data), format="wav")
                    audio_duration_ms = len(audio_segment) # è·å–éŸ³é¢‘æ—¶é•¿ï¼ˆæ¯«ç§’ï¼‰

                    # å®‰å…¨æ£€æŸ¥ï¼šå¯¹äºæçŸ­éŸ³é¢‘ï¼Œä¸è¿›è¡Œç»“å°¾èƒ½é‡åˆ†æï¼Œç›´æ¥è®¤ä¸ºæˆåŠŸ
                    if audio_duration_ms < TTS_TAIL_ANALYSIS_DURATION_MS:
                        logger.info(f"  TTSéŸ³é¢‘æçŸ­ (æ€»æ—¶é•¿ {audio_duration_ms}ms)ï¼Œè·³è¿‡ç»“å°¾èƒ½é‡åˆ†æã€‚è®¤ä¸ºæ­£å¸¸ã€‚")
                        
                        # è§†ä¸ºæœ€ä½³ç»“æœï¼Œç›´æ¥è·³å‡ºé‡è¯•å¾ªç¯
                        best_audio_data_for_saving = current_attempt_audio_data
                        break 
                    
                    # æå–éŸ³é¢‘ç»“å°¾ç‰‡æ®µå¹¶è®¡ç®—èƒ½é‡
                    tail_segment = audio_segment[-TTS_TAIL_ANALYSIS_DURATION_MS:]
                    current_attempt_tail_dbfs = tail_segment.dBFS
                    
                    logger.info(f"  TTSéŸ³é¢‘æ—¶é•¿æ£€æŸ¥: æ–‡æœ¬é•¿åº¦ {len(processed_tts_text)}, æ€»æ—¶é•¿ {audio_duration_ms}ms, ç»“å°¾ {TTS_TAIL_ANALYSIS_DURATION_MS}ms èƒ½é‡ {current_attempt_tail_dbfs:.2f} dBFSã€‚")

                    if current_attempt_tail_dbfs > TTS_TAIL_ENERGY_THRESHOLD_DBFS:
                        logger.warning(f"  TTSéŸ³é¢‘ç»“å°¾èƒ½é‡è¿‡é«˜ ({current_attempt_tail_dbfs:.2f} dBFS > {TTS_TAIL_ENERGY_THRESHOLD_DBFS} dBFS)ï¼Œå¯èƒ½è¢«æˆªæ–­ã€‚")
                        
                        # å¦‚æœå½“å‰å°è¯•æ˜¯è¿„ä»Šä¸ºæ­¢â€œæœ€ä¸æˆ›ç„¶è€Œæ­¢â€çš„ï¼Œå°±æ›´æ–°æœ€ä½³ç»“æœ
                        if current_attempt_tail_dbfs < min_tail_dbfs_found_in_retries:
                            min_tail_dbfs_found_in_retries = current_attempt_tail_dbfs
                            best_audio_data_for_saving = current_attempt_audio_data
                            logger.info(f"    æ›´æ–°æœ€ä½³éŸ³é¢‘ç»“æœ (å½“å‰èƒ½é‡: {current_attempt_tail_dbfs:.2f} dBFS)ã€‚")

                        await asyncio.sleep(2) # çŸ­æš‚ç­‰å¾…åé‡è¯•
                        continue # ç»§ç»­ä¸‹ä¸€æ¬¡é‡è¯•
                    else:
                        logger.info(f"  TTSéŸ³é¢‘ç»“å°¾èƒ½é‡æ£€æŸ¥é€šè¿‡ã€‚è®¤ä¸ºæ­£å¸¸ã€‚")
                        # æˆåŠŸç”Ÿæˆä¸”é€šè¿‡æ£€æŸ¥ï¼Œè¿™æ˜¯æœ€ç†æƒ³çš„æƒ…å†µï¼Œç›´æ¥ä¿å­˜å¹¶è·³å‡º
                        best_audio_data_for_saving = current_attempt_audio_data
                        break 

                except Exception as e:
                    logger.error(f"TTSéŸ³é¢‘ç»“å°¾èƒ½é‡åˆ†ææ—¶å‘ç”Ÿé”™è¯¯: {e} (å°è¯•: {attempt + 1})", exc_info=True)
                    await asyncio.sleep(2) # çŸ­æš‚ç­‰å¾…åé‡è¯•
                    continue # ç»§ç»­ä¸‹ä¸€æ¬¡é‡è¯•

            except requests.exceptions.RequestException as e:
                logger.error(f"è°ƒç”¨ TTS å¾®æœåŠ¡å¤±è´¥ (å°è¯•: {attempt + 1}): {e}", exc_info=True)
                if isinstance(e, requests.exceptions.ConnectionError):
                    logger.warning("æ£€æµ‹åˆ°è¿æ¥é”™è¯¯ï¼Œå°†ç«‹å³ä¸­æ­¢å¯¹æ­¤è¡Œçš„é‡è¯•ã€‚")
                    # ç›´æ¥æŠ›å‡ºå¼‚å¸¸ï¼Œè®©å¤–å±‚çš„ processAllAndSplice èƒ½å¤Ÿæ•è·å¹¶å¤„ç†
                    raise HTTPException(status_code=503, detail=f"æ— æ³•è¿æ¥åˆ°ä¸‹æ¸¸TTSæœåŠ¡: {e}")
                await asyncio.sleep(2) # çŸ­æš‚ç­‰å¾…åé‡è¯•
                continue # ç»§ç»­ä¸‹ä¸€æ¬¡é‡è¯•
            except Exception as e:
                logger.error(f"TTS_v2 API å†…éƒ¨å¤„ç†å¤±è´¥ (å°è¯•: {attempt + 1}): {e}", exc_info=True)
                await asyncio.sleep(2) # çŸ­æš‚ç­‰å¾…åé‡è¯•
                continue # ç»§ç»­ä¸‹ä¸€æ¬¡é‡è¯•
        
        # å¾ªç¯ç»“æŸåï¼Œæ£€æŸ¥æ˜¯å¦æœ‰å¯ä¿å­˜çš„éŸ³é¢‘æ•°æ®
        if best_audio_data_for_saving is None:
            logger.error(f"TTSç”Ÿæˆå¤±è´¥ï¼Œæ‰€æœ‰ {TTS_GENERATION_MAX_RETRIES} æ¬¡å°è¯•å‡æœªèƒ½è·å–åˆ°æœ‰æ•ˆçš„éŸ³é¢‘æ•°æ®ã€‚")
            raise HTTPException(status_code=500, detail="TTSç”Ÿæˆå¤±è´¥ï¼Œæœªèƒ½è·å–åˆ°ä»»ä½•éŸ³é¢‘æ•°æ®ã€‚")

        # 5. ä¿å­˜éŸ³é¢‘æ–‡ä»¶ (ä¿å­˜æœ€ä½³ç»“æœ)
        wav_output_dir = os.path.join(OUTPUT_DIR, req.novel_name, 'wavs', req.chapter_name)
        os.makedirs(wav_output_dir, exist_ok=True)        
        for old_wav_file in os.listdir(wav_output_dir):
            if old_wav_file.startswith(req.line_identifier + '-'):
                old_file_path = os.path.join(wav_output_dir, old_wav_file)
                try:
                    os.remove(old_file_path)
                    logger.info(f"å·²åˆ é™¤æ—§éŸ³é¢‘æ–‡ä»¶: {old_file_path}")
                except Exception as e:
                    logger.warning(f"åˆ é™¤æ—§éŸ³é¢‘æ–‡ä»¶å¤±è´¥ '{old_file_path}': {e}")
        safe_speaker = sanitize_for_filename(req.speaker)
        safe_timbre = sanitize_for_filename(req.timbre)
        output_wav_name = f"{req.line_identifier}-{safe_speaker}-{safe_timbre}.wav"
        output_full_path = os.path.join(wav_output_dir, output_wav_name)
        
        with open(output_full_path, "wb") as f:
            f.write(best_audio_data_for_saving) # ä¿å­˜æœ€ä½³æ•°æ®
        
        return JSONResponse(content={"status": "success", "file_name": output_wav_name})
    
    except requests.exceptions.RequestException as e:
        logger.error(f"è°ƒç”¨ TTS å¾®æœåŠ¡å¤±è´¥: {e}", exc_info=True)
        raise HTTPException(status_code=503, detail=f"æ— æ³•è¿æ¥åˆ° TTS æœåŠ¡: {e}")
    except Exception as e:
        logger.error(f"TTS v2 API å¤±è´¥: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"æœåŠ¡å™¨å†…éƒ¨é”™è¯¯: {e}")
        
@app.post("/api/splice_audio")
async def splice_audio(req: SpliceRequest):
    # --- 1. æ„å»ºåŸºç¡€è·¯å¾„ (ä¿æŒä¸å˜) ---
    project_dir = os.path.join(PROJECTS_DIR, req.novel_name)
    timbres_path = os.path.join(project_dir, 'character_timbres.json')
    
    safe_chapter_name = req.chapter_name 
    chapter_json_path = os.path.join(project_dir, 'chapters_json', f"{safe_chapter_name}.json")
    wav_input_dir = os.path.join(OUTPUT_DIR, req.novel_name, 'wavs', safe_chapter_name)
    final_output_dir = os.path.join(OUTPUT_DIR, req.novel_name)
    os.makedirs(final_output_dir, exist_ok=True)

    if not os.path.exists(chapter_json_path):
        raise HTTPException(status_code=404, detail=f"ç« èŠ‚JSONæ–‡ä»¶æœªæ‰¾åˆ°: {chapter_json_path}")

    try:
        # --- 2. è¯»å–é…ç½®æ–‡ä»¶å’Œç« èŠ‚æ•°æ® (ä¿æŒä¸å˜) ---
        character_timbres = {}
        if os.path.exists(timbres_path):
            with open(timbres_path, 'r', encoding='utf-8') as f:
                character_timbres = json.load(f)
        
        with open(chapter_json_path, 'r', encoding='utf-8') as f:
            chapter_data = json.load(f)

        # --- 3. åç«¯è‡ªå·±æ„å»ºæƒå¨çš„æ–‡ä»¶åˆ—è¡¨ (ä¿æŒä¸å˜) ---
        files_to_splice_authoritative = []
        for i, item in enumerate(chapter_data):
            speaker = item.get("speaker")
            line_id = item.get("line_id")
            if not line_id: #å¦‚æœ line_id ä¸¢å¤±ï¼Œåˆ™è·³è¿‡
                logger.warning(f"è·³è¿‡æ‹¼æ¥è¡Œ {i+1} ï¼Œå› ä¸ºç¼ºå°‘ line_idã€‚")
                continue
            
            timbre_to_use = item.get("timbre_override")
            if not timbre_to_use:
                timbre_to_use = character_timbres.get(speaker)

            if not speaker or not timbre_to_use:
                logger.warning(f"è·³è¿‡æ‹¼æ¥ç¬¬ {i+1} (Line ID: {line_id})è¡Œï¼Œå› ä¸ºç¼ºå°‘è§’è‰²æˆ–éŸ³è‰²ä¿¡æ¯ã€‚")
                continue
            
            safe_speaker = "".join(c for c in speaker if c.isalnum() or c in " _-").rstrip()
            safe_timbre = "".join(c for c in timbre_to_use if c.isalnum() or c in " _-").rstrip()
            wav_file_name = f"{line_id}-{safe_speaker}-{safe_timbre}.wav" 
            files_to_splice_authoritative.append(wav_file_name)

        # --- 4. æ‰§è¡Œæ‹¼æ¥ ---
        if not files_to_splice_authoritative:
            raise HTTPException(status_code=400, detail="æ ¹æ®ç« èŠ‚å†…å®¹ï¼Œæ²¡æœ‰æ‰¾åˆ°å¯æ‹¼æ¥çš„éŸ³é¢‘æ–‡ä»¶ã€‚")

        combined = AudioSegment.empty()
        for wav_file_name in files_to_splice_authoritative:
            wav_path = os.path.join(wav_input_dir, wav_file_name)
            if os.path.exists(wav_path):
                try:
                    combined += AudioSegment.from_wav(wav_path)
                except Exception as e:
                    logger.error(f"åŠ è½½ WAV æ–‡ä»¶å¤±è´¥ '{wav_path}': {e}")
            else:
                logger.warning(f"æ‹¼æ¥æ—¶æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œå·²è·³è¿‡: {wav_path}")
        
        # --- æ–°å¢ï¼šæ£€æŸ¥åˆå¹¶åçš„ AudioSegment æ˜¯å¦ä¸ºç©º ---
        if len(combined) == 0:
            raise HTTPException(status_code=404, detail="æ‰€æœ‰é¢„æœŸçš„WAVæ–‡ä»¶éƒ½ä¸å­˜åœ¨æˆ–æ— æ³•åŠ è½½ï¼Œæ— æ³•ç”Ÿæˆåˆå¹¶éŸ³é¢‘ã€‚")
        else:
            logger.info(f"æˆåŠŸåˆå¹¶éŸ³é¢‘ç‰‡æ®µã€‚æ€»æ—¶é•¿: {combined.duration_seconds:.2f} ç§’, å¸§ç‡: {combined.frame_rate}, å£°é“: {combined.channels}")

        with open(CONFIG_FILE, 'r', encoding='utf-8') as f: config = json.load(f)
        export_settings = config.get("audio_export", {"format": "mp3", "quality": "192k"})
        output_format = export_settings.get("format", "mp3")
        
        output_filename = f"{safe_chapter_name}.{output_format}"
        output_path = os.path.join(final_output_dir, output_filename)
        
        export_params = {}
        output_quality = export_settings.get("quality", "196k") # é»˜è®¤æ¯”ç‰¹ç‡ç¨å¾®æé«˜ä¸€ç‚¹ï¼Œæ›´é€šç”¨

        pydub_export_format = output_format 

        if output_format == 'mp3':
            export_params['bitrate'] = output_quality
        elif output_format == 'm4a':
            # å°†pydub.exportçš„formatå‚æ•°è®¾ç½®ä¸º'mp4'ï¼Œå› ä¸ºm4aæ˜¯mp4å®¹å™¨çš„ä¸€ç§
            pydub_export_format = 'mp4' 
            export_params['parameters'] = ["-c:a", "aac", "-b:a", output_quality]
            logger.info(f"M4A å¯¼å‡ºå‚æ•°: ä½¿ç”¨ FFmpeg æ ¼å¼ 'mp4', ç¼–ç å™¨ 'aac', æ¯”ç‰¹ç‡ '{output_quality}'.")
        elif output_format == 'ogg':
            export_params['codec'] = 'libvorbis'
            # quality å‚æ•°éœ€è¦å»æ‰ 'q'ï¼Œä¾‹å¦‚ä» 'q5' å˜ä¸º '5'
            export_params['parameters'] = ["-q:a", output_quality.replace('q','')]
        else:
            logger.warning(f"ä¸æ”¯æŒçš„è¾“å‡ºæ ¼å¼: {output_format}ï¼Œå°†å›é€€åˆ°é»˜è®¤çš„wavæ ¼å¼ã€‚")
            output_format = "wav" # æ›´æ–°output_formatä»¥ä¾¿ç”Ÿæˆæ­£ç¡®çš„æ–‡ä»¶å
            output_filename = f"{safe_chapter_name}.wav"
            pydub_export_format = "wav" # ç¡®ä¿pydub.exportä¹Ÿä½¿ç”¨'wav'
            # å¯¹äº WAV æ ¼å¼ï¼Œé€šå¸¸ä¸éœ€è¦é¢å¤–å‚æ•°ï¼Œpydub ä¼šå¾ˆå¥½åœ°å¤„ç†

        logger.info(f"æ­£åœ¨å°è¯•å¯¼å‡ºæ‹¼æ¥éŸ³é¢‘åˆ° {output_path}ï¼Œç»™pydubçš„æ ¼å¼å‚æ•°ä¸º: '{pydub_export_format}', FFmpegå‚æ•°: {export_params}")
        
        try:
            # --- å°† pydub_export_format ä¼ é€’ç»™ export æ–¹æ³• ---
            combined.export(output_path, format=pydub_export_format, **export_params)
            
            # --- å¯¼å‡ºåæ–‡ä»¶å¤§å°æ£€æŸ¥ ---
            if not os.path.exists(output_path) or os.path.getsize(output_path) == 0:
                raise Exception(f"éŸ³é¢‘æ–‡ä»¶ '{output_path}' åˆ›å»ºå¤±è´¥æˆ–å¤§å°ä¸º 0ã€‚FFmpeg å¯èƒ½æœªèƒ½æˆåŠŸç¼–ç ã€‚")

            logger.info(f"éŸ³é¢‘å¯¼å‡ºæˆåŠŸï¼š{output_path}, æ–‡ä»¶å¤§å°: {os.path.getsize(output_path)} å­—èŠ‚ã€‚")
                   
            relative_path = os.path.join(req.novel_name, output_filename).replace("\\", "/")
            return {"status": "success", "file_path": f"/output/{relative_path}"}

        except Exception as e:
            logger.error(f"å¯¼å‡ºéŸ³é¢‘åˆ° '{output_path}' å¤±è´¥: {e}", exc_info=True)
            # å¦‚æœæ–‡ä»¶åˆ›å»ºå¤±è´¥æˆ–å¤§å°ä¸º 0ï¼Œå°è¯•æ¸…ç†
            if os.path.exists(output_path) and os.path.getsize(output_path) == 0:
                logger.warning(f"æ£€æµ‹åˆ°é›¶å­—èŠ‚è¾“å‡ºæ–‡ä»¶ '{output_path}'ï¼Œå°è¯•åˆ é™¤ã€‚")
                try:
                    # ç­‰å¾…ä¸€å°æ®µæ—¶é—´ï¼Œç»™ç³»ç»Ÿé‡Šæ”¾æ–‡ä»¶å¥æŸ„çš„æœºä¼š
                    time.sleep(0.1) 
                    os.remove(output_path)
                    logger.info(f"å·²æˆåŠŸåˆ é™¤é›¶å­—èŠ‚çš„è¾“å‡ºæ–‡ä»¶: {output_path}")
                except PermissionError as pe:
                    logger.error(f"åˆ é™¤é›¶å­—èŠ‚æ–‡ä»¶ '{output_path}' æ—¶é‡åˆ°æƒé™é”™è¯¯: {pe}ã€‚æ–‡ä»¶å¯èƒ½ä»è¢«å…¶ä»–ç¨‹åºå ç”¨ã€‚", exc_info=True)
                except Exception as clean_e:
                    logger.error(f"åˆ é™¤é›¶å­—èŠ‚æ–‡ä»¶ '{output_path}' æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {clean_e}", exc_info=True)
            
            # æ— è®ºæ˜¯å¦æˆåŠŸæ¸…ç†äº†é›¶å­—èŠ‚æ–‡ä»¶ï¼Œéƒ½æŠ›å‡ºæœ€åˆçš„å¯¼å‡ºå¤±è´¥å¼‚å¸¸
            raise HTTPException(status_code=500, detail=f"æœåŠ¡å™¨å¯¼å‡ºéŸ³é¢‘å¤±è´¥: {e}")

    except Exception as e:
        logger.error(f"æ‹¼æ¥éŸ³é¢‘æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"æœåŠ¡å™¨æ‹¼æ¥éŸ³é¢‘å¤±è´¥: {e}")

@app.post("/api/check_files_exist")
async def check_files_exist(req: CheckFilesRequest):
    """
    æ¥æ”¶ä¸€ä¸ªæ–‡ä»¶ååˆ—è¡¨ï¼Œå¹¶åœ¨æŒ‡å®šçš„ å°è¯´/ç« èŠ‚ ç›®å½•ä¸­æ£€æŸ¥å®ƒä»¬æ˜¯å¦å­˜åœ¨ã€‚
    è¿”å›ä¸€ä¸ªåªåŒ…å«å®é™…å­˜åœ¨çš„æ–‡ä»¶åçš„åˆ—è¡¨ã€‚
    """
    # 1. æ„å»ºç›®æ ‡ç« èŠ‚çš„WAVæ–‡ä»¶ç›®å½•è·¯å¾„
    #    æ³¨æ„ï¼šreq.chapter_name åº”è¯¥æ˜¯å‰ç«¯ sanitizeTitleForFilename() å¤„ç†è¿‡çš„
    wav_chapter_dir = os.path.join(OUTPUT_DIR, req.novel_name, 'wavs', req.chapter_name)
    
    # 2. å®‰å…¨æ€§æ£€æŸ¥ï¼šç¡®ä¿è·¯å¾„åœ¨åˆæ³•çš„ OUTPUT_DIR å†…éƒ¨
    output_root = os.path.realpath(OUTPUT_DIR)
    target_dir_real = os.path.realpath(wav_chapter_dir)
    if not target_dir_real.startswith(output_root):
        logger.warning(f"æ–‡ä»¶æ£€æŸ¥è¯·æ±‚è¢«é˜»æ­¢ï¼ˆç›®å½•éå†ï¼‰: {req.novel_name}/{req.chapter_name}")
        raise HTTPException(status_code=403, detail="ç¦æ­¢è®¿é—®ã€‚")

    # 3. å¦‚æœç›®å½•æœ¬èº«ä¸å­˜åœ¨ï¼Œåˆ™ç›´æ¥è¿”å›ç©ºåˆ—è¡¨
    if not os.path.isdir(target_dir_real):
        return {"existing_files": []}

    # 4. é«˜æ•ˆåœ°éå†å‰ç«¯æä¾›æ–‡ä»¶ååˆ—è¡¨ï¼Œæ£€æŸ¥æ¯ä¸€ä¸ªæ–‡ä»¶æ˜¯å¦å­˜åœ¨
    existing_files = []
    
    for filename in req.filenames:
        # å¯¹ filename è¿›è¡Œä¸€äº›å®‰å…¨æ£€æŸ¥ï¼Œé˜²æ­¢å®ƒå°è¯•è®¿é—®çˆ¶ç›®å½•ç­‰
        if ".." in filename or os.path.isabs(filename):
            logger.warning(f"æ£€æµ‹åˆ°éæ³•æ–‡ä»¶åæ¨¡å¼ï¼Œå·²è·³è¿‡æ£€æŸ¥: {filename}")
            continue

        full_file_path = os.path.join(target_dir_real, filename)
        if os.path.exists(full_file_path):
            existing_files.append(filename)

    return {"existing_files": existing_files}
    
@app.get("/api/novel/{novel_name}/replace_dict")
async def get_novel_replace_dict(novel_name: str):
    """
    è·å–æŒ‡å®šå°è¯´çš„æ›¿æ¢è¯å…¸è§„åˆ™ã€‚
    """
    replace_dict_path = os.path.join(PROJECTS_DIR, novel_name, 'replace_dict.json')
    if not os.path.exists(replace_dict_path):
        return JSONResponse(content={"rules": []})
    
    try:
        with open(replace_dict_path, 'r', encoding='utf-8') as f:
            rules_data = json.load(f)
            if not isinstance(rules_data, dict) or "rules" not in rules_data or not isinstance(rules_data["rules"], list):
                logger.warning(f"æ›¿æ¢è¯å…¸æ–‡ä»¶ '{replace_dict_path}' ç»“æ„å¼‚å¸¸ï¼Œå·²é‡ç½®ä¸ºç©ºã€‚")
                return JSONResponse(content={"rules": []})
            return JSONResponse(content=rules_data)
    except (json.JSONDecodeError, FileNotFoundError) as e:
        logger.error(f"è¯»å–æˆ–è§£ææ›¿æ¢è¯å…¸æ–‡ä»¶ '{replace_dict_path}' å¤±è´¥: {e}")
        return JSONResponse(content={"rules": []})
    except Exception as e:
        logger.error(f"è·å–æ›¿æ¢è¯å…¸æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        raise HTTPException(status_code=500, detail=f"æœåŠ¡å™¨é”™è¯¯: {e}")

@app.post("/api/novel/{novel_name}/replace_dict")
async def update_novel_replace_dict(novel_name: str, req: UpdateReplaceDictRequest):
    """
    æ›´æ–°ï¼ˆä¿å­˜ï¼‰æŒ‡å®šå°è¯´çš„æ›¿æ¢è¯å…¸è§„åˆ™ã€‚
    """
    project_dir = os.path.join(PROJECTS_DIR, novel_name)
    if not os.path.isdir(project_dir):
        raise HTTPException(status_code=404, detail="å°è¯´é¡¹ç›®æœªæ‰¾åˆ°ã€‚")
        
    replace_dict_path = os.path.join(project_dir, 'replace_dict.json')
    
    try:
        # éªŒè¯æ¯ä¸ªè§„åˆ™çš„ç»“æ„
        for rule in req.rules:
            if not rule.original_word or not rule.replacement_word:
                raise HTTPException(status_code=400, detail="æ›¿æ¢è§„åˆ™ä¸­çš„ 'original_word' å’Œ 'replacement_word' ä¸èƒ½ä¸ºç©ºã€‚")

        serializable_rules = [rule.model_dump(mode='json') for rule in req.rules] 
        with open(replace_dict_path, 'w', encoding='utf-8') as f:
            json.dump({"rules": serializable_rules}, f, ensure_ascii=False, indent=4)
        return {"status": "success", "message": f"å°è¯´ã€Œ{novel_name}ã€çš„æ›¿æ¢è¯å…¸å·²ä¿å­˜ã€‚"}
    except HTTPException: # é‡æ–°æŠ›å‡ºè‡ªå®šä¹‰çš„HTTPException
        raise
    except Exception as e:
        logger.error(f"ä¿å­˜å°è¯´ã€Œ{novel_name}ã€æ›¿æ¢è¯å…¸å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æœåŠ¡å™¨é”™è¯¯: {e}")
        
@app.post("/api/update_chapter_content")
async def update_chapter_content(req: UpdateChapterRequest):
    """
    Receives updated chapter content (as a full JSON array) and overwrites the file on the server.
    This function also intelligently renames old WAV files (index-based) to new (line_id-based) names.
    """
    filepath = req.filepath
    
    # 1. Split the incoming path to construct the correct server path
    try:
        path_parts = filepath.split('/', 1)
        if len(path_parts) != 2:
            raise HTTPException(status_code=400, detail="Invalid filepath format. Expected 'novel_name/chapter.json'.")
        
        novel_name, chapter_filename = path_parts
        
        # Construct the full, correct path on the server
        chapter_json_path = os.path.join(PROJECTS_DIR, novel_name, 'chapters_json', chapter_filename) # <-- ä½¿ç”¨ chapter_json_path
    except Exception:
        raise HTTPException(status_code=400, detail="Could not parse filepath.")

    # 2. Security check to prevent directory traversal attacks
    project_root = os.path.abspath(PROJECTS_DIR)
    if not os.path.abspath(chapter_json_path).startswith(project_root): # <-- ä½¿ç”¨ chapter_json_path
        raise HTTPException(status_code=403, detail="ç¦æ­¢è®¿é—®ã€‚")

    if not os.path.exists(os.path.dirname(chapter_json_path)):
        raise HTTPException(status_code=404, detail=f"é¡¹ç›®æˆ–ç« èŠ‚ç›®å½•æœªæ‰¾åˆ°: {os.path.dirname(chapter_json_path)}")
        
    # --- è·å–ç« èŠ‚å¯¹åº”çš„ WAV ç›®å½•å’ŒéŸ³è‰²é…ç½® ---
    safe_chapter_name = os.path.splitext(chapter_filename)[0]
    wav_output_dir = os.path.join(OUTPUT_DIR, novel_name, 'wavs', safe_chapter_name)
    os.makedirs(wav_output_dir, exist_ok=True) 

    # åŠ è½½è§’è‰²éŸ³è‰²é…ç½®ï¼Œç”¨äºè§£æéŸ³è‰²
    character_timbres = {}
    timbres_config_path = os.path.join(PROJECTS_DIR, novel_name, 'character_timbres.json')
    if os.path.exists(timbres_config_path):
        try:
            with open(timbres_config_path, 'r', encoding='utf-8') as f:
                character_timbres = json.load(f)
        except Exception as e:
            logger.warning(f"åŠ è½½å°è¯´ '{novel_name}' çš„éŸ³è‰²é…ç½®å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨ç©ºé…ç½®ã€‚")

    # --- 3. WAV æ–‡ä»¶æ™ºèƒ½é‡å‘½åé€»è¾‘ ---
    renamed_count = 0
    
    for item in req.content:
        # æ£€æŸ¥æ˜¯å¦æ˜¯éœ€è¦é‡å‘½åçš„æ—§æ ¼å¼è¡Œ (æœ‰ originalIndex ä¸”æœ‰æ•ˆï¼Œå¹¶ä¸”æœ‰ line_id)
        if 'originalIndex' in item and item['originalIndex'] >= 0 and 'line_id' in item:
            
            original_index = item['originalIndex']
            line_id = item['line_id']
            speaker = item.get('speaker', 'æœªçŸ¥')
            timbre = item.get('timbre_override') or character_timbres.get(speaker, 'æœªçŸ¥')

            if timbre == 'æœªçŸ¥' or speaker == 'æœªçŸ¥':
                 continue

            # æ„é€ æ—§æ–‡ä»¶å (0000-speaker-timbre.wav)
            safe_speaker = sanitize_for_filename(speaker)
            safe_timbre = sanitize_for_filename(timbre)
            
            old_wav_name = f"{original_index:04d}-{safe_speaker}-{safe_timbre}.wav"
            old_wav_path = os.path.join(wav_output_dir, old_wav_name)
            
            # æ„é€ æ–°æ–‡ä»¶å (UUID-speaker-timbre.wav)
            new_wav_name = f"{line_id}-{safe_speaker}-{safe_timbre}.wav"
            new_wav_path = os.path.join(wav_output_dir, new_wav_name)

            # æ‰§è¡Œé‡å‘½å
            if os.path.exists(old_wav_path) and not os.path.exists(new_wav_path):
                try:
                    os.rename(old_wav_path, new_wav_path)
                    renamed_count += 1
                    logger.info(f"WAVé‡å‘½åè¿ç§»æˆåŠŸ: {old_wav_name} -> {new_wav_name}")
                except OSError as e:
                    logger.error(f"é‡å‘½åWAVæ–‡ä»¶å¤±è´¥: {e}")

    if renamed_count > 0:
        logger.info(f"æˆåŠŸè¿ç§» {renamed_count} ä¸ªæ—§ WAV æ–‡ä»¶åˆ°æ–°å‘½åè§„åˆ™ã€‚")

    # --- 4. å°†æ–°çš„ç« èŠ‚å†…å®¹å†™å…¥ JSON æ–‡ä»¶ (æ¸…ç† originalIndex) ---
    try:
        # å†™å…¥å‰ï¼Œæ„å»ºä¸€ä¸ªä¸åŒ…å« 'originalIndex' çš„æ–°åˆ—è¡¨
        clean_content = []
        for item in req.content:
            # ä½¿ç”¨å­—å…¸æ¨å¯¼å¼åˆ›å»ºä¸€ä¸ªæ–°çš„å­—å…¸å‰¯æœ¬ï¼Œæ’é™¤ 'originalIndex' å­—æ®µ
            item_to_write = {k: v for k, v in item.items() if k != 'originalIndex'}
            clean_content.append(item_to_write)
        
        with open(chapter_json_path, "w", encoding="utf-8") as f:
            json.dump(clean_content, f, ensure_ascii=False, indent=2) # <-- å†™å…¥æ¸…ç†åçš„å†…å®¹
        
        logger.info(f"Chapter content updated successfully: {chapter_json_path}")
        return {"status": "success", "message": f"ç« èŠ‚å†…å®¹å·²æˆåŠŸä¿å­˜ï¼({renamed_count} ä¸ªéŸ³é¢‘æ–‡ä»¶å·²è¿ç§»)"}
    except Exception as e:
        logger.error(f"Failed to write to chapter file {chapter_json_path}: {e}")
        raise HTTPException(status_code=500, detail=f"ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")

# *** æ ¸å¿ƒä¿®æ”¹ 2: æ·»åŠ æ–°çš„æœç´¢å¥å­API ***
@app.post("/api/search_character_sentences")
async def search_character_sentences(req: SearchSentencesRequest):
    """
    Searches for sentences containing a character's name within the
    original source.txt of selected chapters.
    """
    project_dir = os.path.join(PROJECTS_DIR, req.novel_name)
    source_path = os.path.join(project_dir, 'source.txt')

    if not os.path.exists(source_path):
        raise HTTPException(status_code=404, detail="å°è¯´æºæ–‡ä»¶æœªæ‰¾åˆ°ã€‚")

    try:
        with open(source_path, 'r', encoding='utf-8-sig', errors='ignore') as f:
            content = f.read()
        
        all_chapters_map = {c['title']: c['content'] for c in get_chapters_from_txt(content)}
        
        found_sentences = []
        SENTENCE_LIMIT = 20
        
        # å®šä¹‰ä¸€ä¸ªæ›´æ™ºèƒ½çš„å¥å­åˆ†å‰²æ­£åˆ™è¡¨è¾¾å¼ï¼Œå¯ä»¥å¤„ç†ä¸­è‹±æ–‡æ ‡ç‚¹
        sentence_splitter = re.compile(r'([^ã€‚ï¼ï¼Ÿ.â€¦\n]+[ã€‚ï¼ï¼Ÿ.â€¦\n]?)')

        for title in req.chapter_titles:
            if len(found_sentences) >= SENTENCE_LIMIT:
                break
            
            if title in all_chapters_map:
                chapter_content = all_chapters_map[title]
                # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼è¿›è¡Œåˆ†å‰²
                sentences = sentence_splitter.findall(chapter_content)
                
                for sentence in sentences:
                    sentence = sentence.strip()
                    if not sentence:
                        continue

                    if req.character_name in sentence:
                        found_sentences.append({
                            "source": title,
                            "content": sentence
                        })
                        if len(found_sentences) >= SENTENCE_LIMIT:
                            break
        
        return {"sentences": found_sentences}

    except Exception as e:
        logger.error(f"æœç´¢è§’è‰²å¥å­å¤±è´¥: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"æœåŠ¡å™¨ç«¯æœç´¢å¤±è´¥: {e}")
        
def _get_or_create_categories_data():
    """è¾…åŠ©å‡½æ•°ï¼šè¯»å–åˆ†ç±»æ–‡ä»¶ã€‚å¦‚æœä¸å­˜åœ¨ï¼Œåˆ™æ ¹æ®ç‰©ç†æ–‡ä»¶å¤¹è‡ªåŠ¨åˆ›å»ºã€‚"""
    if os.path.exists(CATEGORIES_FILE):
        with open(CATEGORIES_FILE, 'r', encoding='utf-8') as f:
            try:
                # é¢å¤–å¢åŠ ä¸€ä¸ªæ ¡éªŒï¼Œç¡®ä¿åŸºæœ¬ç»“æ„æ­£ç¡®
                data = json.load(f)
                if "categories" in data and "unassigned" in data:
                    return data
            except json.JSONDecodeError:
                pass # æ–‡ä»¶æŸåï¼Œå°†æ‰§è¡Œä¸‹æ–¹é‡å»ºé€»è¾‘

    all_timbres = sorted([d for d in os.listdir(WAV_DIR) if os.path.isdir(os.path.join(WAV_DIR, d))])
    data = {"categories": {}, "unassigned": all_timbres}
    with open(CATEGORIES_FILE, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    return data

def _save_categories_data(data):
    """è¾…åŠ©å‡½æ•°ï¼šä¿å­˜åˆ†ç±»æ•°æ®åˆ°æ–‡ä»¶ã€‚"""
    with open(CATEGORIES_FILE, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

@app.post("/api/add_character_to_novel")
async def add_character_to_novel(req: AddCharacterRequest):
    """
    Adds a new character to the novel's character_timbres.json and character_profiles.json files.
    """
    novel_name = req.novel_name
    new_char_name = req.character_name.strip()

    if not novel_name or not new_char_name:
        raise HTTPException(status_code=400, detail="å°è¯´åç§°å’Œè§’è‰²åç§°ä¸èƒ½ä¸ºç©ºã€‚")

    project_dir = os.path.join(PROJECTS_DIR, novel_name)
    timbres_path = os.path.join(project_dir, 'character_timbres.json')
    profiles_path = os.path.join(project_dir, 'character_profiles.json')

    character_timbres = {}
    if os.path.exists(timbres_path):
        with open(timbres_path, 'r', encoding='utf-8') as f:
            character_timbres = json.load(f)

    character_profiles = {}
    if os.path.exists(profiles_path):
        with open(profiles_path, 'r', encoding='utf-8') as f:
            character_profiles = json.load(f)

    # æ£€æŸ¥æ˜¯å¦é‡å
    if new_char_name in character_timbres or new_char_name in character_profiles:
        raise HTTPException(status_code=409, detail=f"è§’è‰²åç§° '{new_char_name}' å·²å­˜åœ¨ã€‚")
    
    # é¿å…åˆ›å»ºåä¸º "æ—ç™½" çš„è§’è‰²ï¼Œå› ä¸ºå®ƒæœ‰ç‰¹æ®Šå¤„ç†
    if new_char_name == "æ—ç™½":
        raise HTTPException(status_code=400, detail="ä¸èƒ½åˆ›å»ºåä¸º 'æ—ç™½' çš„è§’è‰²ï¼Œè¿™æ˜¯ä¿ç•™åç§°ã€‚")

    try:
        # æ·»åŠ åˆ° character_timbres.json (åˆå§‹éŸ³è‰²ä¸ºç©º)
        character_timbres[new_char_name] = None
        with open(timbres_path, 'w', encoding='utf-8') as f:
            json.dump(character_timbres, f, ensure_ascii=False, indent=2)

        # æ·»åŠ åˆ° character_profiles.json (åˆå§‹ç®€ä»‹ä¸ºç©º)
        character_profiles[new_char_name] = {
            "gender": "æœªçŸ¥",
            "ageGroup": "æœªçŸ¥",
            "identity": ""
        }
        with open(profiles_path, 'w', encoding='utf-8') as f:
            json.dump(character_profiles, f, ensure_ascii=False, indent=4)

        return {"status": "success", "message": f"è§’è‰² '{new_char_name}' å·²æˆåŠŸæ·»åŠ ã€‚", "new_character_name": new_char_name}
    except Exception as e:
        logger.error(f"æ·»åŠ æ–°è§’è‰² '{new_char_name}' å¤±è´¥: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"æœåŠ¡å™¨æ·»åŠ æ–°è§’è‰²å¤±è´¥: {e}")

@app.get("/api/get_chapter_presence_data")
async def get_chapter_presence_data(novel_name: str):
    """
    è·å–æŒ‡å®šå°è¯´çš„æ‰€æœ‰ç« èŠ‚ä¸­è§’è‰²å¯¹è¯åˆ†å¸ƒå’Œç« èŠ‚é¡ºåºåˆ—è¡¨ã€‚
    ç”¨äºå‰ç«¯éŸ³è‰²å†²çªæ£€æµ‹å’Œé¢„æç¤ºã€‚
    """
    if not novel_name:
        raise HTTPException(status_code=400, detail="å°è¯´åç§°ä¸èƒ½ä¸ºç©ºã€‚")
    
    try:
        data = _get_chapter_character_presence_map(novel_name)
        return {"status": "success", "data": data}
    except Exception as e:
        logger.error(f"è·å–å°è¯´ '{novel_name}' çš„ç« èŠ‚è§’è‰²åˆ†å¸ƒæ•°æ®å¤±è´¥: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"æœåŠ¡å™¨é”™è¯¯: {e}")
        
@app.get("/api/timbres/data")
async def get_timbres_data():
    """è·å–æ‰€æœ‰éŸ³è‰²æ•°æ®ï¼ŒåŒ…æ‹¬åˆ†ç±»å’Œæœªåˆ†ç±»åˆ—è¡¨ã€‚"""
    return _get_or_create_categories_data()

@app.post("/api/timbres/categories")
async def create_new_category(req: CreateCategoryRequest): # <-- ä½¿ç”¨æ–°çš„æ¨¡å‹
    """åˆ›å»ºä¸€ä¸ªæ–°çš„ç©ºåˆ†ç±»ã€‚"""
    data = _get_or_create_categories_data()
    new_category_name = req.category_name.strip()
    if not new_category_name:
        raise HTTPException(status_code=400, detail="åˆ†ç±»åä¸èƒ½ä¸ºç©ºã€‚")
    if new_category_name in data["categories"]:
        raise HTTPException(status_code=409, detail="è¯¥åˆ†ç±»åå·²å­˜åœ¨ã€‚")
    
    data["categories"][new_category_name] = []
    _save_categories_data(data)
    return {"status": "success", "message": f"åˆ†ç±» '{new_category_name}' å·²åˆ›å»ºã€‚", "data": data}

@app.delete("/api/timbres/categories/{category_name}")
async def delete_category(category_name: str):
    """åˆ é™¤ä¸€ä¸ªåˆ†ç±»ï¼Œå¹¶å°†å…¶ä¸‹çš„éŸ³è‰²ç§»åŠ¨åˆ°â€œæœªåˆ†ç±»â€ã€‚"""
    decoded_name = urllib.parse.unquote(category_name) # <-- ä½¿ç”¨ urllib.parse.unquote
    data = _get_or_create_categories_data()

    if decoded_name == "unassigned":
        raise HTTPException(status_code=400, detail="â€œæœªåˆ†ç±»â€ä¸èƒ½è¢«åˆ é™¤ã€‚")

    if decoded_name in data["categories"]:
        timbres_to_move = data["categories"].pop(decoded_name) # ä»åˆ†ç±»ä¸­ç§»é™¤ï¼Œå¹¶è·å–å…¶ä¸‹çš„éŸ³è‰²
        data["unassigned"].extend(timbres_to_move) # å°†è¿™äº›éŸ³è‰²æ·»åŠ åˆ°â€œæœªåˆ†ç±»â€
        data["unassigned"] = sorted(list(set(data["unassigned"]))) # å»é‡å¹¶æ’åº
        _save_categories_data(data)
        logger.info(f"åˆ†ç±» '{decoded_name}' å·²åˆ é™¤ï¼Œå…¶ä¸‹ {len(timbres_to_move)} ä¸ªéŸ³è‰²å·²ç§»è‡³â€œæœªåˆ†ç±»â€ã€‚")
        return {"status": "success", "message": f"åˆ†ç±» '{decoded_name}' å·²åˆ é™¤ã€‚", "data": data}
    raise HTTPException(status_code=404, detail="æœªæ‰¾åˆ°è¦åˆ é™¤çš„åˆ†ç±»ã€‚")

@app.delete("/api/timbres/categories/{category_name}")
async def delete_category(category_name: str):
    """åˆ é™¤ä¸€ä¸ªåˆ†ç±»ï¼Œå¹¶å°†å…¶ä¸‹çš„éŸ³è‰²ç§»åŠ¨åˆ°â€œæœªåˆ†ç±»â€ã€‚"""
    decoded_name = unquote(category_name)
    data = _get_or_create_categories_data()
    if decoded_name in data["categories"]:
        timbres_to_move = data["categories"].pop(decoded_name)
        data["unassigned"].extend(timbres_to_move)
        data["unassigned"] = sorted(list(set(data["unassigned"])))
        _save_categories_data(data)
        return {"status": "success", "message": f"åˆ†ç±» '{decoded_name}' å·²åˆ é™¤ã€‚", "data": data}
    raise HTTPException(status_code=404, detail="æœªæ‰¾åˆ°è¦åˆ é™¤çš„åˆ†ç±»ã€‚")
    
@app.post("/api/timbres/move")
async def move_timbre_to_category(req: SetTimbreCategoryRequest):
    """ç§»åŠ¨ä¸€ä¸ªéŸ³è‰²åˆ°æŒ‡å®šçš„åˆ†ç±»ï¼ˆæˆ–æœªåˆ†ç±»ï¼‰ã€‚"""
    data = _get_or_create_categories_data()
    timbre_name = req.timbre_name
    new_category = req.category_name

    # 1. ä»æ‰€æœ‰æ—§ä½ç½®ç§»é™¤
    for category, timbres in data["categories"].items():
        if timbre_name in timbres:
            timbres.remove(timbre_name)
            break
    if timbre_name in data["unassigned"]:
        data["unassigned"].remove(timbre_name)
    
    # 2. æ·»åŠ åˆ°æ–°ä½ç½®
    if not new_category: # ç§»åŠ¨åˆ° "æœªåˆ†ç±»"
        data["unassigned"].append(timbre_name)
        data["unassigned"].sort()
    else:
        if new_category not in data["categories"]:
            raise HTTPException(status_code=404, detail="ç›®æ ‡åˆ†ç±»ä¸å­˜åœ¨ã€‚")
        data["categories"][new_category].append(timbre_name)
        data["categories"][new_category].sort()

    _save_categories_data(data)
    return {"status": "success", "message": f"éŸ³è‰² '{timbre_name}' å·²ç§»åŠ¨ã€‚", "data": data}
    
@app.get("/api/list_timbres")
async def list_timbres():
    """ (å…¼å®¹æ¥å£) è¿”å›æ‰€æœ‰éŸ³è‰²çš„æ‰å¹³åˆ—è¡¨ã€‚"""
    data = _get_or_create_categories_data()
    all_timbres_list = list(data['unassigned'])
    for timbres in data['categories'].values():
        all_timbres_list.extend(timbres)
    return {"timbres": sorted(list(set(all_timbres_list)))}

# ã€æ–°å¢ã€‘è®¾ç½®éŸ³è‰²åˆ†ç±»çš„æ¥å£
@app.post("/api/timbres/set_category")
async def set_timbre_category(req: SetTimbreCategoryRequest):
    """ä¸ºä¸€ä¸ªéŸ³è‰²è®¾ç½®æˆ–æ›´æ”¹å…¶åˆ†ç±»ã€‚"""
    data = _get_or_create_categories_data()
    timbre_name = req.timbre_name
    new_category = req.category_name

    # 1. ä»æ‰€æœ‰æ—§ä½ç½®ï¼ˆæ— è®ºæ˜¯åœ¨å“ªä¸ªåˆ†ç±»æˆ–æœªåˆ†ç±»ï¼‰ä¸­ç§»é™¤è¯¥éŸ³è‰²
    for category, timbres in data["categories"].items():
        if timbre_name in timbres:
            timbres.remove(timbre_name)
            break
    if timbre_name in data["unassigned"]:
        data["unassigned"].remove(timbre_name)
    
    # 2. å°†éŸ³è‰²æ·»åŠ åˆ°æ–°ä½ç½®
    if not new_category: # å¦‚æœä¼ å…¥ç©ºå­—ç¬¦ä¸²ï¼Œåˆ™ç§»åŠ¨åˆ°â€œæœªåˆ†ç±»â€
        data["unassigned"].append(timbre_name)
        data["unassigned"].sort()
    else: # å¦åˆ™ï¼Œç§»åŠ¨åˆ°æŒ‡å®šåˆ†ç±»
        # å¦‚æœåˆ†ç±»ä¸å­˜åœ¨ï¼Œåˆ™åˆ›å»ºå®ƒ
        if new_category not in data["categories"]:
            data["categories"][new_category] = []
        
        data["categories"][new_category].append(timbre_name)
        data["categories"][new_category].sort()

    _save_categories_data(data)
    return {"status": "success", "message": f"å·²å°†éŸ³è‰² '{timbre_name}' ç§»åŠ¨åˆ°åˆ†ç±» '{new_category or 'æœªåˆ†ç±»'}'ã€‚"}

@app.post("/api/upload_timbre")
async def upload_timbre(
    request: Request,
    file: UploadFile = File(...), 
    category_name: str = Form(""),
    timbre_name: str = Form(...), 
    prompt_text: str = Form(...),
    normalize: str = Form(...)
):
    timbre_dir = os.path.join(WAV_DIR, timbre_name)
    if os.path.exists(timbre_dir):
        raise HTTPException(status_code=409, detail="éŸ³è‰²åç§°å·²å­˜åœ¨ã€‚")

    # --- 1. å°†ä¸Šä¼ æ–‡ä»¶ä¿å­˜åˆ°ä¸´æ—¶ä½ç½® ---
    temp_input_path = os.path.join(TEMP_DIR, f"input_{uuid.uuid4()}_{file.filename}")
    with open(temp_input_path, "wb") as buffer:
        shutil.copyfileobj(file.file, buffer)

    current_audio_path = temp_input_path
    
    try:
        # --- 3. åŠ è½½éŸ³é¢‘å¹¶è¿›è¡Œåç»­å¤„ç† ---
        audio = AudioSegment.from_file(current_audio_path)

        # --- 4. (å¯é€‰) éŸ³é‡æ ‡å‡†åŒ– ---
        do_normalize = normalize.lower() == 'true'
        if do_normalize:
            logger.info(f"æ­£åœ¨ä¸ºéŸ³è‰² '{timbre_name}' è¿›è¡ŒéŸ³é‡æ ‡å‡†åŒ–...")
            audio = pydub_normalize(audio)
            logger.info("éŸ³é‡æ ‡å‡†åŒ–å®Œæˆã€‚")

        # --- 5. æœ€ç»ˆæ ¼å¼åŒ–å¹¶ä¿å­˜ ---
        # ç»Ÿä¸€è½¬æ¢ä¸ºå•å£°é“
        audio = audio.set_channels(1)
        
        os.makedirs(timbre_dir)
        final_wav_path = os.path.join(timbre_dir, "1.wav")
        audio.export(final_wav_path, format="wav")
        
        with open(os.path.join(timbre_dir, "1.txt"), "w", encoding="utf-8") as f:
            f.write(prompt_text)
        
        data = _get_or_create_categories_data()
        if category_name and category_name in data["categories"]:
            data["categories"][category_name].append(timbre_name)
            data["categories"][category_name].sort()
        else:
            data["unassigned"].append(timbre_name)
            data["unassigned"].sort()
        
        _save_categories_data(data)
        
        return {"status": "success", "message": f"éŸ³è‰² '{timbre_name}' å·²æˆåŠŸæ·»åŠ å¹¶å¤„ç†ï¼"}

    except Exception as e:
        logger.error(f"å¤„ç†ä¸Šä¼ éŸ³è‰²æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        # å‡ºé”™æ—¶ç¡®ä¿æ¸…ç† timbre_dir
        if os.path.exists(timbre_dir):
            shutil.rmtree(timbre_dir)
        raise HTTPException(status_code=500, detail=str(e))
    finally:
        # --- 6. æœ€ç»ˆæ¸…ç†æ‰€æœ‰ä¸´æ—¶æ–‡ä»¶å’Œç›®å½• ---
        if os.path.exists(temp_input_path):
            os.remove(temp_input_path)

# *** åˆ é™¤éŸ³è‰²API ***
@app.delete("/api/delete_timbre")
async def delete_timbre(timbre_name: str):
    """
    Deletes a timbre folder and all its contents.
    Includes security checks to prevent directory traversal.
    """
    if not timbre_name:
        raise HTTPException(status_code=400, detail="éŸ³è‰²åç§°ä¸èƒ½ä¸ºç©ºã€‚")
        
    # a. ä» timbre_categories.json æ–‡ä»¶ä¸­ç§»é™¤éŸ³è‰²è®°å½•
    data = _get_or_create_categories_data()
    found_and_removed = False
    
    # ä» "æœªåˆ†ç±»" ä¸­æŸ¥æ‰¾å¹¶ç§»é™¤
    if timbre_name in data["unassigned"]:
        data["unassigned"].remove(timbre_name)
        found_and_removed = True
    
    # å¦‚æœæœªæ‰¾åˆ°ï¼Œåˆ™ä»æ‰€æœ‰åˆ†ç±»ä¸­æŸ¥æ‰¾å¹¶ç§»é™¤
    if not found_and_removed:
        for category, timbres in data["categories"].items():
            if timbre_name in timbres:
                timbres.remove(timbre_name)
                found_and_removed = True
                break
    
    # å¦‚æœæ‰¾åˆ°äº†å¹¶ç§»é™¤äº†è®°å½•ï¼Œåˆ™ä¿å­˜æ–‡ä»¶
    if found_and_removed:
        _save_categories_data(data)
        logger.info(f"å·²ä» timbre_categories.json ä¸­ç§»é™¤éŸ³è‰²è®°å½•: {timbre_name}")
    else:
        logger.warning(f"åœ¨ timbre_categories.json ä¸­æœªæ‰¾åˆ°éŸ³è‰²è®°å½•: {timbre_name}ï¼Œä½†ä»å°†å°è¯•åˆ é™¤ç‰©ç†æ–‡ä»¶å¤¹ã€‚")
        
    # b. åˆ é™¤ç‰©ç†æ–‡ä»¶å¤¹
    # 1. æ„å»ºç›®æ ‡ç›®å½•çš„è·¯å¾„
    timbre_dir = os.path.join(WAV_DIR, timbre_name)
    
    # 2. å®‰å…¨æ€§æ£€æŸ¥ï¼šé˜²æ­¢ç›®å½•éå†æ”»å‡»
    #    é€šè¿‡ realpath è§£æè·¯å¾„ï¼Œç¡®ä¿å®ƒåœ¨åˆæ³•çš„ WAV_DIR å†…éƒ¨
    wav_root = os.path.realpath(WAV_DIR)
    target_path = os.path.realpath(timbre_dir)
    
    if not target_path.startswith(wav_root):
        logger.warning(f"æ½œåœ¨çš„ç›®å½•éå†æ”»å‡»è¢«é˜»æ­¢: {timbre_name}")
        raise HTTPException(status_code=403, detail="ç¦æ­¢è®¿é—®ã€‚")
        
    if not os.path.isdir(target_path):
        raise HTTPException(status_code=404, detail=f"éŸ³è‰² '{timbre_name}' æœªæ‰¾åˆ°ã€‚")

    # 3. æ‰§è¡Œåˆ é™¤æ“ä½œ
    try:
        shutil.rmtree(target_path)
        logger.info(f"éŸ³è‰²å·²æˆåŠŸåˆ é™¤: {target_path}")
        return {"status": "success", "message": f"éŸ³è‰² '{timbre_name}' å·²æˆåŠŸåˆ é™¤ã€‚"}
    except Exception as e:
        logger.error(f"åˆ é™¤éŸ³è‰²å¤±è´¥ '{timbre_name}': {e}")
        raise HTTPException(status_code=500, detail=f"æœåŠ¡å™¨åˆ é™¤æ–‡ä»¶å¤±è´¥: {e}")
        
# *** æ‰“åŒ…ä¸‹è½½API ***
@app.post("/api/download_spliced_chapters")
async def download_spliced_chapters(req: DownloadRequest):
    """
    Takes a list of audio file paths, packages them into a ZIP file in memory,
    and returns it for download.
    """
    if not req.file_paths:
        raise HTTPException(status_code=400, detail="æ²¡æœ‰æä¾›éœ€è¦ä¸‹è½½çš„æ–‡ä»¶è·¯å¾„ã€‚")

    zip_buffer = io.BytesIO()
    output_root = os.path.realpath(OUTPUT_DIR)

    with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
        for file_path in req.file_paths:
            full_path = os.path.join(OUTPUT_DIR, file_path)
            if not os.path.realpath(full_path).startswith(output_root):
                logger.warning(f"ä¸‹è½½è¯·æ±‚è¢«é˜»æ­¢ï¼ˆç›®å½•éå†ï¼‰: {file_path}")
                continue
            if os.path.exists(full_path):
                zip_file.write(full_path, arcname=os.path.basename(full_path))
            else:
                logger.warning(f"ä¸‹è½½è¯·æ±‚ä¸­åŒ…å«ä¸å­˜åœ¨çš„æ–‡ä»¶ï¼Œå·²è·³è¿‡: {full_path}")

    if not zip_file.namelist():
        raise HTTPException(status_code=404, detail="æ‰€æœ‰è¯·æ±‚çš„æ–‡ä»¶éƒ½ä¸å­˜åœ¨æˆ–ä¸åˆæ³•ã€‚")

    zip_buffer.seek(0)
    
    novel_name = "chapters"
    if req.file_paths:
        try:
            novel_name = req.file_paths[0].split('/')[0]
        except:
            pass
    
    zip_filename = f"{novel_name}_spliced.zip"
    
    # *** æ ¸å¿ƒä¿®æ”¹ï¼šæ­£ç¡®å¤„ç†åŒ…å«ä¸­æ–‡çš„æ–‡ä»¶å ***
    try:
        # å°è¯•å°†æ–‡ä»¶åç¼–ç ä¸º ASCIIï¼Œå¦‚æœå¤±è´¥ï¼Œè¯´æ˜å«æœ‰éASCIIå­—ç¬¦
        zip_filename.encode('ascii')
        # å¦‚æœæˆåŠŸï¼Œä½¿ç”¨ç®€å•çš„ header
        headers = {
            'Content-Disposition': f'attachment; filename="{zip_filename}"'
        }
    except UnicodeEncodeError:
        # å¦‚æœå¤±è´¥ï¼Œä½¿ç”¨ RFC 6266 æ¨èçš„æ ¼å¼æ¥å¤„ç†éASCIIå­—ç¬¦
        # 1. åˆ›å»ºä¸€ä¸ªåªåŒ…å«ASCIIå­—ç¬¦çš„å›é€€æ–‡ä»¶å
        fallback_filename = "download.zip"
        # 2. å¯¹åŸå§‹æ–‡ä»¶åè¿›è¡Œ URL ç¼–ç 
        encoded_filename = urllib.parse.quote(zip_filename)
        # 3. æ„é€ å¤åˆçš„ Content-Disposition å¤´
        headers = {
            'Content-Disposition': f'attachment; filename="{fallback_filename}"; filename*=UTF-8\'\'{encoded_filename}'
        }

    # ä½¿ç”¨ StreamingResponse è¿”å›ZIPæ–‡ä»¶æµï¼Œå¹¶é™„ä¸Šæˆ‘ä»¬æ„é€ å¥½çš„headers
    return StreamingResponse(
        zip_buffer,
        media_type="application/x-zip-compressed",
        headers=headers
    ) 

@app.post("/api/stt_elevenlabs", response_model=STTResponse)
async def speech_to_text_elevenlabs(file: UploadFile = File(...)):
    # 1. åŠ è½½é…ç½®ï¼Œè·å– API Key
    try:
        with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
            config = json.load(f)
        api_key = config.get("elevenlabs", {}).get("api_key")
        if not api_key:
            raise HTTPException(status_code=503, detail="æœåŠ¡å™¨æœªé…ç½®ElevenLabs API Keyã€‚")
    except FileNotFoundError:
        raise HTTPException(status_code=500, detail="æœåŠ¡å™¨é…ç½®æ–‡ä»¶ä¸¢å¤±ã€‚")

    # 2. ElevenLabs API ç«¯ç‚¹å’Œå¤´éƒ¨
    url = "https://api.elevenlabs.io/v1/speech-to-text"
    headers = {
        "xi-api-key": api_key
    }

    # 3. å‡†å¤‡è¦å‘é€çš„æ–‡ä»¶æ•°æ®
    # a. å®šä¹‰æ•°æ®å­—æ®µ (å³ä½¿æ˜¯å¯é€‰çš„ï¼Œä¹Ÿæ˜ç¡®å‘é€)
    data = {
        'model_id': 'scribe_v1'
    }
    
    # b. å®šä¹‰æ–‡ä»¶å­—æ®µ
    files = {
        'file': (file.filename, await file.read(), file.content_type)
    }
    
    logger.info(f"æ­£åœ¨å°†éŸ³é¢‘æ–‡ä»¶ '{file.filename}' è½¬å‘åˆ° ElevenLabs STT API...")

    try:
        # 4. å‘é€è¯·æ±‚
        response = requests.post(url, headers=headers, data=data, files=files, timeout=120)
        response.raise_for_status()
        
        result = response.json()
        
        # 5. è§£æå¹¶è¿”å›ç»“æœ
        recognized_text = result.get("text")
        if recognized_text is not None:
            logger.info("ElevenLabs STT æˆåŠŸè¯†åˆ«æ–‡æœ¬ã€‚")
            return STTResponse(status="success", text=recognized_text)
        else:
            logger.error(f"ElevenLabs API è¿”å›æ— æ•ˆå“åº”: {result}")
            raise HTTPException(status_code=500, detail="ElevenLabs API è¿”å›æ— æ•ˆå“åº”ã€‚")

    except requests.exceptions.RequestException as e:
        logger.error(f"è°ƒç”¨ ElevenLabs API å¤±è´¥: {e}", exc_info=True)
        # å°è¯•è§£æElevenLabsè¿”å›çš„æ›´å…·ä½“çš„é”™è¯¯ä¿¡æ¯
        error_detail = "è¿æ¥æˆ–è¯·æ±‚ ElevenLabs API å¤±è´¥ã€‚"
        if e.response is not None:
            try:
                error_data = e.response.json()
                detail_field = error_data.get("detail")

                if isinstance(detail_field, list) and detail_field:
                    # å¦‚æœ detail æ˜¯ä¸€ä¸ªéç©ºåˆ—è¡¨
                    first_error = detail_field[0]
                    if isinstance(first_error, dict) and 'msg' in first_error:
                        error_detail = first_error['msg'] # æå–ç¬¬ä¸€ä¸ªé”™è¯¯çš„ msg
                elif isinstance(detail_field, dict) and 'message' in detail_field:
                    # å¦‚æœ detail æ˜¯ä¸€ä¸ªåŒ…å« message çš„å­—å…¸
                    error_detail = detail_field['message']
                elif isinstance(detail_field, str):
                    # å¦‚æœ detail æœ¬èº«å°±æ˜¯ä¸ªå­—ç¬¦ä¸²
                    error_detail = detail_field
                
                # ç‰¹åˆ«å¤„ç† 401 Unauthorized é”™è¯¯
                if e.response.status_code == 401:
                    error_detail = "ElevenLabs API Key æ— æ•ˆæˆ–æœªæä¾›ã€‚"

            except json.JSONDecodeError:
                error_detail = f"APIè¿”å›é”™è¯¯ (çŠ¶æ€ç : {e.response.status_code})ï¼Œä¸”å“åº”ä½“ä¸æ˜¯æœ‰æ•ˆçš„JSONã€‚"
        
        raise HTTPException(status_code=503, detail=error_detail)
    except Exception as e:
        logger.error(f"STTå¤„ç†æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"æœåŠ¡å™¨å†…éƒ¨é”™è¯¯: {e}")        
        
# =============================================================
# 5. é™æ€æ–‡ä»¶æŒ‚è½½
# =============================================================
app.mount("/output", StaticFiles(directory=OUTPUT_DIR), name="output")
app.mount("/wav", StaticFiles(directory=WAV_DIR), name="wav")
app.mount("/", StaticFiles(directory=ROOT_DIR, html=True), name="static")

# =============================================================
# 6. ä¸»å¯åŠ¨é€»è¾‘ (æ ¸å¿ƒä¿®å¤)
# =============================================================
if __name__ == '__main__':

    # 1. å¯åŠ¨æ—¶å…ˆåˆå§‹åŒ–é…ç½®æ–‡ä»¶
    initialize_llm_config()

    # 2. é…ç½® FFmpeg
    ffmpeg_executable_path = os.path.join(ROOT_DIR, "ffmpeg-8.0-full_build", "bin", "ffmpeg.exe")
    if os.path.exists(ffmpeg_executable_path):
        AudioSegment.converter = ffmpeg_executable_path
        logger.info(f"æˆåŠŸä¸º pydub å®šä½åˆ° ffmpeg.exe: {ffmpeg_executable_path}")
    else:
        logger.warning(f"æœªåœ¨æœŸæœ›çš„ä½ç½®æ‰¾åˆ° ffmpeg.exe: {ffmpeg_executable_path}")
        logger.warning("éŸ³é¢‘æ ¼å¼è½¬æ¢åŠŸèƒ½ï¼ˆå¦‚å¯¼å‡ºMP3ï¼‰å¯èƒ½å—é™ã€‚")

    # 4. è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description="AI Voice Studio Pro - Backend Service")
    parser.add_argument('--port', type=int, default=8000, help="ç«¯å£å·")
    parser.add_argument('--host', type=str, default='0.0.0.0', help="ä¸»æœºåœ°å€")
    args = parser.parse_args()
    
    logger.info(f"æœåŠ¡å™¨å¯åŠ¨ï¼Œè¯·åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€ http://{args.host}:{args.port}")
    
    # 5. å¯åŠ¨ Uvicornï¼Œã€ä¸ä½¿ç”¨ã€‘ reload=True
    uvicorn.run(app, host=args.host, port=args.port)
